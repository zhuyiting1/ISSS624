[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acknowledgement",
    "section": "",
    "text": "This website is made possible thanks to the unreserved knowledge imparted on me by Dr. Kam Tin Seong, Associate Professor of Information Systems (Practice), Singapore Management University (SMU)."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learned how to import, wrangle and visualise geospatial data using the appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#data-sources",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#data-sources",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Data Sources",
    "text": "Data Sources\nThe data used in this exercise are obtained from the following sources on 18 November 2022:\n\nMaster Plan 2014 Subzone Boundary (Web)\nPre-schools Location\nCycling Path\nAirbnb Singapore Listings - scroll down to Singapore, download listings.csv\n\n#1-3 are geospatial data while #4 is in .csv with latitude and longitude information."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below installs and loads sf and tidyverse packages into the R environment.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#importing-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImporting Polygon Feature Data\nFrom the sf package, use st_read() function to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature dataframe.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThere are 323 features and 15 fields in the \\(mpsz\\) dataframe. The geometry type is multipolygon. It is in the svy21 projected coordinates system.\n\n\nImporting Polyline Feature Data\nFrom the sf package, similarly use st_read() function to import CyclingPathGazette shapefile into R as a line feature dataframe.\n\ncyclingpath = st_read(dsn = \"data/geospatial\",\n                      layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2248 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThere are 2248 features and 2 fields in the \\(cyclingpath\\) dataframe. The geometry type is multilinestring. It is also in the svy21 projected coordinates system.\n\n\nImporting KML Data\nAs above, from the sf package, use st_read() function to import pre-schools-location-kml file into R. However, as the data format is .kml, we will use the following format instead of the earlier dsn and layer arguments:\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThere are 1925 features and 2 fields in the \\(preschool\\) dataframe. The geometry type is point. Unlike \\(mpsz\\) and \\(cyclingpath\\), it is in the wgs84 geodetic coordinates system. We will need to transform the data to svy21 coordinates system later.\n\n\nKnowing the Data\nTop-line geometry information of the mpsz dataframe:\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nThe multipolygon information of the first 5 geometries are shown.\nNext, we will use glimpse() of the dplyr package:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() shows the data type of each field (int, chr, date, dbl (double-precision values)) and the first few entries for each field.\nFor complete information of a feature object, head() from R’s built-in package can give us the first n rows of the dataframe:\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting the Geospatial Data",
    "text": "Plotting the Geospatial Data\nplot() from R Graphic allows us to visualise the geospatial features of the data that we cannot easily pick up in plain rows and columns.\n\nplot(mpsz)\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum. We can also specify only plotting the geometry or a specific attribute (like the 6th plot above) as shown below:\n\nplot(st_geometry(mpsz))\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#map-projection",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#map-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Map Projection",
    "text": "Map Projection\nSimilar to data preprocessing for the usual dataframe, geoprocessing should be done to ensure that the data are projected using the same coordinate system. We will be using projection transformation to project a simple feature dataframe from one coordinate system to another.\n\nAssigning EPSG Code\nAlways check for the coordinate system of the source data, which can be done as such:\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nWith reference to the last part of the output (CS[Cartesian,2]), the EPSG code is 9001. However, for svy21, the correct EPSG code should be 3414. We want to assign the correct EPSG code to the \\(mpsz\\) dataframe, using st_set_crs() of the sf package.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\nChecking that it has been performed successfully:\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNow the ID under USAGE indicates “EPSG”, 3414.\n\n\nTransforming Projection from wgs84 to svy21\nGeographic coordinate system is transformed to projected coordinate system here to allow for analysis using distance and/or area measurements. For the \\(preschool\\) dataframe, as we saw earlier that the geodetic CRS is wgs84, it is not appropriate to use st_set_crs() like we did for $mpsz$. Instead, st_transform() from the same sf package can help us do the job. This is because we need to reproject \\(preschool\\) from one coordinate system to another mathematically.\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n# In practice, we need to find the appropriate projection coordinate system to use before perfroming the projection transformation.\n\nChecking the projected coordinate system:\n\nst_geometry(preschool3414)\n\nGeometry set for 1925 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\nst_crs(preschool3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nThe projected coordinate system has been revised to svy21. USAGE ID is now “EPSG”, 3414."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#working-with-aspatial-data",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#working-with-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Working with Aspatial Data",
    "text": "Working with Aspatial Data\nRecall we are also using the listings.csv file from Inside Airbnb. The data is aspatial, as the data itself is not geospatial but it has data fields that capture the x- and y-coordinates of the data points.\n\nStep 1\nLoad the data into a dataframe \\(listings\\):\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\n\nUsing read_csv() from the readr package within tidyverse, we see that the dataframe with 4161 rows and 18 columns is imported to R.\n\n\nStep 2\nTo look at it in a bit more detail:\n\nlist(listings)\n\n[[1]]\n# A tibble: 4,161 × 18\n       id name     host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n    <dbl> <chr>      <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  50646 Pleasan…  227796 Sujatha Centra… Bukit …    1.33    104. Privat…    80\n 2  71609 Ensuite…  367042 Belinda East R… Tampin…    1.35    104. Privat…   145\n 3  71896 B&B  Ro…  367042 Belinda East R… Tampin…    1.35    104. Privat…    85\n 4  71903 Room 2-…  367042 Belinda East R… Tampin…    1.35    104. Privat…    85\n 5 275344 15 mins… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    49\n 6 289234 Booking…  367042 Belinda East R… Tampin…    1.34    104. Privat…   184\n 7 294281 5 mins … 1521514 Elizab… Centra… Newton     1.31    104. Privat…    79\n 8 324945 Cozy Bl… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    49\n 9 330089 Cozy Bl… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    55\n10 330095 10 mins… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    55\n# … with 4,151 more rows, 8 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>,\n#   number_of_reviews_ltm <dbl>, license <chr>, and abbreviated variable names\n#   ¹​host_name, ²​neighbourhood_group, ³​neighbourhood, ⁴​latitude, ⁵​longitude,\n#   ⁶​room_type\n\n\nWe see that the \\(latitude\\) and \\(longitude\\) data fields form 2 of the columns, and are in decimal degree format. As a best guess, we will assume that the data is in wgs84 geographic coordinate system.\n\n\nStep 3\nConvert \\(listings\\) dataframe into a simple feature dataframe and perform transformation to projected coordinate system:\n\nlistings_sf <- st_as_sf(listings, \n                        coords = c(\"longitude\", \"latitude\"), # coords argument takes in x-coordinates before y-coordinates\n                        crs = 4326) %>% # 4326 is the wgs84 geographic coordinate system\n  st_transform(crs = 3414) # %>% performs nested st_transform() to svy21 projected coordinate system\n\nglimpse(listings_sf) # shows the topline info of the transformed data\n\nRows: 4,161\nColumns: 17\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275344, 289…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 145, 85, 85, 49, 184, 79, 49, 55, 5…\n$ minimum_nights                 <dbl> 92, 92, 92, 92, 60, 92, 92, 60, 60, 60,…\n$ number_of_reviews              <dbl> 18, 20, 24, 47, 14, 12, 133, 17, 12, 3,…\n$ last_review                    <date> 2014-12-26, 2020-01-17, 2019-10-13, 20…\n$ reviews_per_month              <dbl> 0.18, 0.15, 0.18, 0.34, 0.11, 0.10, 1.0…\n$ calculated_host_listings_count <dbl> 1, 6, 6, 6, 44, 6, 7, 44, 44, 44, 6, 7,…\n$ availability_365               <dbl> 365, 340, 265, 365, 296, 285, 365, 181,…\n$ number_of_reviews_ltm          <dbl> 0, 0, 0, 0, 1, 0, 0, 3, 2, 0, 1, 0, 0, …\n$ license                        <chr> NA, NA, NA, NA, \"S0399\", NA, NA, \"S0399…\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\n\\(latitude\\) and \\(longitude\\) columns are dropped as they have been used to generate the new \\(geometry\\) column in svy21 projected coordinate system."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#geoprocessing---buffering",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#geoprocessing---buffering",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Geoprocessing - Buffering",
    "text": "Geoprocessing - Buffering\nTo compute a 5-metre buffer on both ends of the current cycling path using the \\(cyclingpath\\) dataframe:\n\nbuffer_cycling_5m <- st_buffer(cyclingpath,\n                               dist = 5,\n                               nQuadSegs = 30)\n\nWe can see that buffer_cycling_5m is as follows:\n\nbuffer_cycling_5m\n\nSimple feature collection with 2248 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 11849.32 ymin: 28342.98 xmax: 42631.09 ymax: 48953.15\nProjected CRS: SVY21\nFirst 10 features:\n   PLANNING_A PLANNING_1                       geometry\n1        <NA>       <NA> POLYGON ((16004.15 36799.78...\n2        <NA>       <NA> POLYGON ((16013.15 36849.86...\n3        <NA>       <NA> POLYGON ((16016.91 36892.98...\n4        <NA>       <NA> POLYGON ((16017.59 36864, 1...\n5        <NA>       <NA> POLYGON ((16022.36 36900.57...\n6        <NA>       <NA> POLYGON ((15903.87 36941.12...\n7        <NA>       <NA> POLYGON ((17791.37 34725.6,...\n8        <NA>       <NA> POLYGON ((17845.19 34694.74...\n9        <NA>       <NA> POLYGON ((16719.51 36124.49...\n10       <NA>       <NA> POLYGON ((17027.74 36260.02...\n\n\nWe then add a new variable in buffer_cycling_5m to calculate the area of the buffer:\n\nbuffer_cycling_5m$AREA <- st_area(buffer_cycling_5m)\nbuffer_cycling_5m\n\nSimple feature collection with 2248 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 11849.32 ymin: 28342.98 xmax: 42631.09 ymax: 48953.15\nProjected CRS: SVY21\nFirst 10 features:\n   PLANNING_A PLANNING_1                       geometry           AREA\n1        <NA>       <NA> POLYGON ((16004.15 36799.78... 186.2934 [m^2]\n2        <NA>       <NA> POLYGON ((16013.15 36849.86... 293.4840 [m^2]\n3        <NA>       <NA> POLYGON ((16016.91 36892.98... 284.8275 [m^2]\n4        <NA>       <NA> POLYGON ((16017.59 36864, 1... 144.8915 [m^2]\n5        <NA>       <NA> POLYGON ((16022.36 36900.57... 281.2016 [m^2]\n6        <NA>       <NA> POLYGON ((15903.87 36941.12... 398.7081 [m^2]\n7        <NA>       <NA> POLYGON ((17791.37 34725.6,... 937.4688 [m^2]\n8        <NA>       <NA> POLYGON ((17845.19 34694.74... 426.5907 [m^2]\n9        <NA>       <NA> POLYGON ((16719.51 36124.49... 570.4396 [m^2]\n10       <NA>       <NA> POLYGON ((17027.74 36260.02... 319.0839 [m^2]\n\n\nWe can see that the new variable \\(AREA\\) in square metres is added.\nFinally, we will derive the total land involved by summing up all the \\(AREA\\):\n\nsum(buffer_cycling_5m$AREA)\n\n1556978 [m^2]\n\n\nWe get 1,556,978 m2 of area in total."
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#geoprocessing---point-in-polygon-count",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#geoprocessing---point-in-polygon-count",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Geoprocessing - Point-in-Polygon Count",
    "text": "Geoprocessing - Point-in-Polygon Count\nAnother analysis that we can do is to find the number of pre-schools in each planning subzone. We will first identify pre-schools located within each planning subzone by using st_intersects() from the sf package. Next, length() of Base R is used to return the number of pre-schools by planning subzone.\n\nmpsz3414$`PreSch Count` <- lengths(st_intersects(mpsz3414, preschool3414))\n\nNote that the symbol used is ” ` ” (backtick) and not ” ’ ” (apostrophe).\nThe summary statistics of the newly derived \\(PreSch Count\\) is as follows:\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    3.00    5.96    9.00   58.00 \n\n\nTo list the planning subzone with the most number of pre-schools, top_n() from the dplyr package is used. In this case, the 1 planning subzone with the highest number of pre-school count and its information are displayed:\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           58\n\n\nNext, we try to calculate the density of pre-school by planning subzone:\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count` / Area * 1000000)\n# mutate() adds new variables and preserves existing ones, \n# whereas transmute() adds new variables and drops existing ones\n# * 1,000,000 converts the unit from per m sq to per km sq\n\n\nsummary(mpsz3414$`PreSch Density`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   4.203   5.538   9.380  35.602"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nTo study the \\(PreSch Density\\) data further, we will use appropriate ggplot2 function from the tidyverse package to visualise the data.\nFor distribution, we use the hist() function of R Graphics to plot a histogram:\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nHowever, this is only suitable for a quick glance of the histogram. To make the plot more presentable and usable (for publication), we tap on ggplot2 functions:\n\nggplot(data = mpsz3414,\n       aes(x = as.numeric(`PreSch Density`))) +\n  geom_histogram(bins = 20,\n                 color = \"black\",\n                 fill = \"light blue\") +\n  labs(title = \"Pre-school distribution in Singapore\",\n       subtitle = \"There are many planning sub-zones with only 1 single pre-school. \\n On the other hand, there are 2 planning sub-zones with at least 20 pre-schools.\",\n       x = \"Pre-school density (per km sq)\",\n       y = \"Frequency\")\n\n\n\n\nShowing a scatterplot between pre-school density and pre-school count:\n\nggplot(data = mpsz3414, \n       aes(x = as.numeric(`PreSch Density`), \n           y = as.numeric(`PreSch Count`))) + \n  geom_point() +\n  labs(x = \"Pre-school density (per km sq)\",\n       y = \"Pre-school count\")"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#choropleth-mapping",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#choropleth-mapping",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Choropleth Mapping",
    "text": "Choropleth Mapping\nWe will now use tmap (thematic maps) package in addition to sf and tidyverse that we used. Installing and loading all 3:\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#data-sources-1",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#data-sources-1",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Data Sources",
    "text": "Data Sources\nThe data used in this exercise are obtained from the following sources on 18 November 2022:\n\nMaster Plan 2014 Subzone Boundary (Web)\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format - scroll down and download respopagesextod2011to2020.csv"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#importing-data",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#importing-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Data",
    "text": "Importing Data\n\nGeospatial Data\nUsing what we have learned:\n\nmpsz <- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex1\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nAttribute Data\nImporting and examining the population data that we downloaded in data source #2:\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\npopdata\n\n# A tibble: 984,656 × 7\n   PA         SZ                     AG     Sex     TOD                Pop  Time\n   <chr>      <chr>                  <chr>  <chr>   <chr>            <dbl> <dbl>\n 1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 1- and 2-Ro…     0  2011\n 2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 3-Room Flats    10  2011\n 3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 4-Room Flats    30  2011\n 4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 5-Room and …    50  2011\n 5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HUDC Flats (exc…     0  2011\n 6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Landed Properti…     0  2011\n 7 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Condominiums an…    40  2011\n 8 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Others               0  2011\n 9 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 1- and 2-Ro…     0  2011\n10 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 3-Room Flats    10  2011\n# … with 984,646 more rows"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#data-wrangling",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#data-wrangling",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nWe only want to use the 2022 values, and include the following variables:\n\nPA: Planning Area\nSZ: Subzone\nFrom AG (age group), create the following new variables:\n\nYOUNG: age group 0-4 to 20-24\nECONOMY ACTIVE: age group 25-29 to 60-64\nAGED: age group 65 and above\nTOTAL: all age group\nDEPENDENCY: ratio between young + aged against economy active group\n\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup() %>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+ \n         rowSums(.[13:15])) %>%\n  mutate(`AGED`=rowSums(.[16:21])) %>%\n  mutate(`TOTAL`=rowSums(.[3:21])) %>%\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#joining-data",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#joining-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Joining Data",
    "text": "Joining Data\nWe want to first convert the PA and SZ data fields to uppercase to be consistent with the SUBZONE_N and PLN_AREA_N data fields in \\(mpsz\\).\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ),\n            .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nNext, left_join() from dplyr is used to join the geographical data from \\(mpsz\\) and attribute table from \\(popdata2020\\) using planning subzone names, in this case SUBZONE_N and SZ, as common identifiers. Left join with reference to \\(mpsz\\) is used to ensure that the output mpsz_pop2020 will be a simple features dataframe.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "Hands-on_Ex1/Hands-on_Ex01.html#choropleth-mapping-using-tmap",
    "href": "Hands-on_Ex1/Hands-on_Ex01.html#choropleth-mapping-using-tmap",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Choropleth Mapping using tmap",
    "text": "Choropleth Mapping using tmap\n\nQuick plot using qtm()\nDoing a cartographic standard choropleth map:\n\ntmap_mode(\"plot\") # gives static map; \"view\" for interactive mode\nqtm(mpsz_pop2020,\n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\nCustomisable thematic map using tmap()’s elements\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by Planning Subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA) and\\n Population data from Department of Statistics (DOS)\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nStep-by-Step Break-Down\n\n\n1) Base Map\nWe first use the tm_shape() function to define the input data and tm_polygons() function to draw the planning subzone polygons:\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n2) Adding Colours (Choropleth Map)\nNext, we assign the target variable Dependency to tm_polygons() function to display a choropleth map showing the geographical distribution (like a map-based histogram):\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nSome points to note for tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. This style rounds breaks into whole numbers where possible and spaces them evenly.\nThe default colour scheme used is YlOrRd (Yellow Orange Red) of ColorBrewer.\nBy default, missing values will be shaded in grey.\n\n\n\n3) Using tm_fill() and tm_border() instead\ntm_polygons() is a wrapper of tm_fill() (shading) and tm_border() (borders).\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nAbove is a map with fill only (coloured by dependencies in each planning subzone), no borders.\n\ntm_shape(mpsz_pop2020) + \n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1, alpha = 1)\n\n\n\n\nUsing tm_borders, we have added light grey borders (default colour “grey40”) with 0.1 line width and 1 for not transparent (default alpha uses that of the colour and is typically 1).\n\n\n4) Data Classification\nQuantile data classification:\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nData classification into 5 equal intervals between the minimum and the maximum values:\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nComparing the different styles available:\n\nstyle1 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: SD\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle2 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Equal\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle3 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Pretty\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle4 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\",\n          title = \"Style: Quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Quantile\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle5 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Kmeans\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle6 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Fisher\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle7 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Jenks\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nstyle8 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"headtails\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Style: Headtails\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\n\ntmap_arrange(style1, style2, style3, style4, style5, style6, style7, style8, ncol = 4)\n\n\n\n\nBetween the styles shown above, Kmeans appear to give a more evenly balanced set of colours across subzones, followed by quantile.\nComparing same classification method with different number of classes:\n\nn2 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"2 Classes\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE)\nn6 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"6 Classes\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE)\nn10 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"10 Classes\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE)\nn20 <- tm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"20 Classes\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 0.5, \n            legend.width = 0.4,\n            frame = TRUE)\n\ntmap_arrange(n2, n6, n10, n20, ncol = 2)\n\n\n\n\nThe larger the number of classes, the finer the data classification and the more colours are seen in the choropleth map. For style = “jenks”, 6-10 classes appear to be optimal visually in differentiating the different gradients.\n\n\n5) Custom Breaks\nGetting some descriptive statistics of the DEPENDENCY field:\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nWith reference to the summary statistics, we set break points at 0.60, 0.70, 0.80 and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 100).\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 100)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe note that the cutoffs are similar to our top pick for style (“kmeans”), which uses ~c(0, 0.5, 0.7, 0.8, 10, 19), and our first runner-up (“quantile”), at ~c(0, 0.6, 0.7, 0.8, 19).\n\n\n6) Colour Scheme\nUsing ColorBrewer palette = “Blues”:\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nInverse green:\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n7) Map Layouts - Legend\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by Planning Subzone\\n(Jenks Classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n8) Map Layouts - Style\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\nThe default tmap_style() is “white”.\n\n\n\n8) Map Layouts - Cartographic Furniture\nCartographic furniture refers to features such as compass, scale bar and grid lines.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby Planning Subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA) and\\n Population data from Department of Statistics (DOS)\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\ntmap_style(\"white\")\n\n\n\n9) Multiple Choropleth Maps\nAssigning multiple values to at least 1 of the aesthetic arguments:\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nAssigning different styles and colour palettes to each plot:\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\nDefining group-by variable in tm_facets():\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nCreating multiple stand-alone maps with tmap_arrange() as we have done above:\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n10) Mapping Spatial Object Meeting a Selection Criterion\nHere, we select the “CENTRAL REGION” within REGION_N for display:\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe end :)"
  },
  {
    "objectID": "Hands-on_Ex1.html",
    "href": "Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This is the paragraph for overview."
  },
  {
    "objectID": "Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "Getting Started",
    "text": "Getting Started\nThis is the paragraph for getting started."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "This is my second hands-on exercise for geospatial analytics! I will apply what I learnt on global and local indicators of spatial association from my second geospatial lecture on the Hunan dataset from my earlier in-class exercise."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#data",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#data",
    "title": "Hands-on Exercise 2",
    "section": "Data",
    "text": "Data\nUnzip Hands-on_Ex1, copy the data folder to C:/zhuyiting1/ISSS624/Hands-on_Ex2. It should be in the same folder as Hands-on_Ex2.qmd."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#importing-shapefile-data",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#importing-shapefile-data",
    "title": "Hands-on Exercise 2",
    "section": "Importing Shapefile Data",
    "text": "Importing Shapefile Data\nIn addition to tidyverse (which includes dplyr) and sf packages that we used earlier, we will need the spdep package for this exercise. We do it by including it in the p_load() function. As it requires the spData package, we will install and load it as well. Finally, we need the tmap package for quick plots.\n\npacman::p_load(spData, tidyverse, sf, spdep, tmap)\n\nNow we want to import the Hunan shapefile into R.\n\nhunan <- st_read(dsn = \"data/geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex2\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe can see that the data is in geodetic CRS wgs84."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#importing-attribute-data-in-csv",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#importing-attribute-data-in-csv",
    "title": "Hands-on Exercise 2",
    "section": "Importing Attribute Data in CSV",
    "text": "Importing Attribute Data in CSV\n\nAspatial Hunan Data\nThe code chunk below imports the aspatial Hunan 2012 data.\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#joining-hunan-data",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#joining-hunan-data",
    "title": "Hands-on Exercise 2",
    "section": "Joining Hunan Data",
    "text": "Joining Hunan Data\nThe code chunk below joins the spatial and aspatial data for Hunan using the left_join() function of the dplyr package.\n\nhunan <- left_join(hunan, hunan2012)\n\nR recognises the variable County to be the only common variable between the two dataframes and performs the join accordingly. As the two dataframes have the exact same number of observations with matching county, left_join() here works the same as a inner_join(), right_join() and full_join(). It appends the hunan2012 data to the right of the original hunan data. Notably, the geometry column from the original hunan dataframe remains at the rightmost column of the new hunan dataframe."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#visualising-regional-development-indicator",
    "title": "Hands-on Exercise 2",
    "section": "Visualising Regional Development Indicator",
    "text": "Visualising Regional Development Indicator\nWith some data about the Hunan province, we want to create a quick thematic map to visualise the distribution of Gross Domestic Product Per Capita (GDPPC) in 2012, using the qtm() function from the tmap package.\n\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.3, \n            legend.width = 0.7)\n  \nquantile <- tmap::tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.3, \n            legend.width = 0.7)\n\ntmap_arrange(equal, quantile, asp = 1, ncol = 2)\n\n\n\n\nWe see that the equal interval classification map shows that most regions (~60%) have GDPPC of only 1/5 of the wealthiest region. In addition, we observe that the wealth is mostly concentrated in the Northeast region of the Hunan province."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#global-spatial-autocorrelation",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#global-spatial-autocorrelation",
    "title": "Hands-on Exercise 2",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\nWe want to know whether such a distribution of GDPPC is by pure chance (randomness), and if not, whether there is a positive/negative correlation between neighbouring regions or outliers. To do this, we will compute the global spatial autocorrelation statistics and perform spatial complete randomness test for global spatial autocorrelation.\n\nContiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights are used to define the neighbouring relationships between the geographical units (i.e. counties) in the study area.\nIn the code chunk below, poly2nb() of the spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. The default option uses the Queen’s Case to define each county’s neighbours (queen=TRUE), which is what we will use here.\n\nwm_q <- poly2nb(hunan)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThere are 448 pairs of neighbours found by the Queen’s case logic (i.e. all sides and corners that are touching each other, in other words at least 1 shared boundary point is needed for “neighbour” definition), from the 88 counties in Hunan. From the summary report, the link number distributions shows the frequency of the number of links or neighbours that each county has, the most being 11 neighbours for 1 county (region 85). On the other extreme, the 2 least connected regions (30 and 65) only have 1 neighbour each.\nBy calling the County column of the hunan dataframe, we can see that the county with the most neighbours is Taoyuan and that with the least neighbours are Xinhuang and Linxiang. This is consistent with the basemap that we plotted previously, where Taoyuan is a large county in the North surrounded by multiple smaller counties, and Xinhuang and Linxiang being counties along the West and Northeast borders of Hunan respectively.\n\nhunan$County[c(85, 30, 65)]\n\n[1] \"Taoyuan\"  \"Xinhuang\" \"Linxiang\"\n\n\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"County\", size = 0.5)\nbasemap\n\n\n\n\n\n\nRow-standardised Weights Matrix\nBased on the neighbour relationship determined by Queen’s Case above, we will assign spatial weights to each pairs of counties in the form of a weights matrix. Each row and each column represent 1 of the 88 counties, forming a 88 x 88 matrix. The numbers 1 and 0 are used to indicate between each row and column pair (e.g. region 1-2 represented by the value in the first row and second column), whether they are neighbours (1) or not (0). The values along the diagonal (from top left to bottom right) is always 0 as they represent the same region (e.g. 1-1, 2-2, etc.) Such a matrix is symmetrical along the same diagonal.\nAs each region has different number of neighbours, in practice, row-standardised weights are used instead of spatial weights. Row-standardisation is done by dividing the values in each row by the sum of that row, so that the weight is a relative fraction based on the total number of neighbours that the region has (proportion by row sum). Row-standardisation weights matrix ensures that the spatial parameter in many spatial stochastic processes are comparable between models. It is applicable where polygon features are defined by administrative boundaries. While this is the most intuitive way of summarising the neighbours’ values, this approach has a limitation in that polygons along the edges of the study area will base their lagged values on fewer polygons, thus potentially over- or under-estimating the true nature of the spatial correlation in the data. More robust options such as the binary coding could be explored.\nThe code chunk below performs row standardisation for spatial weights using the nb2listw() function from the spdep package, with input being an object of class nb. The default style is “W” which is row standardisation. Other styles include “B” for basic binary coding, “C” for globally standaridsation, “U” for C divided by the number of neighbours, and “S” for variance-stablising coding scheme. For the zero.policy, we will set it to TRUE to permit the weights list to be formed with zero-length weights vector, which means that weights vector of zero length are inserted for regions without neighbour in the neighbours list. Nevertheless, from the poly2nb() function above, we know that all regions have at least 1 neighbour.\n\nrswm_q <- nb2listw(wm_q,\n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nNext, we will perform Moran’s I statistical testing using moran.test() of spdep.\n\n\nMoran’s I Statistical Testing\nThe code chunk below performs Moran’s I statistical testing. Using an upper-tailed test, the null and alternative hypotheses are as follows:\nH0: The observed spatial patterns of GDPPC in Hunan in 2012 are not clustered (i.e. either random or dispersed).\nH1: The observed spatial patterns of GDPPC in Hunan in 2012 are clustered.\n\nmoran.test(hunan$GDPPC,\n           listw = rswm_q,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nWith a p-value of < 0.05, at 5% significant level and 95% confidence level, we reject H0. Hence, there is sufficient evidence to support that the observed spatial patterns of GDPPC in Hunan in 2012 are clustered. In fact, a positive Moran’s I statistic of 0.301 supports that there is positive clustering in the GDPPC in Hunan in 2012 (i.e. counties with higher GDPPC tend to be geographically clustered/neighbours).\n\n\nMonte Carlo Moran’s I\nWhen we doubt that the assumptions of Moran’s I (i.e. normality and randomisation) are true, we can use a Monte Carlo simulation under the assumption of no spatial pattern and assigning all regions the mean value. We then compare the actual Moran’s I to that of the randomly simulated distribution to obtain the p-value (pseudo significance).\nThe code chunk below performs permutation test for Moran’s I statistics by using moran.mc() of spdep. A total of 1000 simulations will be performed with the seed number 1234.\n\nset.seed(1234)\nbperm = moran.mc(hunan$GDPPC,\n                 listw = rswm_q,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 alternative = \"greater\",\n                 na.action = na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nUsing an upper-tailed test, we see that p-value = 0.001 is still < 0.05. We similarly reject H0 and conclude that at 5% significance level, there is sufficient evidence to support that the spatial distribution of GDPPC is positively clustered in Hunan in 2012.\n\n\nVisualising Monte Carlo Moran’s I\nIn the code chunk below, we will visualise the simulated Moran’s I test statistics by plotting the distribution of the statistical values as a histogram using hist() and abline() of R Graphics.\n\nmean(bperm$res[1:1000])\n\n[1] -0.01472993\n\nvar(bperm$res[1:1000])\n\n[1] 0.004466925\n\nsummary(bperm$res[1:1000])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06167 -0.02113 -0.01473  0.02617  0.30075 \n\n\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Moran's I\")\nabline(v = 0,\n       col = \"red\")\n\n\n\n\nFrom the distribution, we see that the simulated Moran’s I values are right-skewed, with a median below the 0 reference line. The Moran’s I of 0.301 falls on the narrow right-tail of the distribution, corroborating with our earlier result of it being statistically significant.\nThe above can similarly be done using ggplot2.\n\ndf <- data.frame(bperm$res)\nggplot(df,\n       aes(x = bperm$res)) +\n  geom_histogram(binwidth = 0.02,\n                 boundary = 0,\n                 color = \"black\",\n                 fill = \"grey\") +\n  geom_vline(xintercept = 0,\n             color = \"red\") + \n  labs(title = \"Histogram of simulated Moran's I\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nGlobal Spatial Autocorrelation: Geary’s C\nIn this section, we will perform Geary’s C statistical testing by using the geary.test() function of spdep. The same null and alternative hypotheses apply.\nH0: The observed spatial patterns of GDPPC in Hunan in 2012 are not clustered (i.e. either random or dispersed).\nH1: The observed spatial patterns of GDPPC in Hunan in 2012 are clustered.\n\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nWith a p-value of < 0.05, at 5% significant level and 95% confidence level, we reject H0. Hence, there is sufficient evidence to support that the observed spatial patterns of GDPPC in Hunan in 2012 are clustered. In fact, a positive Geary’s C statistic of 0.691 < 1 (small c) supports that there is positive clustering in the GDPPC in Hunan in 2012 (i.e. counties with higher GDPPC tend to be geographically clustered/neighbours).\n\n\nMonte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic using geary.mc() of spdep. We similarly set seed number of 1234 for the simulation for reproducible results.\n\nset.seed(1234)\nbperm = geary.mc(hunan$GDPPC,\n                 listw = rswm_q,\n                 nsim = 999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe Monte Carlo simulation returned similar result of a significant p-value (0.001) at 5% significance level. Hence, we reject H0 and conclude that there is sufficient evidence to support that the spatial distribution of GDPPC in Hunan in 2012 is clustered.\n\n\nVisualising Monte Carlo Geary’s C\nLike we did for Moran’s I, we will plot a histogram to reveal the distribution of the simulated values of Geary’s C by using the following code chunks.\n\nmean(bperm$res[1:1000])\n\n[1] 1.004089\n\nvar(bperm$res[1:1000])\n\n[1] 0.007527444\n\nsummary(bperm$res[1:1000])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6907  0.9501  1.0050  1.0041  1.0594  1.2722 \n\n\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Geary's C\")\nabline(v = 1,\n       col = \"red\")\n\n\n\n\nUnlike Moran’s I, the cutoff/value indicating randomness is 1 instead of 0, as indicated by the vertical red line in the histogram above. The Geary’s C value of 0.691 is very close to the extreme left of the distribution, supporting the statistically significant result that we saw using the statistical testing."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#spatial-correlogram",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#spatial-correlogram",
    "title": "Hands-on Exercise 2",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nSpatial correlograms are used to examine patterns of spatial autocorrelation. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance. Row standardisation is not needed.\n\nMoran’s I Correlogram\nIn the code chunk below, sp.correlogram() of spdep is used to compute a 1- to 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used is Moran’s I. The plot() of R’s base Graph is used to plot the output.\n\nMI_corr <- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"I\",\n                          style = \"W\")\nplot(MI_corr)\n\n\n\n\nFrom the plot, Moran’s I is positive for up to 4 lags, beyond which Moran’s I drops below 0. It is also noted that lag of 7 or more will return empty neighbour sets, in which case we set zero.policy = TRUE in the code chunk below and try to plot up to 10 lags.\n\nMI_corr_10 <- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 10,\n                          method = \"I\",\n                          style = \"W\",\n                          zero.policy = TRUE)\nplot(MI_corr_10)\n\n\n\n\nComparing with the earlier plot, we see that the standard deviation for Moran’s I decreases from 1 to 6 lags, but increases thereafter with the introduction of zero-length weights vectors. Moran’s I also returns to a positive level from 9 lags, albeit with very large standard deviations.\nIn addition to this, it is necessary to examine the full statistical report as not all autocorrelation values are statistically significant. This is done using the print() function in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe observe that at 5% significance level, the autocorrelation values are statistically significant for all lags between 1 and 6, except 4-lag.\n\nprint(MI_corr_10)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n          estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)   0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)   0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)   0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)   0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88)  -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88)  -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n7 (83)  -0.1598792  -0.0121951  0.0031980          -2.6115        0.009014 ** \n8 (67)  -0.1016594  -0.0151515  0.0051687          -1.2033        0.228869    \n9 (45)   0.0151929  -0.0227273  0.0101956           0.3755        0.707254    \n10 (19)  0.0210858  -0.0555556  0.0266889           0.4691        0.638973    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhen we include more lags by allowing zero-length weights vector, the autocorrelation is statistically significant at 5% significant level up to 7 lags.\n\n\nGeary’s C Correlogram\nIn the code chunk below, we perform a similar analysis using the sp.correlogram() function from the spdep package, except using Geary’s C global spatial autocorrelation. We also plot the output using plot() from R’s base Graph, and print() the full report for the p-values.\n\nGC_corr <- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"C\",\n                          style = \"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see that the correlogram for Geary’s C has an opposite shape (concave) compared to that for Moran’s I (convex). This is because Moran’s I has a range of -1 to 1 and defines positive clustering with >0 values, while Geary’s C only has positive values with randomness at 1 and positive clustering between 0 and 1. In other words, larger Moran’s I implies positive clustering but larger Geary’s C implies negative clustering (dispersing).\nGeary’s C correlogram also shows larger standard deviations across lags when compared to Moran’s I. From the report, the autocorrelation values are statistically significant at 5% significance level for 1, 2 and 5 lags, less than that for Moran’s I."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 2",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal Indicator of Spatial Association (LISA) is a subset of localised geospatial statistics methods for analysing the location-related tendency (clusters or outliers) in the attributes of geographically referenced data (points or area). The LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation. The sum of LISAs for all observations is proportional to a global indicator of spatial association.\nWe will apply local Moran’s I to detect clusters and/or outliers from the 2012 GDPPC of the Hunan province.\n\nLocal Moran’s I\nThe code chunk below computes the local Moran’s I using the localmoran() function of the spdep package. We will use the row standardised weights matrix here.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nWe obtain the following statistics for the first 6 output:\n\nIi: Local Moran statistic\nE.Ii: Expectation of local Moran statistic\nVar.Ii: Variance of local Moran statistic\nZ.Ii: Standard deviate of local Moran statistic\nPr(): p-value of local Moran statistic\n\nThe code chunk below lists the content of the local Moran matrix derived using printCoefmat() function from R’s Stats package, arranged by the County name in alphabetical order.\n\nprintCoefmat(data.frame(localMI[fips,], \n                        row.names = hunan$County[fips],\n                        check.names = FALSE))\n\n                       Ii        E.Ii      Var.Ii    Z.Ii Pr(z != E(Ii))    \nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -0.0725      0.9422301    \nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791      0.1391057    \nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -0.0663      0.9471636    \nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185      0.2230456    \nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  0.1293      0.8971056    \nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -0.0768      0.9387606    \nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590      0.0007822 ***\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895      0.1119416    \nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  0.6830      0.4946117    \nCili           7.3176e-02 -1.6747e-03  4.7902e-02  0.3420      0.7323546    \nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297      0.3031703    \nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159      0.1881947    \nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338      0.1023002    \nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  0.5120      0.6086619    \nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -0.9510      0.3415864    \nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305      0.3027630    \nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  0.7793      0.4357997    \nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  0.2627      0.7928094    \nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  0.1274      0.8986110    \nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  0.4629      0.6434065    \nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -0.2562      0.7978131    \nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  0.4349      0.6636633    \nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678      0.2855921    \nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -0.0475      0.9621124    \nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  0.5407      0.5887023    \nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  0.7969      0.4255374    \nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -0.5363      0.5917276    \nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204      0.3075618    \nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  0.7180      0.4727569    \nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759      0.2396152    \nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -0.0197      0.9843090    \nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543      0.1756424    \nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  0.4497      0.6529132    \nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  0.2342      0.8148123    \nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623      0.2451020    \nLi             1.0225e-03 -2.4048e-07  5.1060e-06  0.4526      0.6508382    \nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467      0.2952490    \nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335      0.0328837 *  \nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  0.3909      0.6959021    \nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -0.1768      0.8596957    \nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  0.1363      0.8915545    \nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972      0.0051555 ** \nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787      0.0376449 *  \nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690      0.1709996    \nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  0.9956      0.3194403    \nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  0.7166      0.4736044    \nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715      0.0003550 ***\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -0.1099      0.9125016    \nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274      0.0676458 .  \nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  0.7536      0.4511108    \nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698      3.049e-05 ***\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  0.7640      0.4448892    \nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  0.6771      0.4983276    \nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -0.7181      0.4726740    \nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  0.9797      0.3272227    \nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510      0.1467765    \nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583      0.0631298 .  \nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807      0.0928305 .  \nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -0.4773      0.6331568    \nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  0.0652      0.9480354    \nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  0.8536      0.3933400    \nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544      0.0636875 .  \nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002      0.0357113 *  \nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  0.5090      0.6107279    \nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582      0.2899569    \nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873      0.0001014 ***\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935      0.0282749 *  \nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213      1.551e-05 ***\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092      0.0704213 .  \nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539      0.0507157 .  \nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  0.4241      0.6715036    \nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  0.2667      0.7897221    \nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502      0.1210854    \nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  0.1929      0.8470456    \nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -0.0068      0.9945429    \nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726      0.1698803    \nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -0.6867      0.4922880    \nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202      0.2223756    \nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795      0.1390190    \nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409      0.2538993    \nYou            7.8750e-02 -7.2728e-03  1.2116e-01  0.2471      0.8048036    \nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  0.0069      0.9944802    \nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121      0.0699726 .  \nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  0.8608      0.3893219    \nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  0.9221      0.3564539    \nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855      0.2358293    \nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688      0.0001094 ***\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -0.7014      0.4830289    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAt 5% significance level, the p-value is significant for Changsha, Liling, Liuyang, Longhui, Miluo, Pingjiang, Taojiang, Wangcheng, Wugang, Xiangtan and Zhuzhou. We will display the results in the next section.\n\n\nMapping local Moran’s I\nBefore mapping the local Moran’s I, we want to append the local Moran’s I dataframe (i.e. localMI) to the hunan SpatialPolygonDataFrame. The code chunk below does this using the cbind() function from R base which combine the vectors as columns in the final matrix. We also rename the p-value (Pr.z….E.Ii) variable title to Pr.Ii for neatness.\n\nhunan.localMI <- cbind(hunan, localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\nhunan.localMI\n\nSimple feature collection with 88 features and 40 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3 Shape_Leng Shape_Area    County\n1   Changde 21098   Anxiang      County   1.869074 0.10056190   Anxiang\n2   Changde 21100   Hanshou      County   2.360691 0.19978745   Hanshou\n3   Changde 21101    Jinshi County City   1.425620 0.05302413    Jinshi\n4   Changde 21102        Li      County   3.474325 0.18908121        Li\n5   Changde 21103     Linli      County   2.289506 0.11450357     Linli\n6   Changde 21104    Shimen      County   4.171918 0.37194707    Shimen\n7  Changsha 21109   Liuyang County City   4.060579 0.46016789   Liuyang\n8  Changsha 21110 Ningxiang      County   3.323754 0.26614198 Ningxiang\n9  Changsha 21111 Wangcheng      County   2.292093 0.13049161 Wangcheng\n10 Chenzhou 21112     Anren      County   2.240739 0.13343936     Anren\n       City avg_wage deposite     FAI Gov_Rev Gov_Exp     GDP GDPPC      GIO\n1   Changde    31935   5517.2  3541.0  243.64  1779.5 12482.0 23667   5108.9\n2   Changde    32265   7979.0  8665.0  386.13  2062.4 15788.0 20981  13491.0\n3   Changde    28692   4581.7  4777.0  373.31  1148.4  8706.9 34592  10935.0\n4   Changde    32541  13487.0 16066.0  709.61  2459.5 20322.0 24473  18402.0\n5   Changde    32667    564.1  7781.2  336.86  1538.7 10355.0 25554   8214.0\n6   Changde    33261   8334.4 10531.0  548.33  2178.8 16293.0 27137  17795.0\n7  Changsha    40446  21415.0 43599.0 2473.10  4605.5 81113.0 63118  99254.0\n8  Changsha    40744  18662.0 49234.0 2448.90  4812.2 73250.0 62202 114145.0\n9  Changsha    45171  12122.0 48829.0 2285.50  3802.3 37488.0 70666 148976.0\n10 Chenzhou    28058   4598.9  6386.1  220.57  1454.7  4941.2 12761   4189.2\n      Loan   NIPCR  Bed    Emp  EmpR EmpRT Pri_Stu Sec_Stu Household\n1   2806.9  7693.7 1931 336.39 270.5 205.9  19.584  17.819     148.1\n2   4550.0  8269.9 2560 456.78 388.8 246.7  42.097  33.029     240.2\n3   2242.0  8169.9  848 122.78  82.1  61.7   8.723   7.592      81.9\n4   6748.0  8377.0 2038 513.44 426.8 227.1  38.975  33.938     268.5\n5    358.0  8143.1 1440 307.36 272.2 100.8  23.286  18.943     129.1\n6   6026.5  6156.0 2502 392.05 329.6 193.8  29.245  26.104     190.6\n7  23408.0 15719.0 6225 919.62 721.4 300.1  90.978  58.819     374.8\n8  18435.0 13763.0 4351 852.96 757.6 318.3  80.715  68.853     391.7\n9  10330.0 16495.0 1678 361.48 268.6 131.2  28.838  24.815     161.3\n10  2555.3  3271.8  970 290.82 255.4  99.4  33.171  17.505     104.6\n   Household_R NOIP Pop_R    RSCG  Pop_T      Agri Service Disp_Inc      RORP\n1        135.4   53 346.0  3957.9  528.3  4524.410 14100.0    16610 0.6549309\n2        208.7   95 553.2  4460.5  804.6  6545.350 17727.0    18925 0.6875466\n3         43.7   77  92.4  3683.0  251.8  2562.460  7525.0    19498 0.3669579\n4        256.0   96 539.7  7110.2  832.5  7562.340 53160.0    18985 0.6482883\n5        157.2   99 246.6  3604.9  409.3  3583.910  7031.0    18604 0.6024921\n6        184.7  122 399.2  6490.7  600.5  5266.510  6981.0    19275 0.6647794\n7        369.8  733 642.7 16233.0 1285.5 10844.470 26617.8    27345 0.4999611\n8        369.6  552 655.5 15623.0 1186.5 12804.480 18447.7    24020 0.5524652\n9        154.8  314 266.6  5623.3  533.4  5222.356  6648.6    27690 0.4998125\n10       121.9   34 243.2  2386.4  388.7  2357.764  3814.1    16072 0.6256753\n      ROREmp           Ii          E.Ii       Var.Ii        Z.Ii        Pr.Ii\n1  0.8041262 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904 0.9471636332\n2  0.8511756  0.025878173 -6.061953e-04 1.016664e-02  0.26266425 0.7928093714\n3  0.6686757 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705 0.9843089778\n4  0.8312558  0.001022468 -2.404783e-07 5.105969e-06  0.45259801 0.6508382339\n5  0.8856065  0.014814881 -6.829362e-05 1.449949e-03  0.39085814 0.6959020959\n6  0.8407091 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835 0.6331568039\n7  0.7844544  3.368821673 -7.750185e-02 1.518028e+00  2.79715225 0.0051555232\n8  0.8882011  1.560689600 -7.387766e-02 8.001247e-01  1.82735933 0.0676457604\n9  0.7430563  4.421958618 -1.106694e-01 1.359593e+00  3.88727819 0.0001013746\n10 0.8782065 -0.399322576 -7.011066e-03 7.034768e-02 -1.47912938 0.1391057404\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\nThe code chunk below plots the local Moran’s I values and their statistical significance (based on p-values) using the choropleth mapping functions from the tmap package.\n\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\",\n          breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette = \"-Blues\",\n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp = 1, ncol = 2)\n\n\n\n\nOn the left, we note regions of positive (blue) and negative (orange) Moran’s I statistics, indicative of positive and negative clustering relationships. On the right, we see that the p-values are significant at 5% significance level for the regions in darker shades of blue. They generally correspond to regions with high positive local Moran’s I statistics. The negative clustering region (high region surrounded by low neighbours) is not statistically significant as it only has 3 neighbours by contiguity weights matrix."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 2",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations colour-coded by type of spatial autocorrelation. The first step is to plot the Moran scatterplot.\n\nPlotting Moran Scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attributes at each location and the average value of the same attribute at neighbouring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC,\n                  rswm_q,\n                  labels = as.character(hunan$County),\n                  xlab = \"GDPPC 2012\",\n                  ylab = \"Spatially lagged GDPPC 2012\")\n\n\n\n\nThe Moran scatterplot can be interpreted by the 4 quadrants:\n\nTop-right (high-high): Positive autocorrelation, i.e. clusters (the region and its neighbours all have high values)\nBottom-left (low-low): Positive autocorrelation, i.e. clusters (the region and its neighbours all have low values)\nTop-left (low-high): Negative autocorrelation, i.e. outlier (low outlier among high neighbours)\nBottom-right (high-low): Negative autocorrelation, i.e. outlier (high outlier among low neighbours)\n\nWe see that most regions follow a cluster autocorrelation pattern rather than outlier pattern."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#plotting-moran-scatterplot-with-standardised-variables",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#plotting-moran-scatterplot-with-standardised-variables",
    "title": "Hands-on Exercise 2",
    "section": "Plotting Moran Scatterplot with Standardised Variables",
    "text": "Plotting Moran Scatterplot with Standardised Variables\nFirst, we use scale() from base R to centre and scale the variables. Here, centering is done by subtracting the mean (omitting NAs) from the corresponding columns, and scaling is done by dividing the (centred) variables by their standard deviations. The as.vector() from the pbdDMAT package added at the end of the code chunk below is to ensure that the data type for hunan$Z.GDPPC is a non-distributed vector instead of a distributed matrix. This is so that we can then append it to our dataframe later.\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% as.vector\n\nPlotting the Moran scatterplot again using the code chunk below, this time as nci2.\n\nnci2 <- moran.plot(hunan$Z.GDPPC,\n                   rswm_q,\n                   labels = as.character(hunan$County),\n                   xlab = \"z-GDPPC 2012\",\n                   ylab = \"Spatially lagged z-GDPPC 2012\")\n\n\n\n\nWe see that the x- and y-axes are scaled to 0 (for the division of the 4 quadrants).\n\nPreparing LISA Map Classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant <- vector(mode = \"numeric\",\n                   length = nrow(localMI))\n\nNext, we centre the variable of interest around its mean.\n\nDV <- hunan$GDPPC - mean(hunan$GDPPC)\n\nThis is followed by centering the local Moran’s I around its mean. This is for consistency with the DV method, and it is sufficient to simply use the local Moran’s I value without centering it around the mean (i.e. the code chunk below works the same as C_mI <- localMI[, 1]).\n\nC_mI <- localMI[, 1] - mean(localMI[, 1])\n\nThen, we set a statistical significance level for local Moran’s at 5%.\n\nsignif <- 0.05\n\nThe next 4 command lines define the high-high, low-low, low-high and high-low quadrants.\n\nquadrant[DV < 0 & C_mI > 0] <- 1 # C_mi > 0 -> cluster // DV refers to GDPPC wrt mean -> -ve means low-low\nquadrant[DV < 0 & C_mI < 0] <- 2 # C_mi < 0 -> outlier\nquadrant[DV > 0 & C_mI < 0] <- 3 # C_mi < 0 -> outlier\nquadrant[DV > 0 & C_mI > 0] <- 4 # C_mi > 0 -> cluster\n\nFinally, we place the non-significant Moran’s value in category 0.\n\nquadrant[localMI[,5] > signif] <- 0\n\nIn fact, we can combine all the steps into a single code chunk below.\n\nquadrant <- vector(mode = \"numeric\",\n                   length = nrow(localMI))\nDV <- hunan$GDPPC - mean(hunan$GDPPC)\nC_mI <- localMI[, 1] - mean(localMI[, 1])\nsignif <- 0.05\nquadrant[DV < 0 & C_mI > 0] <- 1\nquadrant[DV < 0 & C_mI < 0] <- 2\nquadrant[DV > 0 & C_mI < 0] <- 3\nquadrant[DV > 0 & C_mI > 0] <- 4\nquadrant[localMI[,5] > signif] <- 0\n\n\n\nPlotting LISA Map\nFinally, we can build the LISA map using the code chunk below.\n\ngdppc <- qtm(hunan, \"GDPPC\", fill.palette = \"Blues\")\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", # white for non-significant Moran's values\n            \"#f1fa73\", # yellow for low-low\n            \"#91fa5c\", # green for low-high\n            \"#5cfacb\", # cyan for high-low\n            \"#1239ff\") # blue for high-high\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, asp=1, ncol=2)\n\n\n\n\nThe plot on the right shows that the statistically significant Moran’s I values are in blue for high-high autocorrelation and yellow for low-low autocorrelation (clusters). These regions are all found on the East side of the Hunan province, and they generally correspond to the higher GDPPC regions (see plot on the left). It also shows 3 outlier regions in green and cyan."
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 2",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBesides detecting clusters and outliers, localised spatial statistics can also be used to detect hot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings.\n\nGetis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics. It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nDeriving Distance-based Weights Matrix\nFirst, we need to define a new set of neighbours based on distance for Getis-Ord, instead of shared borders used for spatial autocorrelation.\nThere are two types of distance-based proximit matrix, namely:\n\nFixed distance weights matrix; and\nAdaptive distance weights matrix.\n\n\n\nDeriving the Centroid\nDistance-based weights matrix requires the centroids of the polygons to be determined.\nTo do this, we need the coordinates in a separate dataframe using a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of the sf object hunan. The function used is st_centroid() from sf package. We will use map_dbl() variation of map() from the purrr package.\nTo get our longitude values we map the st_centroid() function over the geometry column of hunan and access the longitude value through double bracket notation [[ ]] and value 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]] for latitude.\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind() to put longitude and latitude into the same object.\n\ncoords <- cbind(longitude, latitude)\ncoords\n\n      longitude latitude\n [1,]  112.1531 29.44362\n [2,]  112.0372 28.86489\n [3,]  111.8917 29.47107\n [4,]  111.7031 29.74499\n [5,]  111.6138 29.49258\n [6,]  111.0341 29.79863\n [7,]  113.7065 28.23215\n [8,]  112.3460 28.13081\n [9,]  112.8169 28.28918\n[10,]  113.3534 26.57906\n[11,]  113.8942 25.98122\n[12,]  112.4006 25.63215\n[13,]  112.5542 25.33880\n[14,]  113.6636 25.54967\n[15,]  112.9206 25.26722\n[16,]  113.1883 26.21248\n[17,]  113.4521 25.93480\n[18,]  112.4209 26.36132\n[19,]  113.0152 27.08120\n[20,]  112.6350 26.75969\n[21,]  112.7087 27.27930\n[22,]  112.9095 26.42079\n[23,]  111.9522 26.80117\n[24,]  110.2606 27.89384\n[25,]  110.0921 27.54115\n[26,]  109.7985 26.91321\n[27,]  109.5765 26.54507\n[28,]  109.7211 27.78801\n[29,]  109.7339 26.21157\n[30,]  109.1537 27.22941\n[31,]  110.6442 27.83407\n[32,]  110.5916 28.57282\n[33,]  109.5984 27.39828\n[34,]  111.4783 27.67997\n[35,]  112.1745 27.46256\n[36,]  111.2315 27.86930\n[37,]  110.3149 26.32113\n[38,]  111.3248 26.48991\n[39,]  110.5859 27.10164\n[40,]  110.9593 27.34884\n[41,]  111.8296 27.18765\n[42,]  110.1926 26.70972\n[43,]  110.7334 26.78494\n[44,]  110.9123 26.54354\n[45,]  111.4599 27.42910\n[46,]  112.5268 27.92456\n[47,]  112.3406 27.77407\n[48,]  109.5602 28.66808\n[49,]  109.5071 28.01142\n[50,]  109.9954 28.60033\n[51,]  109.4273 28.42749\n[52,]  109.7587 28.31518\n[53,]  109.5044 29.21940\n[54,]  109.9899 28.16053\n[55,]  109.9664 29.01206\n[56,]  111.3785 28.28449\n[57,]  112.4350 29.23817\n[58,]  112.5558 28.97135\n[59,]  111.7379 24.97087\n[60,]  112.1831 25.31559\n[61,]  111.9743 25.65101\n[62,]  111.7009 25.91101\n[63,]  112.2196 25.88615\n[64,]  112.6472 29.48614\n[65,]  113.5102 29.49285\n[66,]  113.1172 28.79707\n[67,]  113.7089 28.76024\n[68,]  112.7963 28.71653\n[69,]  110.9276 29.39439\n[70,]  113.6420 26.80361\n[71,]  113.4577 27.66123\n[72,]  113.8404 26.37989\n[73,]  113.4758 27.17064\n[74,]  113.1428 27.62875\n[75,]  110.3017 29.39053\n[76,]  113.1957 29.25343\n[77,]  111.7410 26.36035\n[78,]  112.1831 28.49854\n[79,]  111.3390 27.01465\n[80,]  111.8208 27.75124\n[81,]  110.0753 27.23539\n[82,]  112.3965 27.08323\n[83,]  112.7683 25.82828\n[84,]  113.1679 28.30074\n[85,]  111.4495 28.95406\n[86,]  112.7956 27.68910\n[87,]  111.5896 25.49530\n[88,]  111.2393 25.19355\n\n\ncoords now has 2 columns to indicate the longitude and latitude for the centroid of each region.\n\n\nDetermining Cut-off Distance\nWe want to determine the upper limit for distance band using the following steps:\n\nReturn a matrix with indices of points belonging to the set of k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids using knn2nb() of spdep.\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, and in km otherwise. As the Hunan data was in geodesic CRS, the unit will be in km.\nRemove the list structure of the returned object by using unlist() of base R.\n\n\nk1 <- knn2nb(knearneigh(coords, k = 1))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79km. We will use this as the upper threshold to ensure that all units will have at least 1 neighbour.\n\n\nComputing Fixed Distance Weights Matrix\nNow, we will compute the distance weights matrix by using dnearneigh() of spdep.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n324 links were made between the 88 regions.\nNext, nb2listw() of spdep is used to convert the nb object into spatial weights object. A binary style is used to indicate whether a pair of region is neighbours (1) or not (0).\n\nwm62_lw <- nb2listw(wm_d62, style = \"B\")\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nHere, we see that 6 regions only have 1 neighbour each, while the maximum number of neighbours defined by 62km distance between centroids is 6, compared to the earlier definition by Queen’s Case of 11.\n\n\nComputing Adaptive Distance Weights Matrix\nOne of the characteristics of fixed distance weights matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours than the less densely settled areas (usually the rural areas). Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the number of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry. In this case, we will set all regions to have 8 neighbours each (k = 8). Such an adaptive weighting scheme will use shorter distances or bandwidths where data are dense and longer where data are sparse.\n\nknn <- knn2nb(knearneigh(coords, k = 8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nSimilarly, nb2listw() of spdep is used to convert the nb object into spatial weights object in binary style.\n\nknn_lw <- nb2listw(knn, style = \"B\")\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex2/Hands-on_Ex2.html#computing-gi-statistics",
    "href": "Hands-on_Ex2/Hands-on_Ex2.html#computing-gi-statistics",
    "title": "Hands-on Exercise 2",
    "section": "Computing Gi Statistics",
    "text": "Computing Gi Statistics\n\nGi Statistics Using Fixed Distance\nThe code chunk below calculates the Gi statistics of the 88 regions using the fixed distance weights matrix.\n\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf dataframe by using the code chunk below.\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nThe code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\nGi Statistics Using AdaptiveDistance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knn_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan.gi, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\nMapping Gi Values with Fixed and Adaptive Distance Weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed and adaptive distance weights matrix.\n\ngdppc <- qtm(hunan, \"GDPPC\", fill.palette = \"Blues\", title = \"GDPPC 2012\")\n\n\nGimap.fixed <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_fixed\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Fixed distance weights matrix\",\n            main.title.size = 0.9,\n            main.title.position = \"center\")\n\nGimap.adaptive <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Adaptive distance weights matrix\",\n            main.title.size = 0.9,\n            main.title.position = \"center\")\n\ntmap_arrange(gdppc,\n           Gimap.fixed,\n           Gimap.adaptive,\n           asp = 1,\n           ncol = 3)\n\n\n\n\nOverall, fixed distance method works well for point data. It is often a good option for polygon data when there is large variation in polygon size, and a consistent scale of analysis is desired. Adaptive distance or k-nearest neighbours method, on the other hand, is effective when we want to ensure a minimum number of neighbours in the analysis. This is especially when the values associated with the features are skewed (i.e. not normally distributed), and as a rule of thumb we want to evaluate each feature within the context of at least 8 neighbours.\nWe saw earlier from the histogram of the simulated Moran’s I values that there is a slight right-skew. Here, we observe that the fixed distance method show positive clusters around in the high GDPPC 2012 regions, and negative clusters generally in the West part of Hunan. The adaptive distance method shows a wider area for strong positive clusters in the same high GDPPC 2012 regions, and a more negative cluster in the Southwest region."
  },
  {
    "objectID": "Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "This exercise aims to delineate homogeneous regions by using geographically referenced multivariate data. There are two main analyses, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\n\nConvert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package.\nConvert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate functions of sf package.\nPerform cluster analysis using hclust() of Base R.\nPerform spatially constrained cluster analysis using skater() of Base R.\nVisualise analysis output using ggplot2 and tmap packages.\n\n\n\n\n\n\n\nIn geobusiness and spatial policy, it is common practice to delineate the market or planning areas into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar, into homogeneous regions by using multiple Information and Community Technology (ICT) measures, namely: Radio, Telephone, Land line phone, Mobile phone, Computer and Internet at home.\n\n\n\nTwo data sets will be used in this study:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries). This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv. This is an extract of the 2014 Myanmar Population and Housing Census Myanmar, at the township level.\n\nBoth data sets were downloaded from Myanmar Information Management Unit (MIMU).\n\n\n\n\nWe will prepare our R environment by installing and loading the necessary R packages, using p_load() function of pacman package.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling - sf, rgdal and spdep\nAttribute data handling - tidyverse, specifically readr, ggplot2 and dplyr\nChoropleth mapping - tmap\nMultivariate data visualisation and analysis - coorplot, ggpubr, factoextra and heatmaply\nCluster analysis - cluster, NbClust, ClustGeoand psych\n\nThe code chunk below installs and loads the required R packages into the R environment.\n\npacman::p_load(rgdal, spdep, tmap, sf, \n               ggpubr, cluster, factoextra, NbClust, \n               heatmaply, corrplot, psych, tidyverse)\n\n\n\n\n\n\nIn this section, we import Myanmar Township Boundary GIS data and its associated attribute table into the R environment.\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. We can import it into the R environment using st_read() of sf. We also use the piping function %>% from dplyr and perform filter() to extract only the data for the Shan state. This is done in the code chunk below.\n\nshan_sf <- st_read(dsn = \"data/geospatial\",\n                   layer = \"myanmar_township_boundaries\") %>% \n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex3\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported Myanmar township boundary object is assigned to shan_sf. It is saved in simple feature data.frame format. We see that the full shapefile before filtering for Shan state has 330 features and 14 fields, which should include multipolygon geometry as its last field based on the geometry type displayed. Looking at the Environment pane, we see that shan_sf is filtered down to 55 observations. The bounding box shows the range of x and y, which are between 9 and 102 and suggests that the data is in decimal degrees format. The coordinate reference system (CRS) used is geographic, in wgs84.\nWe can view the content of shan_sf simple features data.frame by calling it directly, using the code chunk below.\n\nshan_sf\n\nSimple feature collection with 55 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   OBJECTID           ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1       163 Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2       203 Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3       240 Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4       106 Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5        72 Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6        40 Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7       194 Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8       159 Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9        61 Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10      124 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                 ST_2            LABEL2 SELF_ADMIN ST_RG T_NAME_WIN T_NAME_M3\n1  Shan State (North)    Mongmit\\n61072       <NA> State   rdk;rdwf      မိုးမိတ်\n2  Shan State (South)    Pindaya\\n77769       Danu State     yif;w,     ပင်းတယ\n3  Shan State (South)    Ywangan\\n76933       Danu State      &GmiH       ရွာငံ\n4  Shan State (South)  Pinlaung\\n162537       Pa-O State  yifavmif;   ပင်လောင်း\n5  Shan State (North)     Mabein\\n35718       <NA> State     rbdrf;      မဘိမ်း\n6  Shan State (South)     Kalaw\\n163138       <NA> State       uavm      ကလော\n7  Shan State (South)      Pekon\\n94226       <NA> State     z,fcHk       ဖယ်ခုံ\n8  Shan State (South)          Lawksawk       <NA> State   &yfapmuf    ရပ်စောက်\n9  Shan State (North) Nawnghkio\\n128357       <NA> State  aemifcsdK    နောင်ချို\n10 Shan State (North)   Kyaukme\\n172874       <NA> State   ausmufrJ    ကျောက်မဲ\n       AREA                       geometry\n1  2703.611 MULTIPOLYGON (((96.96001 23...\n2   629.025 MULTIPOLYGON (((96.7731 21....\n3  2984.377 MULTIPOLYGON (((96.78483 21...\n4  3396.963 MULTIPOLYGON (((96.49518 20...\n5  5034.413 MULTIPOLYGON (((96.66306 24...\n6  1456.624 MULTIPOLYGON (((96.49518 20...\n7  2073.513 MULTIPOLYGON (((97.14738 19...\n8  5145.659 MULTIPOLYGON (((96.94981 22...\n9  3271.537 MULTIPOLYGON (((96.75648 22...\n10 3920.869 MULTIPOLYGON (((96.95498 22...\n\n\nAs the sf.data.frame conforms to Hardy Wickham’s tidy framework, we can use glimpse() of dplyr to check the data types of its fields.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 15\n$ OBJECTID   <dbl> 163, 203, 240, 106, 72, 40, 194, 159, 61, 124, 71, 155, 101…\n$ ST         <chr> \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (Sout…\n$ ST_PCODE   <chr> \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\",…\n$ DT         <chr> \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"…\n$ DT_PCODE   <chr> \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MM…\n$ TS         <chr> \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kal…\n$ TS_PCODE   <chr> \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR015…\n$ ST_2       <chr> \"Shan State (North)\", \"Shan State (South)\", \"Shan State (So…\n$ LABEL2     <chr> \"Mongmit\\n61072\", \"Pindaya\\n77769\", \"Ywangan\\n76933\", \"Pinl…\n$ SELF_ADMIN <chr> NA, \"Danu\", \"Danu\", \"Pa-O\", NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ST_RG      <chr> \"State\", \"State\", \"State\", \"State\", \"State\", \"State\", \"Stat…\n$ T_NAME_WIN <chr> \"rdk;rdwf\", \"yif;w,\", \"&GmiH\", \"yifavmif;\", \"rbdrf;\", \"uavm…\n$ T_NAME_M3  <chr> \"မိုးမိတ်\", \"ပင်းတယ\", \"ရွာငံ\", \"ပင်လောင်း\", \"မဘိမ်း\", \"ကလော\", \"ဖယ်ခုံ\", \"…\n$ AREA       <dbl> 2703.611, 629.025, 2984.377, 3396.963, 5034.413, 1456.624, …\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (…\n\n\n\n\n\nThe csv file is imported using read_csv() of readr using the code chunk below.\n\nict <- read_csv(\"data/aspatial/Shan-ICT.csv\")\n\nThe imported attribute data set is saved in R’s tibble data.frame format and named ict. We see that it also has 55 observations, and 11 columns which include the number of households for each ICT variable from the 2014 Myanmar Population and Housing Census Myanmar.\nThe code chunk below uses summary() of Base R to show the summary statistics for each of the 11 variables in the ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThe highest number among the maximum values is 62,388 households for television, which is consistent with our knowledge of it being a common source of entertainment and communication method in this era.\n\n\n\nThe unit of measurement of the values are the number of households. Using these values directly will be biased by the underlying total number of households. In general, the townships with relative higher total number of households will also have higher absolute number of households owning radio, television, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable, using the mutate() function of dplyr to create new columns as shown in the code chunk below. We also use rename() of dplyr to make the variable names more easily understood and when using them in the mutate() functions.\n\nict_derived <- ict %>%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%\n  rename(`DT_PCODE`=`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nWe can see that the new variables are appended to theict data.frame in ict_derived, using the summary() function of Base R.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nThe derived proportions range between 1.041 for the Internet and 842.4 for television.\n\n\n\nWe combine both sets of data into a single data.frame using left_join() of dplyr, which appends the second data.frame to the first based on the observations in the first. The shan_sf simple feature data.frame will be used as the base data object, and the ict_derived data.frame will be used as the join table.\nAs there are 4 variables which are common across both data sets, namely DT, DT_PCODE, TS, and TS_PCODE, we will specify the criteria for joining, in this case by TS_PCODE. We do so using the code chunk below.\n\nshan_sf <- left_join(shan_sf, \n                     ict_derived, \n                     by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n\n\n\n\n\n\n\nWe plot the distribution of variables (i.e. number of households with radio) using appropriate Exploratory Data Analysis (EDA) using the combination ggplot() (to initialise a ggplot object) with geom_histogram() (to plot a histogram) of ggplot2 as shown in the code chunk below.\n\nggplot(data = shan_sf, \n       aes(x = `RADIO`)) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\")\n\n\n\n\nSeeing the distribution being right-skewed and having counts that are far out to the right and disjoint from the rest of the frequency bars, we also use geom_boxplot() of ggplot2 to plot a boxplot, which is useful for detecting outliers which are displayed as individual points outside of the box-and-whiskers.\n\nggplot(data = shan_sf,\n       aes(x = `RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\nWe see 3 individual dots displayed outside of the box-and-whiskers, signifying 3 outliers beyond 12,500 households.\nNext, we also plot the distribution of the newly derived variable (i.e. Radio penetration rate (PR)) by using the code chunk below.\n\nggplot(data = shan_sf, \n       aes(x = `RADIO_PR`)) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\")\n\n\n\n\nWe see that the Radio PR appears more normal with less outliers, but still slightly right-skewed.\nWe similarly plot the boxplot for Radio PR using the code chunk below.\n\nggplot(data = shan_sf, \n       aes(x = `RADIO_PR`)) +\n  geom_boxplot(color = \"black\", \n               fill = \"light blue\")\n\n\n\n\nWe can repeat the process for all 6 modes of ICT in the data, and combine them into a single figure using ggarrange() of ggpubr to group and arrange the plots (in a similar way to tmap_arrange() of tmap, except for ggplots instead of tmaps).\n\nradio <- ggplot(data=shan_sf, \n             aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv <- ggplot(data=shan_sf, \n             aes(x=`TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone <- ggplot(data=shan_sf, \n             aes(x=`LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone <- ggplot(data=shan_sf, \n             aes(x=`MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer <- ggplot(data=shan_sf, \n             aes(x=`COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet <- ggplot(data=shan_sf, \n             aes(x=`INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\nFrom the histograms, we see that all ICT PRs are right-skewed with potential outliers, except TV PR which is left-skewed. We confirm this using boxplots.\n\nradio <- ggplot(data=shan_sf, \n             aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\ntv <- ggplot(data=shan_sf, \n             aes(x=`TV_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\nllphone <- ggplot(data=shan_sf, \n             aes(x=`LLPHONE_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\nmphone <- ggplot(data=shan_sf, \n             aes(x=`MPHONE_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\ncomputer <- ggplot(data=shan_sf, \n             aes(x=`COMPUTER_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\ninternet <- ggplot(data=shan_sf, \n             aes(x=`INTERNET_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\nIt turns out that TV PR has outliers on both the left and right of the box-and-whiskers. In addition, for mobile phone PR, there is no outlier.\n\n\n\nTo have a quick look at the distribution of Radio PR of Shan State at township level, we will use a choropleth map.\nThe code chunk below plots a choropleth map using qtm() of tmap. This is a quick and easy way to get a quick view of the spatial distribution, coloured by 5 equal intervals, by simply specifying (i) the simple feature data.frame and (ii) the variable by which to colour the map.\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\nWe see darker regions of Radio PR in the extreme Northwest, along the West side and in the Southeast side of Shan State.\nIn order to reveal that the distribution shown in the choropleth map above is biased to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLD.map) and the other for the total number of households with Radio (RADIO.map), using\n\ntm_shape()- to plot the geometries of the sf data.frame,\ntm_fill() - to specify the variable by which the polygons are coloured, number of intervals, types of intervals (e.g. equal, quantile), displayed title of the legend, and display of histogram of the values of variable plotted,\ntm_borders() - to customise the border properties such as transparency,\ntm_layout() - to customise the plot layout such as the title of plot, size of legend, position of title and legend, and\ntmap_arrange() - to plot multiple tmaps\n\nof tmap in the code chunk below.\n\nTT_HOUSEHOLDS.map <- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\",\n          legend.hist = TRUE) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Total number of households\",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.outside = TRUE)\n\nRADIO.map <- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \",\n          legend.hist = TRUE) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Number of households with Radio\",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.outside = TRUE)\n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\nWe use style = \"jenks\" which identifies groups of similar values in the data, and maximises the difference between categories.\nWe note that the choropleth maps above show that townships with relatively larger number of households are also showing relatively higher number of Radio ownership, as seen in the darker coloured regions.\nWe plot the choropleth maps showing the distribution of the total number of households and Radio PR using the code chunk below. Here we use tm_polygons(), tm_facets() and tm_legend() instead from tmap to plot the two maps side-by-side easily.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\nHere, we see that the Radio PR is no longer directly correlated with the total number of households. In particular, we see that there are dark red regions in the extreme Northwest, in the Southwest and Southeast regions in the Radio PR plot on the right, but are yellow or light orange in the total households plot on the left. This removes the bias from the underlying household distribution!\n\n\n\n\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated. This is because if highly correlated variables are used for cluster analysis, specific aspects covered by these variables will be over-represented in the clustering solution. In this regard, absolute correlation (r) above 0.85 are problematic and should be avoided by removing either one of the pair of highly correlated variables.\nWe use corrplot.mixed() of corrplot to visualise and analyse the correlation of the input variables, using the code chunk below.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated (r = 0.87 > 0.85). This suggest that only one of them should be used in the cluster analysis instead of both. In this exercise, we will use COMPUTER_PR instead of INTERNET_PR as the former provides direct output and interaction with users, similar to other ICT variables in this data set.\n\n\n\n\n\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars <- shan_sf %>%\n  st_set_geometry(NULL) %>%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNext, we need to change the rows by township name instead of row number by using the code chunk below.\n\nrow.names(cluster_vars) <- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNow, we will delete the TS.x field which is a duplicate of the row names now, by using the code chunk below.\n\nshan_ict <- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\n\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis. Some common variable standardisation techniques are:\n\nZ-score - transforms normal variants to standard score form;\nMin-Max - transforms data to a value between 0 and 1; and\nDecimal Scaling - normalises by moving the decimal points of the maximum value of the variable to <1.\n\n\n\n\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method, and then displays the summary statistics of the clustering variables using describe() of psych to display standard deviation.\n\nshan_ict.z <- scale(shan_ict)\npsych::describe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nThe mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively by virtue of Z-score standardisation.\nNote that Z-score standardisation method should only be use if all variables come from normal distribution.\n\n\n\nIn the code chunk below, normalize() of heatmaply is used to stadardise the clustering variables by using Min-Max method. The summary() of Base R is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std <- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nWe see that the value range of the Min-Max standarised clustering variables is now within 0 and 1 (inclusive).\n\n\n\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Non-standardised\")\n\nshan_ict_s_df <- as.data.frame(shan_ict.std)\ns <- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df <- as.data.frame(shan_ict.z)\nz <- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\nWe see that before standardisation, the distribution appears to be right-skewed, hence Z-score standardisation may not be the best option if we wish to use standardised data. Nevertheless, the general shape of the distribution is similar across both standardisation methods.\nNotice that the overall distribution of the clustering variables will change after the data standardisation. Hence, it is advisible NOT to perform data standardisation if the values range of the clustering variables are not very large. In this case, as we earlier saw that the PRs of all ICTs analysed are in the scale of 0-100, we will proceed with clustering analysis using the non-standardised PRs.\nOn the other hand, it is advisable that heavily skewed data are first transformed to a more symmetric distribution. The general variable transformation techniques are:\n\nLog transformation - taking the natural log of the clustering variables;\nSquare root - taking the square root of the variables; and\nWindorising - replacing all values greater than x percentile with the xth percentile and values less than (100-x)th percentile with the values of the (100-x)th percentile. This reduces the effects of outliers on statistical analyses, particularly hierarchical clustering which is sensitive to noise and outliers.\n\nIn this exercise, we see that the clustering variables are neither heavily skewed (only a slight right-skew for most ICT PRs) nor have many extreme outliers (up to 3). Hence, we will not perform variable transformation.\n\n\n\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R stats.\ndist() supports six distance proximity calculations, they are: euclidean,maximum,manhattan,canberra,binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat <- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection. As the full list by township is long, we will not display it here and you can try it yourself. :)\n\nproxmat\n\n\n\n\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward <- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(1234)\ngap_stat <- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.677830 0.2707006 0.03692273\n [2,] 8.130029 8.346462 0.2164322 0.04088387\n [3,] 7.992265 8.200253 0.2079877 0.03762167\n [4,] 7.862224 8.079170 0.2169462 0.04018998\n [5,] 7.756461 7.977981 0.2215201 0.04229538\n [6,] 7.665594 7.890134 0.2245409 0.04501316\n [7,] 7.590919 7.812990 0.2220709 0.04364077\n [8,] 7.526680 7.739537 0.2128575 0.04477188\n [9,] 7.458024 7.670476 0.2124519 0.04623855\n[10,] 7.377412 7.603947 0.2265346 0.04762720\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\n\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n\n\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat <- data.matrix(shan_ict)\n\n\n\n\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\n\n\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of Base R will be used in the code chunk below to derive a 6-cluster model.\n\ngroups <- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename() of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.\n\n\n\n\nIn this section, we will derive spatially constrained cluster by using skater() method of spdep package.\n\n\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp <- as_Spatial(shan_sf)\n\n\n\n\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list. In the code chunk below, the default Queen’s contiguity method is used.\n\nshan.nb <- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nThere are 264 pairs of neighbours defined based on the definition of at least 1 shared boundary point, among the 55 towns in Shan State.\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan State township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(shan_sp, \n     border=grey(.5))\nplot(shan.nb, \n     coordinates(shan_sp), \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\nNote that when plotting the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts <- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style=\"B\" to make sure the cost values are not row-standardised.\n\nshan.w <- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst <- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\n\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nR tells us that it is a 54 by 3 matrix. The dimension is 1 less than the total number of towns of 55 in Shan State, because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   31   25 229.44658\n[2,]   25   10 163.95741\n[3,]   10    1 144.02475\n[4,]   10    9 157.04230\n[5,]    9    8  90.82891\n[6,]    8    6 140.01101\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(shan_sp, border=gray(.5))\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 <- skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using str() of utils to display the internal structure of clust6, using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 31 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 31 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the groups argument of clust6 in the code chunk below.\n\nccs6 <- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetically, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nFinally, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(shan_sp, border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat <- as.matrix(clust6$groups)\nshan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\nFor easy comparison, we place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map <- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map <- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\n\n\n\nThe difference between the SKATER approach versus hierarchical clustering is clear - the former takes into account the spatial configuration of the towns and cluster those closer in proximity (neighbours) as seen on the plot on the right, while the former shows clusters that are all over the place. This allows us to further study the ICT PRs in Shan State and advice on practical questions in relation to them."
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "Geographically-weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we learn how to build hedonic pricing models by using GWR methods. The dependent variable in this exercise is the resale prices of condominium in Singapore in 2015. The independent variables are divided into either structural or locational."
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#data",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#data",
    "title": "Hands-on Exercise 4",
    "section": "2. Data",
    "text": "2. Data\nTwo data sets are used in this model-building exercise. They are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL); and\n2015 condominium resale data in csv format (i.e. Condo_resale_2015.csv)"
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#r-packages",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#r-packages",
    "title": "Hands-on Exercise 4",
    "section": "3. R Packages",
    "text": "3. R Packages\nWe start by setting up the necessary R packages in the R environment by installing and loading them. The R packages that are used in this exercise are:\n\nsf - Spatial data handling;\ntidyverse - Attribute data handling including reading csv file (readr), variable creation (ggplot2) and creating statistical plots (dplyr);\nggpubr - Combining multiple statistical plots;\ntmap - Plotting choropleth map;\nspdep - Creating neighbour list and matrix;\ncorrplot - Conducting multivariate data visualisation and analysis;\nolsrr - Building Ordinary Least Square (OLS) regression models and perform diagnostic tests for assumptions prior to conducting linear regression analysis;\ngtsummary - Generating presentation-ready data summary and statistical tables; and\nGWmodel - Calibrating geographically weighted family of models (Lu et al, 2014).\n\nThe code chunk below installs and loads the above R packages in the R environment, using the p_load() function of pacman package.\n\npacman::p_load(sf, tidyverse, ggpubr, tmap, spdep,\n               corrplot, olsrr, gtsummary, GWmodel)\n\n\n3.1. About GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 4",
    "section": "4. Geospatial Data-Wrangling",
    "text": "4. Geospatial Data-Wrangling\n\n4.1. Importing Geospatial Data\nRhe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\zhuyiting1\\ISSS624\\Hands-on_Ex4\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that the mpsz simple feature object does not have EPSG information.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\n4.2. Updating CRS Information\nThe code chunk below updates mpsz with the correct ESPG code (i.e. 3414).\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, we can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the above indicates ID[“EPSG”, 3414] now.\nWe saw earlier that mpsz spans from 2,667.538m to 56,396.44m in the x-direction and from 15,748.72m to 50,256.33m in the y-direction. We can confirm this for mpsz_svy21 using st_bbox() of sf to return the bounding of a simple feature object.\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#aspatial-data-wrangling",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#aspatial-data-wrangling",
    "title": "Hands-on Exercise 4",
    "section": "5. Aspatial Data-Wrangling",
    "text": "5. Aspatial Data-Wrangling\n\n5.1. Importing Aspatial Data\nThe Condo_resale_2015 data is in csv file format. We use read_csv() of readr to import the data as a tibble data fame called condo_resale, in the code chunk below.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nWe use glimpse() of dplyr to check if the data file has been imported correctly.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\ncondo_resale consists 23 columns and 1,436 rows. The data type for all columns is <dbl> or double-precision floating-point format, which is the format for storing a real number. We can see all the column names and take a peek at the first couple of entries for each column, which is useful when we want to call specific columns etc.\nNext, summary() of base R is used to display the summary statistics of the condo_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n5.2. Converting Aspatial Data Frame into a sf Object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf.\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs = 4326) %>%\n  st_transform(crs = 3414)\n\nNotice that we piped st_transform() of sf package to convert the coordinates from wgs84 (i.e. crs = 4326) to svy21 (i.e. crs = 3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLI…¹ AREA_…²   AGE PROX_…³ PROX_…⁴ PROX_…⁵ PROX_…⁶ PROX_…⁷ PROX_…⁸\n     <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   118635 3000000     309    30    7.94   0.166   2.52     6.62   1.77   0.0584\n2   288420 3880000     290    32    6.61   0.280   1.93     7.51   0.545  0.616 \n3   267833 3325000     248    33    6.90   0.429   0.502    6.46   0.378  0.141 \n4   258380 4250000     127     7    4.04   0.395   1.99     4.91   1.68   0.382 \n5   467169 1400000     145    28   11.8    0.119   1.12     6.41   0.565  0.461 \n6   466472 1320000     139    22   10.3    0.125   0.789    5.09   0.781  0.0994\n# … with 12 more variables: PROX_MRT <dbl>, PROX_PARK <dbl>,\n#   PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>, and abbreviated variable names\n#   ¹​SELLING_PRICE, ²​AREA_SQM, ³​PROX_CBD, ⁴​PROX_CHILDCARE, ⁵​PROX_ELDERLYCARE,\n#   ⁶​PROX_URA_GROWTH_AREA, ⁷​PROX_HAWKER_MARKET, ⁸​PROX_KINDERGARTEN\n\n\nWe see the first 6 rows of each column. Note that the output is in point feature data frame."
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 4",
    "section": "6. Exploratory Data Analysis (EDA)",
    "text": "6. Exploratory Data Analysis (EDA)\nIn the section, we use statistical graphing functions of ggplot2 package to perform EDA.\n\n6.1. EDA Using Statistical Graphs\nWe plot the distribution of SELLING_PRICE using ggplot() and geom_histogram() of ggplot2 in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed distribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nWe then plot LOG_SELLING_PRICE using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nWe can see that the distribution is more symmetrical and less right-skewed after log transformation.\n\n\n6.2. Multiple Histogram Plots Distribution of Variables\nWe can also plot multiple small histograms (also known as trellis plot) by using ggarrange() of ggpubr package. This is useful to help us study the distribution of multiple variables at once.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\nWe see that all the variables are relatively symmetrical, no severely skewed distribution.\n\n\n6.3. Plotting Statistical Point Map\nFinally, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive viewing mode of tmap by using the code chunk below.\n\ntmap_mode(\"view\")\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style = \"quantile\") +\n  tm_view(set.zoom.limits = c(11,14)) +\n  tmap_options(check.and.fix = TRUE)\n\n\n\n\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode so that we keep subsequent map plots as static plots and save memory.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#hedonic-pricing-modelling-in-r---simple-linear-regression-model",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#hedonic-pricing-modelling-in-r---simple-linear-regression-model",
    "title": "Hands-on Exercise 4",
    "section": "7. Hedonic Pricing Modelling in R - Simple Linear Regression Model",
    "text": "7. Hedonic Pricing Modelling in R - Simple Linear Regression Model\nHedonic pricing is a model that identifies price factors according to the premise that price is determined both by internal characteristics of the good being sold and external factors affecting it (Hargrave, 2021).\nIn this section, we build hedonic pricing models for condominium resale units using lm() of base R.\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr <- lm(formula = SELLING_PRICE ~ AREA_SQM, \n                data = condo_resale.sf)\n\nlm() returns an object of class lm or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm().\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\[\nSELLING PRICE = -258121.1 + 14719 * AREA SQM\n\\]\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value < 2.2 * 1016 is smaller than 0.05, at 5% significance level, we reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients section of the report reveals that the p-values of both the estimates of the Intercept and AREA_SQM are smaller than 0.05. In view of this, the null hypothesis of the B0 (Intercept) and B1 (AREA_SQM) are equal to 0 will be rejected. As a results, there is sufficient evidence to support that that the B0 and B1 are good parameter estimates of the SELLING_PRICE.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nThe plot above reveals that there are a few statistical outliers with relatively high selling prices."
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#hedonic-pricing-modelling-in-r---multiple-linear-regression-method",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#hedonic-pricing-modelling-in-r---multiple-linear-regression-method",
    "title": "Hands-on Exercise 4",
    "section": "8. Hedonic Pricing Modelling in R - Multiple Linear Regression Method",
    "text": "8. Hedonic Pricing Modelling in R - Multiple Linear Regression Method\n\n8.1. Analysing Multicollinearity Between Independent Variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other (correlation <= 0.85). If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics, which results in the collinear independent variables being over-represented in the multiple linear regression model.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R graphics, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), \n         diag = FALSE, \n         order = \"AOE\",\n         tl.pos = \"td\", \n         tl.cex = 0.5, \n         method = \"number\", \n         type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that FREEHOLD is highly correlated to LEASEHOLD_99YR (r = -0.84). In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASEHOLD_99YR is excluded in the subsequent model building.\n\n\n8.2. Building a Hedonic Pricing Model Using Multiple Linear Regression Method\nThe code chunk below using lm() to calibrate the multiple linear regression model, using all variables from condo_resale except LEASEHOLD_99YR which we said we would remove in the earlier sub-section due to multicollinearity with FREEHOLD.\n\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM +\n                  AGE +\n                  PROX_CBD + \n                  PROX_CHILDCARE + \n                  PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA +\n                  PROX_HAWKER_MARKET + \n                  PROX_KINDERGARTEN + \n                  PROX_MRT + \n                  PROX_PARK + \n                  PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + \n                  PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + \n                  PROX_BUS_STOP + \n                  NO_Of_UNITS + \n                  FAMILY_FRIENDLY + \n                  FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\nThe adjusted R-squared of 0.6474 suggests a relatively good fit of the multiple linear regression model built.\nSince p-value <2.2 * 1016 is smaller than 0.05, at 5% significance level, we reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that multiple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients section of the report reveals that the p-values of Intercept and 14 of the 18 variables are smaller than 0.05. Hence, the intercept and the 14 variables are statistically significant in this regression model.\n\n\n8.3. Preparing Publication Quality Table: olsrr Method\nWith reference to the report above, as not all of the independent variables included in the model are statistically significant, we will revised the model by removing those variables which are not statistically significant, namely:\n\nPROX_HAWKER_MARKET\nPROX_KINDERGARTEN\nPROX_TOP_PRIMARY_SCH\nPROX_SUPERMARKET\n\nNow, we are ready to calibrate the revised model by using the code chunk below. We also use ols_regress() of olsrr instead of summary() to generate publication quality table with the appropriate formating.\n\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                   AGE + \n                   PROX_CBD + \n                   PROX_CHILDCARE + \n                   PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + \n                   PROX_MRT + \n                   PROX_PARK + \n                   PROX_PRIMARY_SCH + \n                   PROX_SHOPPING_MALL + \n                   PROX_BUS_STOP + \n                   NO_Of_UNITS + \n                   FAMILY_FRIENDLY + \n                   FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n8.4. Preparing Publication Quality Table: gtsummary Method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note(), as shown in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nInformation on more customisation options is available at Tutorial: tbl_regression.\n\n\n8.5. Checking for Multicollinearity\nIn this sub-section, we will use olsrrpackage which is specially programmed for performing OLS regression. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n8.6. Regression Assumptions - Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the horizontal 0 line, without any distinct pattern. Hence, we can conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n8.7. Regression Assumptions - Test for Normality Assumption\nIt is also important to note that in a linear regression, the residual errors are assumed to be normally distributed. We check this by using ols_plot_resid_hist() of olsrr to perform the normality assumption test, in the code chunk below.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nFor formal statistical test, ols_test_normality() of olsrr package can be used as shown in the code chunk below.\n\nols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is sufficient statistical evidence that the residuals are not normally distributed.\n\n\n8.8. Test for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attribute. Hence, it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\nThen, we convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we use tmap package to display the distribution of the residuals on an interactive map.\nThe code chunk below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nThe code chunks below creates an interactive point symbol map.\n\ntm_shape(mpsz_svy21) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nWe switch the tmap mode back to \"plot\" before proceeding.\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation, as the residuals do not appear to be distributed at random over the geographical space.\nTo validate that our hypothesis, the Moran’s I test will be performed\nFirst, we determine the minimum distance to ensure that all subzones have at least 1 neighbour, using the following steps:\n\nCreate coords by joining the LONGITUDE and LATITUDE columns of condo_resale using cbind() of base R.\nReturn a matrix k1 with the indices of points belonging to the set of the k nearest neighbours of each other using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour subzone number IDs by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. This function returns in the units of the coordinates if the coordinates are projected, and in km otherwise.\nRemove the list structure of the returned object using unlist().\nUse summary() to determine the maximum distance between all nearest neighbour pairs to determine the upper bound for fixed-distance weight matrix.\n\n\ncoords <- cbind(condo_resale$LONGITUDE, condo_resale$LATITUDE)\n\nk1 <- knn2nb(knearneigh(coords))\n\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02913 0.05445 0.07053 0.07895 1.44241 \n\n\nAs the maximum distance is 1.44241km, we will round it up and use 1.5km or 1500m as the upper bound to ensure that all subzones have at least 1 neighbour.\nNext, we compute the distance-based weight matrix by using dnearneigh() of spdep.\n\nnb <- dnearneigh(coordinates(condo_resale.sp), \n                 0, 1500, \n                 longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw <- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nThereafter, lm.morantest() of spdep package is used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 2.2 * 1016, which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble positive cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-on Exercise 4",
    "section": "9. Building Hedonic Pricing Models Using GWmodel",
    "text": "9. Building Hedonic Pricing Models Using GWmodel\nIn this section, we will model hedonic pricing using both the fixed and adaptive bandwidth schemes.\n\n9.1. Building Fixed Bandwidth GWR Model\n\n9.1.1. Computing Fixed Bandwidth\nIn the code chunk below, bw.gwr() of GWModel is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive = FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argument.\n\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + \n                     AGE + \n                     PROX_CBD + \n                     PROX_CHILDCARE + \n                     PROX_ELDERLYCARE  + \n                     PROX_URA_GROWTH_AREA + \n                     PROX_MRT + \n                     PROX_PARK + \n                     PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + \n                     PROX_BUS_STOP + \n                     NO_Of_UNITS + \n                     FAMILY_FRIENDLY + \n                     FREEHOLD, \n                   data = condo_resale.sp, \n                   approach = \"CV\", \n                   kernel = \"gaussian\", \n                   adaptive = FALSE, \n                   longlat = FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405m.\n\n\n9.1.2. GWModel Method - Fixed Bandwidth\nNow we can use the code chunk below to calibrate the GWR model using fixed bandwidth and gaussian kernel.\n\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                         AGE +\n                         PROX_CBD +\n                         PROX_CHILDCARE +\n                         PROX_ELDERLYCARE  +\n                         PROX_URA_GROWTH_AREA +\n                         PROX_MRT +\n                         PROX_PARK +\n                         PROX_PRIMARY_SCH +\n                         PROX_SHOPPING_MALL +\n                         PROX_BUS_STOP +\n                         NO_Of_UNITS +\n                         FAMILY_FRIENDLY +\n                         FREEHOLD,\n                       data = condo_resale.sp, \n                       bw = bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-10 15:18:45 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2022-12-10 15:18:45 \n\n\nThe output is saved in a list of class gwrm. The report shows that the adjusted r-square of the GWR is 0.8430, which is significantly better than the globel multiple linear regression model of 0.6472.\n\n\n\n9.2. Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the GWR-absed hedonic pricing model by using adaptive bandwidth approach.\n\n9.2.1. Computing Adaptive Bandwidth\nSimilar to the earlier section, we will first use bw.ger() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + \n                        AGE  + \n                        PROX_CBD + \n                        PROX_CHILDCARE + \n                        PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + \n                        PROX_MRT + \n                        PROX_PARK + \n                        PROX_PRIMARY_SCH + \n                        PROX_SHOPPING_MALL + \n                        PROX_BUS_STOP + \n                        NO_Of_UNITS + \n                        FAMILY_FRIENDLY + \n                        FREEHOLD, \n                      data = condo_resale.sp, \n                      approach = \"CV\", \n                      kernel = \"gaussian\", \n                      adaptive = TRUE, \n                      longlat = FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended number of data points to be used.\n\n\n9.2.2. Constructive Adaptive Bandwidth GWR Model\nNow, we can go ahead to calibrate the GWR-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                            AGE + \n                            PROX_CBD + \n                            PROX_CHILDCARE + \n                            PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + \n                            PROX_MRT + \n                            PROX_PARK + \n                            PROX_PRIMARY_SCH + \n                            PROX_SHOPPING_MALL + \n                            PROX_BUS_STOP + \n                            NO_Of_UNITS + \n                            FAMILY_FRIENDLY + \n                            FREEHOLD, \n                          data = condo_resale.sp, \n                          bw = bw.adaptive, \n                          kernel = \"gaussian\", \n                          adaptive = TRUE, \n                          longlat = FALSE)\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-10 15:18:50 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2022-12-10 15:18:51 \n\n\nThe report shows that the adjusted r-square of the GWR is 0.8561 which is significantly better than the global multiple linear regression model of 0.6472. It is also slightly higher than the r-square of the GWR for fixed bandwidth model, which is 0.8430.\n\n\n\n9.3. Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardised residuals have a mean of 0 and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n9.3.1. Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\n\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, \n                                  as.matrix(gwr.adaptive.output))\n\nWe use glimpse() to display the content of condo_resale.sf.adaptive sf data frame.\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                <dbl> 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       <dbl> 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 <dbl> -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               <dbl> 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              <dbl> 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   <dbl> -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              <dbl> -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        <dbl> 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      <dbl> -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  <dbl> -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              <dbl> -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             <dbl> -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      <dbl> 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    <dbl> 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         <dbl> 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           <dbl> 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       <dbl> -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              <dbl> 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    <dbl> 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                <dbl> 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           <dbl> 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            <dbl> 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             <dbl> 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  <dbl> 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             <dbl> 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       <dbl> 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     <dbl> 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE <dbl> 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             <dbl> 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            <dbl> 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     <dbl> 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   <dbl> 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        <dbl> 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          <dbl> 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      <dbl> 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             <dbl> 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            <dbl> 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             <dbl> 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  <dbl> -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             <dbl> -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       <dbl> 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     <dbl> -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV <dbl> -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             <dbl> -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            <dbl> -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     <dbl> 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   <dbl> 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        <dbl> 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          <dbl> 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      <dbl> -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             <dbl> 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                <dbl> 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               <dbl> 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               <dbl> 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                <POINT [m]> POINT (22085.12 29951.54), POINT (25656.…\n\n\nWe also use summary() to review the summary statistics of yhat of gwr.adaptive.\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n9.3.2. Visualising Local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n9.3.3. Visualising by URA Planning Subzone\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex4/Hands-on_Ex4.html#references",
    "href": "Hands-on_Ex4/Hands-on_Ex4.html#references",
    "title": "Hands-on Exercise 4",
    "section": "10. References",
    "text": "10. References\nLu, B., Harris, P., Charlton, M., Brunsdon, C. (2014.) The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models. Geo-spatial Information Science. 17, 85-101. https://doi.org/10.1080/10095020.2014.917453\nHargrave, M. (2021.) Hedonic Pricing: Definition, How the Model Is Used, and Example. Investopedia. https://www.investopedia.com/terms/h/hedonicpricing.asp"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "This is my first in-class exercise for geospatial analytics! We will review the key components of Hands-on Exercise 1 and learn more using the data from the In-class Exercise 1 zip file."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1",
    "section": "Getting Started",
    "text": "Getting Started\n\nR packages\nThe code chunk below will install and load tidyverse and sf packages.\n\npacman::p_load(tidyverse, sf)\n\npacman is a wrapper that wraps an installation package and a loading package, so that the package(s) is/are installed and loaded in one breath without having to use install.packages() + library().\nThe syntax pacman:: is necessary because while the package pacman is installed, it is not loaded. Hence, it needs to be specified so that R knows to use p_load() from pacman package.\nWhen rendering, R is making sure that tidyverse and sf packages are installed.\nTo view the (local) webpage in web browser, using the bottom right-hand quadrant of the RStudio > Files > ISSS624 folder > _site > In-class Ex > In-class Ex1 > In-class Ex1.html > left-click select View in Web Browser.\n\n\nData\nUnzip Hands-on_Ex1, copy the data folder to C:/zhuyiting1/ISSS624/In-class_Ex/In-class_Ex1. It should be in the same folder as In-class_Ex1.qmd."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "title": "In-class Exercise 1",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImporting Polygon Features\nThis code chunk will import ESRI shapefile into R.\n\nmpsz <- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\zhuyiting1\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo read the syntax of the function, select the function name in the code and press F1. The Help page in the bottom right-hand quadrant of RStudio will open up.\nThe dsn argument is to tell st_read() where is the destination (relative path). As the code document (qmd file) is in the same directory as the data folder, we can start from “data/” to call the data folder within.\nPro tip: Using the bottom right-hand quadrant, go to Files tab, go to the folder/subfolder that we are trying to import the data (in this case data/geospatial) and look at the folder and file names to key in as inputs for st_read().\nFrom the output, we can obtain a couple of information including:\n\nProjected CRS: SVY21 (Singapore-based projected coordinates system). Not in degrees format.\nUnit of measurement: metres.\n\nUnder Environment tab in the top right-hand quadrant of RStudio, mpsz data is loaded in R. Clicking on the blue |> button to the left of mpsz, we can take a quick peek of the dataframe. Up to the first 10 records are displayed (to save space). To look at the dataframe in greater detail, click on the dataframe name mpsz to open the full data table.\n\n\nImporting and Transforming into Projected Coordinate System\nDecimal degree format (between 0o and 360o) good for exact location but not for distance measurement. This is because the Earth is ellipsoid and the distance gets closer further away from the equator in this format.\nOn the other hand, projected coordinate system flattens the Earth. This makes the distance roughly equivalent no matter where we are.\nReading: Geographic vs Projected Coordinate Systems\nTo convert wgs84 (geographic coordinate system) to svy21 (Singapore projected coordinate system), use the function st_transform() and argument crs = 3414.\nThe following code chunk imports the pre-schools location data (kml file) and pipes it into the transformation step in one sitting.\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\") %>% \n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\zhuyiting1\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nWhile the output above still indicates geodetic CRS wgs84, when we click on preschool in the Environment tab to open the data table, we can see that the geometry values are no longer in degree decimal format."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#in-class-exercise-1",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#in-class-exercise-1",
    "title": "In-class Exercise 1",
    "section": "In-class Exercise 1",
    "text": "In-class Exercise 1"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-shapefile-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-shapefile-data",
    "title": "In-class Exercise 1",
    "section": "Importing Shapefile Data",
    "text": "Importing Shapefile Data\nNow, let’s add the In-class Ex 1 data in the data folder.\nWe will also need the spdep package for this exercise. We do it by including it in the p_load() function. As it requires the spData package, we will install and load it as well. Finally, we need the tmap package for quick plots.\n\npacman::p_load(spData, tidyverse, sf, spdep, tmap)\n\nWe also want to do some housekeeping to keep our laptop memory freed up from unused datasets. In the Environment tab, click on the broom logo to remove unused datasets.\n\nNow we want to import the Hunan ESRI shapefile into R. This is the Hunan county boundary layer.\n\nhunan <- st_read(dsn = \"data/geospatial\",\n                layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\zhuyiting1\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nWe can see that the data is in geodetic CRS wgs84."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-attribute-data-in-csv",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-attribute-data-in-csv",
    "title": "In-class Exercise 1",
    "section": "Importing Attribute Data in CSV",
    "text": "Importing Attribute Data in CSV\n\nAspatial Hunan Data\nThe code chunk below imports the aspatial Hunan 2012 data. This dataset contains selected local development indicators such as Gross Domestic Product per capita (GDPPC) for each county in 2012.\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#joining-hunan-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#joining-hunan-data",
    "title": "In-class Exercise 1",
    "section": "Joining Hunan Data",
    "text": "Joining Hunan Data\nThe code chunk below joins the spatial and aspatial data for Hunan using the left_join() function of the dplyr package.\n\nhunan <- left_join(hunan, hunan2012)\n\nR recognises the variable “County” to be the only common variable between the two dataframes and performs the join accordingly. As the two dataframes have the exact same number of observations with matching county, left_join() here works the same as a inner_join(), right_join() and full_join(). It appends the hunan2012 data to the right of the original hunan data. Notably, the geometry column from the original hunan dataframe remains at the rightmost column of the new hunan dataframe."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualising-regional-development-indicator",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualising-regional-development-indicator",
    "title": "In-class Exercise 1",
    "section": "Visualising Regional Development Indicator",
    "text": "Visualising Regional Development Indicator\nWith some data about the Hunan province, we want to create a quick thematic map to visualise the distribution of GDPPC in 2012, using the qtm() function from the tmap package.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"County\", size=0.5)\n\ngdppc <- qtm(hunan, fill = \"GDPPC\")\n\ntmap_arrange(basemap, gdppc, asp = 1, ncol = 2)\n\n\n\n\nWe see that qtm() does a quick plot using equal interval classification. From the map, most regions (~60%) have GDPPC of only 1/5 of the wealthiest region. In addition, we observe that the wealth is mostly concentrated in the Northeast region of the Hunan province, and the top tier GDPPC in 2012 was held by only 1 county - Changsha."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#computing-contiguity-spatial-weights",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#computing-contiguity-spatial-weights",
    "title": "In-class Exercise 1",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nIn this section, we will use poly2nb() of spdep to compute contiguity weights matrices for the study area, Hunan. This function builds a neighbours list based on regions with contiguous (shared) boundaries. The default criteria used in this function is Queen’s criteria, which considers any boundary that is touching by at least 1 point to be a neighbour.\n\n\nComputing (Queen’s Criteria) Contiguity-based Neighbours\nThe code chunk below computes Queen’s contiguity weights matrix.\n\nwm_q <- poly2nb(hunan, queen = TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThere are 448 pairs of neighbours found by the Queen’s case, from the 88 counties in Hunan. From the summary report, the link number distributions shows the frequency of the number of links or neighbours that each county has, the most being 11 neighbours for 1 county (region 85). On the other extreme, the 2 least connected regions (30 and 65) only have 1 neighbour each.\nBy calling the County column of the hunan dataframe, we can see that the county with the most neighbours is Taoyuan and that with the least neighbours are Xinhuang and Linxiang. This is consistent with the basemap that we plotted previously, where Taoyuan is a large county in the North surrounded by multiple smaller counties, and Xinhuang and Linxiang being counties along the West and Northeast borders of Hunan respectively.\n\nhunan$County[c(85, 30, 65)]\n\n[1] \"Taoyuan\"  \"Xinhuang\" \"Linxiang\"\n\n\nFor each polygon in our polygon object, wm_q() lists all neighboring polygons. For example, to see the neighbours for the region with the most number of neighbours (polygon number 85), we can use the following code chunk.\n\nwm_q[[85]]\n\n [1]  1  2  3  5  6 32 56 57 69 75 78\n\n\nThe 11 neighbours of Polygon 85, which is Taoyuan, are listed above. The numbers represent the polygon IDs as stored in the hunan SpatialPolygonsDataFrame class.\nTo retrieve the GDPPC of Taoyuan and all its 11 neighbours, the following code chunk can be used.\n\nnb85 <- wm_q[[85]]\nnb85 <- hunan$GDPPC[c(85, nb85)]\nnb85\n\n [1] 22879 23667 20981 34592 25554 27137 24194 14567 21311 18714 14624 19509\n\n\nWe can see that the GDPPC of Taoyuan and its 11 neighbours (based on Queen’s criteria) are 22879, 23667, 20981, 34592, 25554, 27137, 24194, 14567, 21311, 18714, 14624 and 19509 respectively.\nThe complete weights matrix can be listed using str() (display internal structure) from R’s utils package.\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\nComputing (Rook’s Criteria) Contiguity-based Neighbours\nThe code chunk below computes the Rook’s Case contiguity weights matrix. Rook’s Case considers only regions with shared boundaries with more than 1 shared point to be neighbours.\n\nwm_r <- poly2nb(hunan, queen = FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nA similar summary report is generated as above. The number of links reduced from 448 for Queen’s Case to 440 for Rook’s Case, indicating that there are 8 pairs of regions that are each linked by 1 point each. Now, the most connected area has 10 neighbours instead of 11."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualising-contiguity-weights",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualising-contiguity-weights",
    "title": "In-class Exercise 1",
    "section": "Visualising Contiguity Weights",
    "text": "Visualising Contiguity Weights\nA connectivity graph takes a point and displays a line between each pairs of neighbouring points. For this exercise, we need to obtain the points from the polygon geometry data. The most common method used is to obtain the polygon centroids, which we will do using the sf package.\n\nGetting Latitude and Longitude of Polygon Centroids\nInstead of just running st_centroid(), which assumes that the coordinates are planar, on the sf object hunan, we use a mapping function map_dbl() from purrr to transform each element of a vector into a vector of the same length.\n\nTo obtain the longitude values, we map the st_centroid() function over the geometry column of hunan and access the longitude value through double bracket notation [[ ]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude using [[2]] for the second value.\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nWe then use cbind() to append the longitude and latitude values into the same object coords. It is important that the longitude is placed before the latitude as longitude represents the x-coordinates and is hence read as the first variable in plotting functions. This is the opposite of conventional geography where latitude is quoted before longitude.\n\ncoords <- cbind(longitude, latitude)\n\nWe check the first few observations to see if things are formatted correctly before moving on to the plots.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\nPlotting Queen’s vs Rook’s Contiguity-based Neighbours Map\nThe code chunk below plots the Queen’s and Rook’s contiguity-based neighbours map. par() is used to set the parameters for the plots, having the plots in 1 row, 2 columns using the mfrow argument.\n\npar(mfrow = c(1, 2))\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main = \"Queen Contiguity\")\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main = \"Rook Contiguity\")\n\n\n\n\nWe see that some of the points that are joined in the Queen’s Case are not in the Rook’s Case (e.g. the cross in the bottom-right region)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#computing-distance-based-neighbours",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#computing-distance-based-neighbours",
    "title": "In-class Exercise 1",
    "section": "Computing Distance-based Neighbours",
    "text": "Computing Distance-based Neighbours\nAnother way to define neighbour relationship is to used distance-based matrix. Using dnearneigh() of spdep, neighbours of a region are determined based on the Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If projected coordinates are used and either specified in the coordinates object x or with x as a two-column matrix and longlat=TRUE, great circle distances in km will be calculated assuming wgs84 reference ellipsoid.\n\nDetermining the Cut-off Distance\nFirst, we need to determine the upper limit for distance band by using the following steps:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number IDs by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. This function returns in the units of the coordinates if the coordinates are projected, and in km otherwise.\nRemove the list structure of the returned object using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km. We can use 62km as the upper threshold to ensure that all regions will at least have 1 neighbour.\n\n\nComputing Fixed-Distance Weights Matrix\nUsing 62 km as the upper bound and 0 km as a lower bound (i.e. all regions with centroids within 62 km distance of a particular region will be considered its neighbours), we compute the distance weights matrix using dnearneigh() of spdep.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nFor the 88 regions (counties) in Hunan, there are a total of 324 links (neighbour-neighbour pairs) which are 62 km or less between each other, averaging to 3.7 neighbours per region.\nTo look at the number of neighbours for each region and their region IDs, we use the str() function of R’s utils package to see its internal structure.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nWe can also do so by displaying the structure of the distance-bsaed weights matrix using table() of base R and card() of spdep. The latter tallies the numbers of neighbours of regions in the neighbours list, and feeds into the former to build a contingency table where each row is a county (alphabetically ordered) and “1” is indicated for the number of neighbours that it has (columns).\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nWe can use n.comp.nb() of spdep to perform depth first search on neighbours list and confirm that all regions are neighbours to each other (i.e. within a single neighbours list). If any of the regions are disjoint, they will be indicated by a 2nd region etc.\n\nn_comp <- n.comp.nb(wm_d62)\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\nWe see from the result above that all 88 regions are in the same neighbours list.\n\n\nPlotting Fixed Distance Weights Matrix\nPlotting it using plot() of R graphics shows us the neighbour relationship in map form, and points() of R graphics can show us any disjoint neighbours list in different colours.\n\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_d62, coords, add = TRUE)\npoints(coords, col = n_comp$comp.id, pch = 19)\n\n\n\n\nRegions that are larger in area and/or at the edge of Hunan are seen to have a smaller number neighbours (1-2), while those that are smaller and/or inland tend to have more neighbours (5-6). This is because by using fixed distance, larger areas can find fewer neighbours within the same distance compared to smaller areas, and usually on the shorter/narrower sides.\nIf we want to visualise the links of 1st nearest neighbours, we can use the code chunk below.\n\npar(mfrow = c(1, 2))\nplot(hunan$geometry, border = \"lightgrey\")\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08, main = \"1st nearest neighbours\")\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_d62, coords, add = TRUE, pch = 19, cex = 0.6, main = \"Distance link\")\n\n\n\n\nThe plot on the left shows the links between all pairs of 1st neighbours in red, and the one on the right is the reference plot for all distance links based on the cut-off distance of 62 km.\n\n\nComputing Adaptive Distance Weights Matrix\nOne of the characteristics of fixed distance weights matrix is that more densely settled areas (usually urban areas) tend to have more neighbours while less densely settled areas (usually rural areas) tend to have less neighbours. Having many neighbours smooths the neighbour relationship across more neighbours.\nWe can control the number of neighbours that each region has, by using k-nearest neighbours instead of stipulating a fixed distance threshold for neighbour-neighbour relationship, either accepting asymmetric neighbours or imposing symmetry, using knn2nb() and knearneigh() combination in the code chunk below. This is similar to how we determined the largest distance for 1st neighbour pairs in order to come up with the fixed distance weights matrix, except without needing to compute the distances between each pair.\n\nknn6 <- knn2nb(knearneigh(coords, k = 6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nWe see that by setting k = 6 so that each region has exactly 6 neighbours, we get a total of 88 * 6 = 528 links.\nWe similarly display the content of the matrix using str() of R’s utils to see the region IDs of all 6 neighbours for each region.\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\n\nPlotting Adaptive Distance-based Neighbours\nWe plot the adaptive distance weights matrix using the code chunk below.\n\nplot(hunan$geometry, border = \"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nUnlike the plot for fixed distance weights matrix, the adaptive distance weights matrix plot looks more evenly connected by forcing each region to have 6 neighbours exactly.\n\n\nComputing Inverse Distance Weights (IDW) Matrix\nWe can also derive spatial weights matrix based on the Inverse Distance Method.\nThe code chunk below computes the distance between areas by using nbdists() of spdep, similar to how we computed the distances between 1st neighbour pairs in the fixed distance weights matrix approach. We then use lapply() of base R to apply the function to inverse the distance computed (1/dist).\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\n\nRow-Standardised Weights Matrix\nNext, we need to assign weights to each neighbouring polygon. In our case, each neighbouring polygon will be assigned equal weight (style = \"W\"). This is accomplished by assigning the fraction of 1/(number of neighbours) to each neighbouring county, then summing the weighted income values.\nWhile this is the most intuitive way to summarise the values of all the neighbours of a particular region, it has the drawback of over- or under-estimating the true nature of the spatial autocorrelation in the data as the polygons along the edges of the study area will base their lagged values on fewer polygons.\nIn this exercise, we use style = \"W\" option for simplicity, but should note that more robust options such as style = \"B\" is available for binary. We do so on the Queen’s contiguity weights matrix.\n\nrswm_q <- nb2listw(wm_q, style = \"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe argument zero.policy = TRUE allows for lists of non-neighbours. This should be used with caution as we may not be aware of missing neighbours in the dataset, which R would warn us about if zero.policy = FALSE is used by returning an error.\nTo see the weights assigned to the neighbours of the first polygon, we can call the values of weights column for polygon ID 1 using the code chunk below.\n\nrswm_q$weight[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nAs we saw earlier, polygon ID 1 has 5 neighbours (screenshot below). Hence, row-standardisation assigned a weight of 0.2 to each of its 5 neighbours, as seen from the output above. The application of this is that when R computes the average values among polygon ID 1’s neighbours, each neighbour’s value will be multiplied by 0.2 before being tallied (weighting).\n\nUsing the same method, we can also derive a distance weight matrix by using the code chunk below.\n\nrswm_ids <- nb2listw(wm_q, glist = ids, style = \"B\", zero.policy = TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\nWe can similarly check the weights assigned to the neighbours of polygon ID 1 by Queen’s contiguity and inverse distance weights matrix, using the code chunk below.\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\nThe weights assigned to each neighbour is no longer uniform across neighbours for the same region, but are standardised based on the inverse of the distance between the centroids of the region and each neighbour. Here, a higher weight is assigned to the neighbour that is closest (smallest distance), in this case the 2nd neighbour or polygon ID 3.\nWe can also use summary() and unlist() for the summary statistics of the weights by IDW method using the code chunk below.\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\nThe minimum, maximum, mean, median and interquartile range of the IDW weights are listed above. We can potentially use some of these as thresholds in determining other suitable cut-offs inverse distance thresholds for neighbour definition."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#application-of-spatial-weight-matrix",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#application-of-spatial-weight-matrix",
    "title": "In-class Exercise 1",
    "section": "Application of Spatial Weight Matrix",
    "text": "Application of Spatial Weight Matrix\nIn this section, we will create 4 different spatial lagged variables:\n\nspatial lag with row-standardised weights;\nspatial lag as a sum of neighbouring values;\nspatial window average; and\nspatial window sum.\n\n\nSpatial Lag with Row-standardised Weights\nWe will not compute the Gross Domestic Product per capita (GDPPC) value for each region, using the average among its neighbours. These values are often referred to as spatially lagged values. We will use the GDPPC data from the Hunan_2012.csv file and joined to the geospatial hunan shapefile data.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecall in the previous section, we retrieved the GDPPC of polygon ID 85 (Taoyuan) and its 11 neighbours to based on Queen contiguity, stored in nb85.\n\nRerunning the code without including the region itself, and computing the mean GDPPC of all 11 neighbours using the code chunk below, we get 22259.09, which is the same as the GDPPC.lag for polygon ID 85.\n\nnb85 <- wm_q[[85]]\nnb85 <- hunan$GDPPC[nb85]\nmean(nb85)\n\n[1] 22259.09\n\n\n\nGDPPC.lag[85]\n\n[1] 22259.09\n\n\n\nrswm_q$weight[85]\n\n[[1]]\n [1] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n [7] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n\n\nWe can append the spatially lag GDPPC values onto hunan sf dataframe using the code chunk below.\n\nlag.list <- list(hunan$County, GDPPC.lag)\nlag.res <- as.data.frame(lag.list)\ncolnames(lag.res) <- c(\"County\", \"lag GDPPC\")\nhunan <- left_join(hunan, lag.res)\n\nThe following table shows the average neighbour income values (stored in the lag GDPPC field) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 36 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3 Shape_Leng Shape_Area  County    City\n1 Changde 21098 Anxiang      County   1.869074 0.10056190 Anxiang Changde\n2 Changde 21100 Hanshou      County   2.360691 0.19978745 Hanshou Changde\n3 Changde 21101  Jinshi County City   1.425620 0.05302413  Jinshi Changde\n4 Changde 21102      Li      County   3.474325 0.18908121      Li Changde\n5 Changde 21103   Linli      County   2.289506 0.11450357   Linli Changde\n6 Changde 21104  Shimen      County   4.171918 0.37194707  Shimen Changde\n  avg_wage deposite     FAI Gov_Rev Gov_Exp     GDP GDPPC     GIO   Loan  NIPCR\n1    31935   5517.2  3541.0  243.64  1779.5 12482.0 23667  5108.9 2806.9 7693.7\n2    32265   7979.0  8665.0  386.13  2062.4 15788.0 20981 13491.0 4550.0 8269.9\n3    28692   4581.7  4777.0  373.31  1148.4  8706.9 34592 10935.0 2242.0 8169.9\n4    32541  13487.0 16066.0  709.61  2459.5 20322.0 24473 18402.0 6748.0 8377.0\n5    32667    564.1  7781.2  336.86  1538.7 10355.0 25554  8214.0  358.0 8143.1\n6    33261   8334.4 10531.0  548.33  2178.8 16293.0 27137 17795.0 6026.5 6156.0\n   Bed    Emp  EmpR EmpRT Pri_Stu Sec_Stu Household Household_R NOIP Pop_R\n1 1931 336.39 270.5 205.9  19.584  17.819     148.1       135.4   53 346.0\n2 2560 456.78 388.8 246.7  42.097  33.029     240.2       208.7   95 553.2\n3  848 122.78  82.1  61.7   8.723   7.592      81.9        43.7   77  92.4\n4 2038 513.44 426.8 227.1  38.975  33.938     268.5       256.0   96 539.7\n5 1440 307.36 272.2 100.8  23.286  18.943     129.1       157.2   99 246.6\n6 2502 392.05 329.6 193.8  29.245  26.104     190.6       184.7  122 399.2\n    RSCG Pop_T    Agri Service Disp_Inc      RORP    ROREmp lag GDPPC\n1 3957.9 528.3 4524.41   14100    16610 0.6549309 0.8041262  24847.20\n2 4460.5 804.6 6545.35   17727    18925 0.6875466 0.8511756  22724.80\n3 3683.0 251.8 2562.46    7525    19498 0.3669579 0.6686757  24143.25\n4 7110.2 832.5 7562.34   53160    18985 0.6482883 0.8312558  27737.50\n5 3604.9 409.3 3583.91    7031    18604 0.6024921 0.8856065  27270.25\n6 6490.7 600.5 5266.51    6981    19275 0.6647794 0.8407091  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nWe can visualise the GDPPC of each county vsthe spatial lag GDPPC, using the code chunk below for quick plot.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp = 1, ncol = 2)\n\n\n\n\nWe see that the maximum GDPPC reduced from the 80,000 to 100,000 band to 50,000 to 60,000 when we plotted spatial lag GDPPC. This is because there was only 1 county with 2012 GDPPC in the highest band, and this only contributed to the spatial lag GDPCC of its neighbours by a weighted proportion. In fact, we see that the neighbours of the high GDPPC regions show high spatial lag GDPPC.\n\n\nSpatial Lag as a Sum of Neighbour Values\nWe can calculate spatial lag as a sum of neighbour values by assigning binary weights (style = \"B\"). This requires us to go back to our neighbours list, and apply a function that assigns binary weights instead of row standardisation. We will also use glist= argument in the nb2listw() function to explicitly assign these weights.\nWe start by applying a function that assigns a value of 1 for each neighbour using lapply(), similar to when we applied the function for IDW.\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 <- nb2listw(wm_q, glist = b_weights, style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nChecking using polygon ID 85 (Taoyuan which has 11 neighbours by Queen contiguity), we see that the weights for all 11 neighbours are 1 instead of 1/11 under row-standardisation.\n\nb_weights2$weight[85]\n\n[[1]]\n [1] 1 1 1 1 1 1 1 1 1 1 1\n\n\nWith the proper weights assigned, we can use lag.listw() to compute a lag variable from our weights and GDPPC.\n\nlag_sum <- list(hunan$County, lag.listw(b_weights2, hunan$GDPPC))\nlag.res <- as.data.frame(lag_sum)\ncolnames(lag.res) <- c(\"County\", \"lag_sum GDPPC\")\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nWe see that now, the lag sum GDPPC for Taoyuan (polygon ID 85) is the simple sum of the GDPPC of all its neighbours, vis-à-vis the weighted average with row-standardised weights in the earlier sub-section.\n\nlag_sum[[2]][85]\n\n[1] 244850\n\n\n\nsum(nb85)\n\n[1] 244850\n\n\nLike what we did for spatial lag with row-standardised weights, we append the lag_sum GDPPC field to hunan sf dataframe using the code chunk below.\n\nhunan <- left_join(hunan, lag.res)\n\nNow, we can plot GDPPC, Spatial Lag GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\nlag_sum_gdppc <- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, asp = 1, ncol = 3)\n\n\n\n\nWe see that in the lag_sum GDPPC plot, the highest bracket increased from 50,000 to 100,000 to 400,000 to 500,000. The spatial lag sum of GDPPC tends to be higher regions with greater number of neighbours, all else equal.\n\n\nSpatial Window Average\nSpatial window average uses row-standardised weights and include the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights. To begin, we assign wm_q to a new variable wm_q1, because we will directly alter its structure to add the diagonal elements.\n\nwm_q1 <- wm_q\n\nTo add the diagonal element to the neighbour list, we use include.self() from spdep.\n\nwm_q1 <- include.self(wm_q1)\n\nWe see that the number of links increased by 88 from 448 to 536.\nNow, we obtain weights with nb2listw()\n\nwm_q1 <- nb2listw(wm_q1)\nsummary(wm_q1)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 10 12 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 2 links\n1 most connected region:\n85 with 12 links\n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nWe create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gdppc <- lag.listw(wm_q1, hunan$GDPPC)\nlag_w_avg_gdppc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we convert the lag variable listw object into a dataframe using as.data.frame().\n\nlag.list.wm_q1 <- list(hunan$County, lag_w_avg_gdppc)\nlag_wm_q1.res <- as.data.frame(lag.list.wm_q1)\ncolnames(lag_wm_q1.res) <- c(\"County\", \"lag_window_avg GDPPC\")\nlag.list.wm_q1\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nWe now append lag_window_avg GDPPC to hunan sf dataframe using left_join() of dplyr.\n\nhunan <- left_join(hunan, lag_wm_q1.res)\n\nFinally, we do a quick plot of GDPPC, lag GDPPC and lag_window_avg GDPPC maps next to each other using qtm() of tmap.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\nw_avg_gdppc <- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, w_avg_gdppc, asp = 1, ncol = 3)\n\n\n\n\nWe see that the Northeast counties that are dark orange in the lag GDPPC plot are now dark red in the lag_window_avg GDPPC plot. We also see that the highest bracket is lowered from 50,000 to 60,000 to 40,000 to 50,000, as the highest GDPPC from Changsha is now split over a larger number of counties.\n\n\nSpatial Window Sum\nThe final visualisation that we will do is spatial window sum. It is the counterpart of the window average, except without row-standardised weights. To do this, we again assign binary weights to the neighbour structure that includes the diagonal element, and similarly start by assigning wm_q to a new variable, this time wm_q2.\n\nwm_q2 <- wm_q\n\nWe use include.self() from spdep to add the diagonal element to the neighbour list.\n\nwm_q2 <- include.self(wm_q2)\nwm_q2\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we assign binary weights to the neighbour structure just like we did in the Spatial Lag Sum sub-section.\n\nb_weights3 <- lapply(wm_q2, function(x) 0*x + 1)\nb_weights3[85]\n\n[[1]]\n [1] 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values of 1.\n\nb_weights4 <- nb2listw(wm_q2, glist = b_weights3, style = \"B\")\nb_weights4\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable w_sum_gdppc with lag.listw().\n\nw_sum_gdppc <- list(hunan$County, lag.listw(b_weights4, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nWe conver the lag list variable listw object into a dataframe using as.data.frame().\n\nw_sum_gdppc.res <- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) <- c(\"County\", \"w_sum GDPPC\")\n\nThe code chunk below uses left_join() of dplyr to append w_sum GDPPC values to hunan sf dataframe.\n\nhunan <- left_join(hunan, w_sum_gdppc.res)\n\nLastly, qtm() of tmap is used to plot the GDPPC, lag_sum GDPPC and w_sum GDPPC.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc <- qtm(hunan, \"lag_sum GDPPC\")\nw_sum_gdppc <- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, w_sum_gdppc, asp = 1, ncol = 3)\n\n\n\n\nWe note that the only change before lag_sum GDPPC and w_sum GDPPC plots is the 3 regions to the top and right of the region with highest spatial lag sum GDPPC jumping to the next window sum GDPPC tier, as these are generally regions with high GDPPC themselves and adding the absolute GDPPC boosts their total GDPPC from themselves and their neighbours, sufficient to move to the next GDPPC bracket."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "Water is a scarce resource. Goal 6 of the United Nations’ (UN’s) Sustainable Development Goals (SDGs) is to ensure availability and sustainable management of water and sanitation for all."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#getting-started",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#getting-started",
    "title": "In-class Exercise 2",
    "section": "Getting Started",
    "text": "Getting Started\n\nWater Point Data\nWe obtained the global water point and small water scheme level data from Water Point Data Exchange (WPdx) Global Data Repositories (WPdx, 2020). We accessed the WPdx-Plus (WPdx+) option and downloaded the full Shapefile under the Export option. As the data consists of water points around the world, we will later filter for water points within Nigeria in R in a subsequent step.\nAfter downloading the Shapefile which can take a few minutes due to the large file size, unzip the folder and copy the Shapefiles (.dbf, .prj, .shp and .shx) into a data subfolder that shares the same directory as this Quarto file for ease of calling the files. We also want to rename all four files to geo_export so that we can reference these filenames more easily when we import the data.\n\n\n3.2. Geographical Boundaries of Nigeria\nWe also need the geographical boundaries of Nigeria to make meaningful sense of its water point locations and to aid spatial visualisation. Here, we downloaded the Level-2 Administrative Boundaries (also known as Local Government Area (LGA)) data (ADM2) for Nigeria in year 2020 from geoBoundaries, the largest open and free database of political administrative boundaries globally (geoBoundaries, 2022). One can filter for Nigeria’s data by typing it in under the Name filter, followed by clicking on the download button under the column geoBoundaries, sub-column Full Release and for the row Nigeria, NGA, ADM2, 2020.\nSimilar to the water point data, we unzip the folder and copy the Shapefiles (.dbf, .prj, .shp and .shx) into the same folder as the water points Shapefiles. Here, we rename the files to geoBoundaries-NGA-ADM2 to indicate the data source (geoBoundaries), country (NGA) and administrative boundary level (ADM2)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#installing-and-loading-packages-in-r",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#installing-and-loading-packages-in-r",
    "title": "In-class Exercise 2",
    "section": "4. Installing and Loading Packages in R",
    "text": "4. Installing and Loading Packages in R\nThe code chunk below uses p_load() from pacman package to brings in the R packages for:\n\nSpatial vector data encoding (sf);\nData-wrangling (tidyverse);\nMap plotting (tmap);\nGeospatial analysis (spdep); and\nRapid Exploratory Data Analysis (EDA) (funModeling).\n\n\npacman::p_load(sf, tidyverse, tmap, spdep, funModeling)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-geospatial-data-in-r",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#importing-geospatial-data-in-r",
    "title": "In-class Exercise 2",
    "section": "5. Importing Geospatial Data in R",
    "text": "5. Importing Geospatial Data in R\n\n5.1. Water Point Geospatial Data\nWe are now ready to import the geospatial data into the Quarto document. The code chunk below does so for the water point data by using st_read() function of the sf package. We specified the data source name (dsn) or directory of the file (\"data/geospatial\"), layer for the name of the Shapefiles (\"geo_export\"), and crs = 4326 to import the data in wgs84 geographic coordinate reference system (CRS), since the Shapefile is in wgs84. We also pipe a filter to obtain data that are in Nigeria only, by using the filter() function of dplyr package from tidyverse. The clean_country_name column is used for the filter, and note that the column name is truncated in the Shapefile due to character limit and should be keyed in correctly to perform the filter successfully.\n\nwp <- st_read(dsn = \"data/geospatial\",\n              layer = \"geo_export\",\n              crs = 4326) %>% \n  filter(clean_coun == \"Nigeria\")\n\nOne point to note is that while we can theoretically transform the data to projected CRS directly by using the st_transform() function of sf to facilitate the accurate computation of distances in a planar configuration, we want to keep it on hold for now as it will result in missing data points when we use st_intersects() subsequently to identify water points within each administrative boundary. This is because st_intersects() only works correctly if the geospatial data are in geographic CRS.\nThe simple feature data frame comprises 95,008 observations of 73 variables. In particular, we are interested in the variable status_clean (truncated to status_cle in the Shapefile), which tells us which water points are functional versus not. In addition, we will use the last variable, geometry, to perform data join for the recoded variables to the LGA boundaries data.\nOn a practical note, to avoid taking up too much memory space in GitHub, which has a memory limit of 100MB, we will extract the necessary data and save them in an rds file, and delete the geo_export Shapefiles from the data/geospatial folder, before committing and pushing the changes to GitHub. This is to prevent error in the process of pushing the commit to GitHub. We do so by running the relevant code chunks below and saving the rds file in the data/spatial folder, and then setting #| eval: false so that the codes that use the original Shapefiles and intermediate large files will not run when knitted. This way, those codes will be suppressed when rendering the Quarto file and analysis can be done using the eventual rds file.\n\nShould we wish to run certain lines of codes that are suppressed, we can set to #| eval: true to allow normal evaluation during rendering, or run it manually in the RStudio environment.\n\nIn the code chunk below, write_rds() of the readr package is used to save the extracted sf data table into an output file in rds data format. We then do not need to go back to the original Shapefile to reload the full set of global water points data each time we use it, as the data size is very large, the time to load is long and it cannot be pushed to GitHub.\n\nwp_nga <- write_rds(wp, \"data/geospatial/wp_nga.rds\")\n\nHowever, do note that after running the above code chunk, the wp_nga.rds file is still too large (140.2MB) to push to GitHub (100MB limit). Hence, we will further extract only the data that we wish to use for our analysis and save it as another .rds file, and remove this one, indicate #| eval: false and delete the wp_nga.rds file from our directory, before we commit and push the changes to GitHub.\n\n\n5.2. Nigeria Level-2 Administrative Boundary Geospatial Data\nWe also import the Nigeria Level-2 Administrative Boundary (LGA) data into our Quarto file, similarly using st_read() of sf in the code chunk below. The data are saved in the form of a simple feature data table nga.\n\nnga <- st_read(dsn = \"data/geospatial\",\n              layer = \"geoBoundaries-NGA-ADM2\",\n              crs = 4326)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\zhuyiting1\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nglimpse(nga)\n\nRows: 774\nColumns: 6\n$ shapeName  <chr> \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", \"Abaka…\n$ Level      <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ shapeID    <chr> \"NGA-ADM2-72505758B79815894\", \"NGA-ADM2-72505758B67905963\",…\n$ shapeGroup <chr> \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NG…\n$ shapeType  <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((7.401109 5...., MULTIPOLYGON (…\n\n\nThere are 774 observations of 6 variables in the nga file, including shapeName for the LGA that each region belongs to and geometry for the polygons, as seen using the glimpse() function of dplyr above. The geometry type is multipolygon. It is also in the wgs84 geographic CRS, just like the water point data. Hence for now, there is no need to perform st_transform() to align their CRS.\nWe also run a check for invalid geometries in the LGA data, using st_is_valid() of sf.\n\nlength(which(st_is_valid(nga) == FALSE))\n\n[1] 0\n\n\nThe output is 0 - there is no invalid geometry for the LGA polygons.\nWe also check for missing values in the LGA data, using is.na() of ursa to return TRUE/FALSE values and rowSums() of raster to tally the number of TRUE.\n\nnga[rowSums(is.na(nga))!=0,]\n\nSimple feature collection with 0 features and 5 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] shapeName  Level      shapeID    shapeGroup shapeType  geometry  \n<0 rows> (or 0-length row.names)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#data-wrangling-cleaning-and-extracting-the-necessary-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#data-wrangling-cleaning-and-extracting-the-necessary-data",
    "title": "In-class Exercise 2",
    "section": "6. Data Wrangling: Cleaning and Extracting the Necessary Data",
    "text": "6. Data Wrangling: Cleaning and Extracting the Necessary Data\n\n6.1. Recoding of Missing Water Point Status Data\nIn the code chunk below, we use replace_na() of tidyr to replace the “NA” data in the status_cle variable with “Unknown”, as this is the variable that will be used subsequently. This is so that the observations with “NA” will not be excluded in subsequent analyses.\n\nwp_nga <- read_rds(\"data/geospatial/wp_nga.rds\") %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))\n\n\n\n6.2. Exploratory Data Analysis (EDA)\nIn the code chunk below, we use freq() of funModeling to display the distribution of status_cle field in wp_nga for a quick view of the available classes and their distributions. We need to suppress this code chunk due to file size limit when we commit the changes and push to GitHub, by setting #|eval: false.\n\nfreq(data = wp_nga,\n     input = \"status_cle\")\n\nWe see that there are 3 status_cle values that describe functional water points, namely\n\nFunctional (45,883, 48%),\nFunctional but needs repair (4,579, 5%), and\nFunctional but not in use (1,686, 2%).\n\nOn the other hand, there are 5 values which indicate that the water points are not functional, including 7 mis-coded values due to a missing hyphen and lower “f”, and they are\n\nNon-Functional (29,385, 31%)\nNon-Functional due to dry season (2,403, 3%)\nAbandoned/Decommissioned (234, <1%)\nAbandoned (175, <1%)\nNon functional due to dry season (7, <1%)\n\nWe see that over 1/3 of the water points are non-functional.\nThere are also 10,656 or 11% missing values which we recoded to Unknown using replace_na().\n\n\n6.3. Extracting Water Point Data\nIn this section, we will extract the water point records by using the classes that we saw above in status_cle field. This will help us obtain the absolute numbers as well as allow us to calculate the % total later.\n\n\n6.4. Extracting Functional Water Points\nIn the code chunk below, we extract the data for the functional water points into wpt_functional using filter() of dplyr for the 3 classes that we identified using freq() of funModeling.\n\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nRunning freq() on wpt_functional to check, we can see that the same number of records for the 3 functional classes are captured as per in wp_nga. We similarly suppress the evaluation of the code chunk below due to file size constraint.\n\nfreq(data = wpt_functional,\n     input = \"status_cle\")\n\n\n\n6.5. Extracting Non-Functional Water Points\nWe repeat the above process for non-functional water points, using the code chunks below.\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\n\nfreq(data=wpt_nonfunctional, \n     input = 'status_cle')\n\n\n\n6.6. Extracting Water Point with Unknown Class\nFinally, for completeness, we also need to extract the water points with unknown status (missing status_cle field), using the code chunk below. Using str() of R’s utils, we confirm that the number of observations (10,656) tallies with that in the earlier frequency bar chart plotted using freq() of funModeling.\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\nstr(wpt_unknown)\n\n\n\n6.7. Performing Point-in-Polygon Count\nWe want to find the number and proportion of functional, non-functional and unknown water points within each LGA. To do this, we use st_intersects() of sf to determine the cross-over between the LGA polygons in nga and water points in wp_nga. Thereafter, lengths() of Base R is used to return the number of water points in each class by LGA. Finally, we use mutate() of dplyr to add the new variables for total wpt, wpt functional, wpt non-functional and wpt unknown to nga sf data table, and assign it to a new variable nga_wp.\n\nnga_wp <- nga %>% \n  mutate(`total wpt` = lengths(\n    st_intersects(nga, wp_nga))) %>%\n  mutate(`wpt functional` = lengths(\n    st_intersects(nga, wpt_functional))) %>%\n  mutate(`wpt non-functional` = lengths(\n    st_intersects(nga, wpt_nonfunctional))) %>%\n  mutate(`wpt unknown` = lengths(\n    st_intersects(nga, wpt_unknown)))\n\nNote that the symbol used is ” ` ” (backtick) and not ” ’ ” (apostrophe). This is used when there is space and hyphen (-) in the variable name (e.g. total wpt).\nThereafter, we compute the percentage functional and percentage non-functional water points as pct_functional and pct_non-functional, using mutate() of dplyr in the code chunk below.\n\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt functional`/`total wpt`) %>%\n  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`)\n\n\n\n6.8 Saving the Analytical Data Table\nWith the tidy sf data table, we save it in rds file format as nga_wp.rds for subsequent analysis, using write_rds() of readr.\n\nwrite_rds(nga_wp, \"data/geospatial/nga_wp.rds\")\n\nBefore we move on to the next section on spatial analysis, we will set #| eval: false for all code chunks that rely on either the geo_export Shapefiles or wp_nga as the files are too large and need to be deleted before committing and pushing the changes to GitHub. We will work with the geoBoundaries-NGA-ADM2 Shapefiles and nga_wp.rds file, which is only around 2.1MB in size respectively."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-the-spatial-distribution-of-water-points---thematic-mapping",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2.html#visualising-the-spatial-distribution-of-water-points---thematic-mapping",
    "title": "In-class Exercise 2",
    "section": "7. Visualising the Spatial Distribution of Water Points - Thematic Mapping",
    "text": "7. Visualising the Spatial Distribution of Water Points - Thematic Mapping\nTo avoid error with the removal of the large data files and suppression of the relevant code chunks above, we use read_rds() to load the nga_wp.rds file at the start of the next section of our analysis.\n\nnga_wp <- read_rds(\"data/geospatial/nga_wp.rds\")\n\nAs we have performed st_intersects(), we can use st_transform() of sf to convert the data from an ellipsoid wgs84 CRS to a planar projected CRS via mathematical reprojection of the coordinates, prior to distance calculations. This is done using EPSG: 26392 for Minna / Nigeria Mid Belt (Spatial Reference, 2022), in the code chunk below. We also check that the transformation has been done correctly using st_geometry() of sf, where the projected CRS field indicates Minna / Nigeria Mid Belt.\n\nnga_wp26392 <- st_transform(nga_wp, \n                            crs = 26392)\nst_geometry(nga_wp26392)\n\nGeometry set for 774 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 26662.71 ymin: 30523.38 xmax: 1344157 ymax: 1096029\nProjected CRS: Minna / Nigeria Mid Belt\nFirst 5 geometries:\n\n\nWe see that the bounding box values have changed from the decimal degree format where the minimum and maximum values of x and y were between 2.7o and 14.7o, to between 26,663m and 1,344,157m (MapTools, 2022; epsg.io, 2022).\nWe set tmap_mode() of tmap to “view” to activate interactive viewing mode instead of static maps, to better zoom into any of the 774 LGAs for further analysis if needed.\n\ntmap_mode(\"view\")\n\nIn the code chunk below, we plot the wgs84 and crs = 26392 versions of the Nigerian LGA using tm_shape() of tmap, and note that the latter appears to flatten the mapping area out a little more than the former. We can also see that the plots are now in interactive mode.\n\nnga_wp_wgs <- tm_shape(nga_wp) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Geographic CRS\",\n            main.title.position = \"center\")\n\nnga_wp_proj <- tm_shape(nga_wp26392) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Projected CRS\",\n            main.title.position = \"center\")\n\ntmap_arrange(nga_wp_wgs, nga_wp_proj, asp = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.1. Quick Plots by Equal Classification\nThe code chunk below uses qtm() of tmap to do a quick thematic map plot of the Nigeria LGA, coloured by the number of water points in equal classification method. The four quadrants represent (from top left in a “Z” shape) total water points, functional water points, non-functional water points and water points with unknown functionality status.\n\ntotal <- qtm(nga_wp26392, \"total wpt\")\nwp_functional <- qtm(nga_wp26392, \"wpt functional\")\nwp_nonfunctional <- qtm(nga_wp26392, \"wpt non-functional\")\nunknown <- qtm(nga_wp26392, \"wpt unknown\")\n\ntmap_arrange(total, wp_functional, wp_nonfunctional, unknown, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy zooming into the choropleth maps and clicking on the areas of interest, we see that Babura has the most number of total water points at 894. There are 11 LGAs in the Northeast region that are without any water points at all, namely Geidam, Mobbar, Abadam, Kukawa, Guzamala, Nganzai, Gubio, Marte, Kala/Balge, Kaga and Gujba.\nThe general patterns of the functional water points appear similar. We see that in Chikun, there are 4 water points in total but 0 functional ones.\nFor non-functional water points, they are mainly found in Ifelodun (278) and Igabi (216), forming 46% and 74% of all water points found in those regions, suggesting that heavy replacement or maintenance work may be needed there.\nThe water points with unknown status information are mainly found in the Central (e.g. Pankshin, Shendam) and South regions (e.g. Izzi, Ikwo).\nWe also visualise the functional and non-functional water points by their proportions of the total number of water points in each LGA, using the code chunk below.\n\npct_wp_functional <- qtm(nga_wp26392, \"pct_functional\")\npct_wp_nonfunctional <- qtm(nga_wp26392, \"pct_non-functional\")\n\ntmap_arrange(pct_wp_functional, pct_wp_nonfunctional, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the left plot, we see that the the highest proportion of functional water points are mostly found in the North half of Nigeria. On the right plot, the proportion of non-functional water points tend to be more dispersed in the remaining LGAs. In general, there are less LGAs with high proportion of non-functional water points compared to that of functional ones."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "title": "In-class Exercise 3",
    "section": "",
    "text": "execute:\necho: false\n\nAbove is to by default not display any code chunks in the global environment, when keyed into the top part of the Quarto document."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#run-through-of-hands-on-exercise-3",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#run-through-of-hands-on-exercise-3",
    "title": "In-class Exercise 3",
    "section": "Run-through of Hands-on Exercise 3",
    "text": "Run-through of Hands-on Exercise 3\n\nR packages\n\nsf: do data import and export\nrgdal: do transformation, change from 1 data type to another\nspdep: used to create spanning trees (including SKATER)\n\nlast week also used for computation of spatial autocorrelation, very rich package)\n\nreadr: reading of csv, read text data in and out of R\n\nif we want to read excel file (.xlsx), especially excel workbook with multiple worksheets -> should use readexcel\n\nstatistical packages -> use package called heaven (?) to bring into R\nggplot2: create plots for statistical methods\ntmap: mapping device\ncoorplot: build correlation plot\nggpubr: glue up multiple\nheatmaply: plotlib (?) version to do interactive heatmap in multidimension\nclusterGeo: soft spatially constrained clustering algorithm package\nfactoextra: mainly for factor analysis, but also have a very nice visual for us to understand how the cluster change using the different clustering methods\nPCA vs factor analysis: PCA rotation is 90 degrees so that newly transformed variables are as far as possible, factor analysis have other rotation methods such as value max (?)\n\nNbClust: access hierarchical clustering results\n\npacman::p_load(rgdal, spdep, tmap, sf, \n               ggpubr, cluster, factoextra, NbClust, \n               heatmaply, corrplot, psych, tidyverse, ClustGeo)\n\n\n\nLoading shapefile data\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. We can import it into the R environment using st_read() of sf. We also use the piping function %>% from dplyr and perform filter() to extract only the data for the Shan state. This is done in the code chunk below.\n\nshan_sf <- st_read(dsn = \"data/geospatial\",\n                   layer = \"myanmar_township_boundaries\") %>% \n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"))\n\nReading layer `myanmar_township_boundaries' from data source \n  `C:\\zhuyiting1\\ISSS624\\In-class_Ex\\In-class_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nHave a habit of examining the data:\n\nPolygon\nDecimal degree format\nWGS84\n\nClick on file |>arrow in the Environment pane to check the variables (field name). Alternatively, use str() to show the field names.\nTo check the full data table, click on the object in the Environment pane. It includes all attribute values in the dbf file + geometry from shp file.\nSetting CRS: important to change to projected CRS when doing distance-based weight matrix. Not required for contiguity-based weight matrix as the CRS does not affect the boundaries touching.\nCode explained:\n%>%: piping - glue different functions together\n\nAs good practice, push the code after %>% to the next line for neatness.\n\n\n\n\nLoading csv data\nThe csv file is imported using read_csv() of readr using the code chunk below.\n\nict <- read_csv(\"data/aspatial/Shan-ICT.csv\")\n\n\nread_csv() of readr used instead of read.csv() of Base R\n\nFormer is a readr function -> retains the original field names.\n\nUse ` ` to encapsulate complete variable names with space\n\nLatter changes the variable names by replacing and space with period.\n\n\n\n\nCalculation of derived ICT penetration rates\n\nict_derived <- ict %>%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%\n  rename(`DT_PCODE`=`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\n6 new variables added to ict_derived using mutate().\nx1000 -> in social science, usually the units are no. of handphones by per 1’000 households\nCan x100 if computing % households with handphones.\nrename() changes the variable names to match that of the shapefile to do join later (e.g. change from Distinct Pcode to DT_PCODE.\n\n\n\nJoining of data\nWe combine both sets of data into a single data.frame using left_join() of dplyr, which appends the second data.frame to the first based on the observations in the first. The shan_sf simple feature data.frame will be used as the base data object, so that the geometry is retained, and the ict_derived data.frame will be used as the join table.\n\nshan_sf <- left_join(shan_sf, \n                     ict_derived, \n                     by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n\n\nWe need to define the variable names to join (by=) , if we did not rename the variables to align them previously.\n\n\n\nVisualisation methods\n\nggplot(data = shan_sf, \n       aes(x = `RADIO`)) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = \"light blue\")\n\n\n\n\n\nBy using ggplot() directly without assigning it to an ouput object (e.g. plot1 <-), it is not saved and only shown when rendered (good for quick view). Assign to object if want to call it later.\nIf don’t want to run the code to plot the graph when rendering (set #| eval: false), can find the html image file which is produced the first time the code chunk was rendered, and paste the code in the report :)\n\n\n\nCorrelation analysis\nWe use corrplot.mixed() of corrplot to visualise and analyse the correlation of the input variables, using the code chunk below.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nIf ellipse is thin and colour is dark -> high correlation.\ncor(ict_derived[,12:17]) -> to pick out columns 12 to 17 only for data.frame and then plotting of correlation plot.\n\n\n\nCluster analysis\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars <- shan_sf %>%\n  st_set_geometry(NULL) %>%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\nUse the select() function to extract the variables out (because we don’t need the rest).\nst_set_geometry(Null) drops the geometric column so that it does not go into the data frame which does not work in the hierarchical clustering.\n\nNext, we need to change the rows by township name instead of row number by using the code chunk below.\n\nrow.names(cluster_vars) <- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNow, we will delete the TS.x field (representing township) which is a duplicate of the row names now, by using the code chunk below.\n\nshan_ict <- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\nDo this to make a tidy data frame so that all the columns are just the variables for clustering analysis.\nTS.x kept and shifted to row name instead of simply deleting because we need it later (displayed in the dendogram instead of just numbers).\n\n\n\nProximity matrix\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat <- dist(shan_ict, method = 'euclidean')\n\n\nproxmat\n\n\nValues represent the proximity matrix between towns in the top row and left-most column. The matrix is symmetrical along the diagonal. The value for the diagonal is 0.\n\n\n\nHierarchical clustering\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward <- hclust(proxmat, method = 'ward.D')\n\n\nThe code is simple as it only requires 2 arguments: the proximity matrix and the clustering method.\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\nBoth hclust() and plot() are of Base R (plot() from Base R Graphics), don’t need to tell plot() how to plot, it knows! :D\ncex = 0.6 scales the resolution to 60% of the full resolution. Useful when the dendogram looks too cluttered and the clusters cannot be read.\n\n\n\nFinding Optimal Clustering Algorithm\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\nHighest value = most optimal model\nFunctional programming:\nWant to run all four hierarchical clustering algorithms in 1 go -> create an object m and names(m)\nThen we define a function with the syntax function(x){ }\nThe function is to substitute each element (hierarchical clustering method) in the list m as method = x in the function for agnes() to compute agglomerative hierarchical clustering of the data set.\nSimilar to looping in conventional programming.\n\n\n\nDetermining Optimal Cluster\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(1234)\ngap_stat <- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.677830 0.2707006 0.03692273\n [2,] 8.130029 8.346462 0.2164322 0.04088387\n [3,] 7.992265 8.200253 0.2079877 0.03762167\n [4,] 7.862224 8.079170 0.2169462 0.04018998\n [5,] 7.756461 7.977981 0.2215201 0.04229538\n [6,] 7.665594 7.890134 0.2245409 0.04501316\n [7,] 7.590919 7.812990 0.2220709 0.04364077\n [8,] 7.526680 7.739537 0.2128575 0.04477188\n [9,] 7.458024 7.670476 0.2124519 0.04623855\n[10,] 7.377412 7.603947 0.2265346 0.04762720\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\nIf we follow the statistics strictly, 2 clusters would be the best. However, we know that we should not have less than 3 clusters as it is a multivariate analysis. Hence, by visual assessment, cluster numbers 5 and 6 may work better.\n\n\n\nMapping of Hierarchical Clusters\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of Base R will be used in the code chunk below to derive a 6-cluster model.\n\ngroups <- as.factor(cutree(hclust_ward, k=6))\n\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename() of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster,\"CLUSTER\")\n\n\n\n\n\n6 clusters plot -> good for visual\nTo convert it into map view, label it in k = 6 and make it into factors\nUse rename() for tidying of field names when the matrix is combined with the geospatial data as the naming done by the cbind() function is not intuitive.\n\n\n\nConverting to sp\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp <- as_Spatial(shan_sf)\n\n\nsf was developed after SKATER -> need to convert to sp object (spatial polygon) first. For doing calculation.\nsp object has multiple tables -> separate the geometry from the rest (like shapefile, split into multiple files).\nUse sf format when plotting with tmap functions.\n\n\nncuts = 5 starts from 0 -> so there are 6 clusters."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#new-chapter-in-hand-on-exercise-3",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#new-chapter-in-hand-on-exercise-3",
    "title": "In-class Exercise 3",
    "section": "New Chapter in Hand-on Exercise 3",
    "text": "New Chapter in Hand-on Exercise 3\n\n9. Spatially Constrained Clustering ClustGeo Method\nIn this section, we gain hands-on experience on using functions of ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n\n9.1. Ward-like Hierarchical Clustering: ClustGeo\nClustGeo package provides function called hclustgo() to perform a typical Ward-like hierarchical clustering, similar to hclust() of base R stats.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function of a dissimilarity matrix, as shown in the code chunk below.\n\nnongeo_cluster <- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster,\n            k = 6,\n            border = 2 : 5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist().\n\n\n9.2. Mapping the Clusters Formed\nSimilar to our Hands-on Exercise 3, we can plot the clusters on a categorical area shaded map by using the steps below.\n\ngroups <- as.factor(cutree(nongeo_cluster, k = 6))\n\n\nshan_sf_ngeo_clust <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed. We compare this (right plot) to the hierarchical clustering h formed by hclust() of base R stats (left plot).\n\nhclust.map <- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nhclustgeo.map <- qtm(shan_sf_ngeo_clust, \"CLUSTER\")\n\ntmap_arrange(hclust.map, hclustgeo.map,\n             asp=NA, ncol=2)\n\n\n\n\nWe see that clusters 3, 4 and 6 are exactly the same across both methods. Furthermore, both plots show clusters that jump across different parts of Shan State geographically.\n\n\n9.3. Spatially Constrained Hierarchical Clustering\nBefore we can perform spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist <- st_distance(shan_sf, shan_sf)\ndistmat <- as.dist(dist)\n\nNote that as.dist() is used to convert the data frame into a matrix.\nNext, choicealpha() is used to determine a suitable value for the mixing parameter alpha, as shown in the code chunk below.\n\ncr <- choicealpha(proxmat, \n                  distmat, \n                  range.alpha = seq(0, 1, 0.1), \n                  K = 6, \n                  graph = TRUE)\n\n\n\n\n\n\n\n\nchoicealpha() is for us to balance 2 matrices.\nBalance out the homogeneity in attributes space (D0) and geographical space (spatial, e.g Queen’s contiguity weight matrix) (D1).\nRanges from 0 to 1\n0-stage = only considering attribute space without consideration of attribute homogeneity\n1: spatial homogeneity\nst_distance() takes the centroid of polygons\nClustGeo is more rigid in terms of algorithm - only accepts Ward\nBut more flexible in terms of being able to use either contiguity-based or distance-based weight matrix\nseq(0, 1, 0.1): 0.1 = interval (increment) between 0 and 1 in the plotting\nK = 6: Note that “K” in this argument is in upper case!! Different from hclust().\n2 graphs plotted\n\n1st graph based on raw\n2nd graph based on normalisation values -> if we find that our data is highly skewed. We will look at this in this exercise.\n\nHelps us determine the optimal alpha value -> aim is to have as high Qnorm as possible\nBased on 2nd graph: can either choose either alpha 0.2 or 0.3.\nSharp increase in spatial homogeneity with <20% drop in attribute homogeneity from 0.1 to 0.2 alpha value.\nIn practice, we should compare a few alpha values to see how the map changes.\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG <- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster object.\n\ngroups <- as.factor(cutree(clustG, k = 6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters. We do so by comparing it (on the right plot) side-by-side with the Ward-like hierarchical clustering that we did earlier without considering the spatial component (on the left plot).\n\nhclustgeo.map <- qtm(shan_sf_ngeo_clust, \"CLUSTER\")\n\nhclustgeo0.3.map <- qtm(shan_sf_Gcluster, \"CLUSTER\")\n\ntmap_arrange(hclustgeo.map, hclustgeo0.3.map,\n             asp=NA, ncol=2)\n\n\n\n\nWe see that now the clusters are no longer all over the place, but follows a more geospatial-related set of rules. At the same time, it is not completely geospatial either, with some clusters still spanning across different regions, e.g. cluster 4.\n\nTo interpret the clusters, we can use heatmaply() to study the features of each cluster, OR do a boxplot (summary statistics) to do so."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "This document is a replicate of the Hands-on Exercise 4, wit additional explanatory notes. Notes are indicated in the following format:\n\nThis is an example of how the additional explanatory notes taken during the In-Class Exercise 4 will appear."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#overview",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#overview",
    "title": "In-class Exercise 4",
    "section": "1. Overview",
    "text": "1. Overview\nGeographically-weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, we learn how to build hedonic pricing models by using GWR methods. The dependent variable in this exercise is the resale prices of condominium in Singapore in 2015. The independent variables are divided into either structural or locational."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#data",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#data",
    "title": "In-class Exercise 4",
    "section": "2. Data",
    "text": "2. Data\nTwo data sets are used in this model-building exercise. They are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL); and\n2015 condominium resale data in csv format (i.e. Condo_resale_2015.csv)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#r-packages",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#r-packages",
    "title": "In-class Exercise 4",
    "section": "3. R Packages",
    "text": "3. R Packages\nWe start by setting up the necessary R packages in the R environment by installing and loading them. The R packages that are used in this exercise are:\n\nsf - Spatial data handling;\ntidyverse - Attribute data handling including reading csv file (readr), variable creation (ggplot2) and creating statistical plots (dplyr);\nggpubr - Combining multiple statistical plots;\ntmap - Plotting choropleth map;\nspdep - Creating neighbour list and matrix;\ncorrplot - Conducting multivariate data visualisation and analysis;\nolsrr - Building Ordinary Least Square (OLS) regression models and perform diagnostic tests for assumptions prior to conducting linear regression analysis;\ngtsummary - Generating presentation-ready data summary and statistical tables; and\nGWmodel - Calibrating geographically weighted family of models (Lu et al, 2014).\n\nThe code chunk below installs and loads the above R packages in the R environment, using the p_load() function of pacman package.\n\npacman::p_load(sf, tidyverse, ggpubr, tmap, spdep,\n               corrplot, olsrr, gtsummary, GWmodel)\n\n\n3.1. About GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.\n\nGWmodel package includes GW linear and generalised regression, machine learning methods."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#geospatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "4. Geospatial Data-Wrangling",
    "text": "4. Geospatial Data-Wrangling\n\n4.1. Importing Geospatial Data\nRhe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\zhuyiting1\\ISSS624\\In-class_Ex\\In-class_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that the mpsz simple feature object does not have EPSG information.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\n4.2. Updating CRS Information\nThe code chunk below updates mpsz with the correct ESPG code (i.e. 3414).\n\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, we can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the above indicates ID[“EPSG”, 3414] now.\nWe saw earlier that mpsz spans from 2,667.538m to 56,396.44m in the x-direction and from 15,748.72m to 50,256.33m in the y-direction. We can confirm this for mpsz_svy21 using st_bbox() of sf to return the bounding of a simple feature object.\n\nst_bbox(mpsz_svy21)\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#aspatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#aspatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "5. Aspatial Data-Wrangling",
    "text": "5. Aspatial Data-Wrangling\n\n5.1. Importing Aspatial Data\nThe Condo_resale_2015 data is in csv file format. We use read_csv() of readr to import the data as a tibble data fame called condo_resale, in the code chunk below.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nWe use glimpse() of dplyr to check if the data file has been imported correctly.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             <dbl> 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            <dbl> 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             <dbl> 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\ncondo_resale consists 23 columns and 1,436 rows. The data type for all columns is <dbl> or double-precision floating-point format, which is the format for storing a real number. We can see all the column names and take a peek at the first couple of entries for each column, which is useful when we want to call specific columns etc.\nNext, summary() of base R is used to display the summary statistics of the condo_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n5.2. Converting Aspatial Data Frame into a sf Object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf.\n\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs = 4326) %>%\n  st_transform(crs = 3414)\n\nNotice that we piped st_transform() of sf package to convert the coordinates from wgs84 (i.e. crs = 4326) to svy21 (i.e. crs = 3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLI…¹ AREA_…²   AGE PROX_…³ PROX_…⁴ PROX_…⁵ PROX_…⁶ PROX_…⁷ PROX_…⁸\n     <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1   118635 3000000     309    30    7.94   0.166   2.52     6.62   1.77   0.0584\n2   288420 3880000     290    32    6.61   0.280   1.93     7.51   0.545  0.616 \n3   267833 3325000     248    33    6.90   0.429   0.502    6.46   0.378  0.141 \n4   258380 4250000     127     7    4.04   0.395   1.99     4.91   1.68   0.382 \n5   467169 1400000     145    28   11.8    0.119   1.12     6.41   0.565  0.461 \n6   466472 1320000     139    22   10.3    0.125   0.789    5.09   0.781  0.0994\n# … with 12 more variables: PROX_MRT <dbl>, PROX_PARK <dbl>,\n#   PROX_PRIMARY_SCH <dbl>, PROX_TOP_PRIMARY_SCH <dbl>,\n#   PROX_SHOPPING_MALL <dbl>, PROX_SUPERMARKET <dbl>, PROX_BUS_STOP <dbl>,\n#   NO_Of_UNITS <dbl>, FAMILY_FRIENDLY <dbl>, FREEHOLD <dbl>,\n#   LEASEHOLD_99YR <dbl>, geometry <POINT [m]>, and abbreviated variable names\n#   ¹​SELLING_PRICE, ²​AREA_SQM, ³​PROX_CBD, ⁴​PROX_CHILDCARE, ⁵​PROX_ELDERLYCARE,\n#   ⁶​PROX_URA_GROWTH_AREA, ⁷​PROX_HAWKER_MARKET, ⁸​PROX_KINDERGARTEN\n\n\nWe see the first 6 rows of each column. Note that the output is in point feature data frame."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#exploratory-data-analysis-eda",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#exploratory-data-analysis-eda",
    "title": "In-class Exercise 4",
    "section": "6. Exploratory Data Analysis (EDA)",
    "text": "6. Exploratory Data Analysis (EDA)\nIn the section, we use statistical graphing functions of ggplot2 package to perform EDA.\n\n6.1. EDA Using Statistical Graphs\nWe plot the distribution of SELLING_PRICE using ggplot() and geom_histogram() of ggplot2 in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed distribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nWe then plot LOG_SELLING_PRICE using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nWe can see that the distribution is more symmetrical and less right-skewed after log transformation.\n\n\n6.2. Multiple Histogram Plots Distribution of Variables\nWe can also plot multiple small histograms (also known as trellis plot) by using ggarrange() of ggpubr package. This is useful to help us study the distribution of multiple variables at once.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\nWe see that all the variables are relatively symmetrical, no severely skewed distribution.\n\n\n6.3. Plotting Statistical Point Map\nFinally, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive viewing mode of tmap by using the code chunk below.\n\ntmap_mode(\"view\")\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style = \"quantile\") +\n  tm_view(set.zoom.limits = c(11,14)) \n\n\n\n\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\n\nError message: Shape contains invalid polygons. Please fix it or set tmap_options(check.and.fix = TRUE) and rerun the plot.\nThis is because that the administrative boundary map has errors, there are some missing polygons / polygons are not exactly touching and that creates some gaps.\nIn theory, we should go back to the data to fix this issue as part of data cleaning. Nevertheless, in this case, we will set tmap_options(check.and.fix = TRUE) in this case to do a quick fix of the topological issue during plotting.\nTo know where to place this piece of code, look at the code chunk above. the first tm_shape() is used to plot the polygons while the second is used to plot the points. Hence, we want to place it in the first part of the code, after tm_shape(), but before tm_polygons() where the polygons are plotted, as the error is with the polygons based on the error message.\n\nBefore moving on to the next section, the code below will be used to turn R display into plot mode so that we keep subsequent map plots as static plots and save memory.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#hedonic-pricing-modelling-in-r---simple-linear-regression-model",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#hedonic-pricing-modelling-in-r---simple-linear-regression-model",
    "title": "In-class Exercise 4",
    "section": "7. Hedonic Pricing Modelling in R - Simple Linear Regression Model",
    "text": "7. Hedonic Pricing Modelling in R - Simple Linear Regression Model\nHedonic pricing is a model that identifies price factors according to the premise that price is determined both by internal characteristics of the good being sold and external factors affecting it (Hargrave, 2021).\nIn this section, we build hedonic pricing models for condominium resale units using lm() of base R.\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr <- lm(formula = SELLING_PRICE ~ AREA_SQM, \n                data = condo_resale.sf)\n\nlm() returns an object of class lm or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm().\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\[\nSELLING PRICE = -258121.1 + 14719 * AREA SQM\n\\]\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value < 2.2 * 1016 is smaller than 0.05, at 5% significance level, we reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients section of the report reveals that the p-values of both the estimates of the Intercept and AREA_SQM are smaller than 0.05. In view of this, the null hypothesis of the B0 (Intercept) and B1 (AREA_SQM) are equal to 0 will be rejected. As a results, there is sufficient evidence to support that that the B0 and B1 are good parameter estimates of the SELLING_PRICE.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nThe plot above reveals that there are a few statistical outliers with relatively high selling prices."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#hedonic-pricing-modelling-in-r---multiple-linear-regression-method",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#hedonic-pricing-modelling-in-r---multiple-linear-regression-method",
    "title": "In-class Exercise 4",
    "section": "8. Hedonic Pricing Modelling in R - Multiple Linear Regression Method",
    "text": "8. Hedonic Pricing Modelling in R - Multiple Linear Regression Method\n\n8.1. Analysing Multicollinearity Between Independent Variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other (correlation <= 0.85). If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics, which results in the collinear independent variables being over-represented in the multiple linear regression model.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R graphics, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data frame.\n\ncorrplot(cor(condo_resale[, 5:23]), \n         diag = FALSE, \n         order = \"AOE\",\n         tl.pos = \"td\", \n         tl.cex = 0.5, \n         method = \"number\", \n         type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that FREEHOLD is highly correlated to LEASEHOLD_99YR (r = -0.84). In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASEHOLD_99YR is excluded in the subsequent model building.\n\nConverting condo_resale tibble data frame to condo_resale.sf simple feature (sf) data frame converts the longitude and latitude columns into a geometry field.\nThe correlation matrix here plots the tibble data frame instead of sf data frame as it cannot read the geometry column.\n\n\n\n8.2. Building a Hedonic Pricing Model Using Multiple Linear Regression Method\nThe code chunk below using lm() to calibrate the multiple linear regression model, using all variables from condo_resale except LEASEHOLD_99YR which we said we would remove in the earlier sub-section due to multicollinearity with FREEHOLD.\n\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM +\n                  AGE +\n                  PROX_CBD + \n                  PROX_CHILDCARE + \n                  PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA +\n                  PROX_HAWKER_MARKET + \n                  PROX_KINDERGARTEN + \n                  PROX_MRT + \n                  PROX_PARK + \n                  PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + \n                  PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + \n                  PROX_BUS_STOP + \n                  NO_Of_UNITS + \n                  FAMILY_FRIENDLY + \n                  FREEHOLD, \n                data  = condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\nThe adjusted R-squared of 0.6474 suggests a relatively good fit of the multiple linear regression model built.\nSince p-value <2.2 * 1016 is smaller than 0.05, at 5% significance level, we reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that multiple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients section of the report reveals that the p-values of Intercept and 14 of the 18 variables are smaller than 0.05. Hence, the intercept and the 14 variables are statistically significant in this regression model.\n\nHere, we use the sf data frame condo_resale.sf because lm() model can work with it.\n\n\n\n8.3. Preparing Publication Quality Table: olsrr Method\nWith reference to the report above, as not all of the independent variables included in the model are statistically significant, we will revised the model by removing those variables which are not statistically significant, namely:\n\nPROX_HAWKER_MARKET\nPROX_KINDERGARTEN\nPROX_TOP_PRIMARY_SCH\nPROX_SUPERMARKET\n\nNow, we are ready to calibrate the revised model by using the code chunk below. We also use ols_regress() of olsrr instead of summary() to generate publication quality table with the appropriate formating.\n\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                   AGE + \n                   PROX_CBD + \n                   PROX_CHILDCARE + \n                   PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + \n                   PROX_MRT + \n                   PROX_PARK + \n                   PROX_PRIMARY_SCH + \n                   PROX_SHOPPING_MALL + \n                   PROX_BUS_STOP + \n                   NO_Of_UNITS + \n                   FAMILY_FRIENDLY + \n                   FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\nolsrr package creates a tidier summary report. It starts with the model summary with the summary statistics of the model that are critical in telling us about the goodness of fit, errors etc. (R, R-Squared, Adj. R-Squared, RMSE) for us to view the performance of the model.\nANOVA part of the report: we look at the Sig. column for the p-value. In this case, the number is reported as 0.0000 as it is very small, and we know that we can reject null hypothesis when it is less than our alpha (0.05 in this case).\nFinally, the parameter estimates are reported. It gives the p-values in the Sig column rounded off to 3 decimal place. In this case, as we have already picked out the independent variables that are significant, all Sig values are < 0.05.\nWe can also read the Beta column: for example, for every 1 unit increase in AREA_SQM, the property price would increase by $12,777.52 according to the model.\nFor categorical variables, e.g. FREEHOLD, if it is freehold (yes), it adds $350,599.81 to the property price.\nThe sign of Beta is important, tells us whether it is a positive or negative relationship and we should interpret whether this is a logical relationship. For example, for the PROX_PRIMARY_SCH variable, for every 1 unit increase in the proximity to primary school (i.e. further away from primary school), the properly price increases by $159,856.14. This may not match our logical understanding of this, and may need to be further investigated. For example, it may be data error, or it may be a surprising but true finding (e.g. being too close to the primary school is very noisy and people do not like staying right beside it).\n\n\n\n8.4. Preparing Publication Quality Table: gtsummary Method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note(), as shown in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nInformation on more customisation options is available at Tutorial: tbl_regression.\n\nThe difference between olsrr and gtsummary is that the latter is more elegant with nicer font size.\ngtsummary is an extension of a package called gt.\n\n\n\n8.5. Checking for Multicollinearity\nIn this sub-section, we will use olsrrpackage which is specially programmed for performing OLS regression. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n8.6. Regression Assumptions - Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data points are scattered around the horizontal 0 line, without any distinct pattern. Hence, we can conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n8.7. Regression Assumptions - Test for Normality Assumption\nIt is also important to note that in a linear regression, the residual errors are assumed to be normally distributed. We check this by using ols_plot_resid_hist() of olsrr to perform the normality assumption test, in the code chunk below.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nFor formal statistical test, ols_test_normality() of olsrr package can be used as shown in the code chunk below.\n\nols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is sufficient statistical evidence that the residuals are not normally distributed.\n\n\n8.8. Test for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attribute. Hence, it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\n\nWe use ` ` for field name in tibble file. We know that the field name is called condo.mlr1.residuals by checking the object in the R Environment pane, and rename it to MLR_RES for easier reference.\n\nThen, we convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we use tmap package to display the distribution of the residuals on an interactive map.\nThe code chunk below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nThe code chunks below creates an interactive point symbol map.\n\ntm_shape(mpsz_svy21) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nWe switch the tmap mode back to \"plot\" before proceeding.\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation, as the residuals do not appear to be distributed at random over the geographical space.\nTo validate that our hypothesis, the Moran’s I test will be performed\nFirst, we determine the minimum distance to ensure that all subzones have at least 1 neighbour, using the following steps:\n\nCreate coords by joining the LONGITUDE and LATITUDE columns of condo_resale using cbind() of base R.\nReturn a matrix k1 with the indices of points belonging to the set of the k nearest neighbours of each other using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour subzone number IDs by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. This function returns in the units of the coordinates if the coordinates are projected, and in km otherwise.\nRemove the list structure of the returned object using unlist().\nUse summary() to determine the maximum distance between all nearest neighbour pairs to determine the upper bound for fixed-distance weight matrix.\n\n\ncoords <- cbind(condo_resale$LONGITUDE, condo_resale$LATITUDE)\n\nk1 <- knn2nb(knearneigh(coords))\n\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\n\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.02913 0.05445 0.07053 0.07895 1.44241 \n\n\nAs the maximum distance is 1.44241km, we will round it up and use 1.5km or 1500m as the upper bound to ensure that all subzones have at least 1 neighbour.\nNext, we compute the distance-based weight matrix by using dnearneigh() of spdep.\n\nnb <- dnearneigh(coordinates(condo_resale.sp), \n                 0, 1500, \n                 longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw <- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nThereafter, lm.morantest() of spdep package is used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 2.2 * 1016, which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble positive cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "In-class Exercise 4",
    "section": "9. Building Hedonic Pricing Models Using GWmodel",
    "text": "9. Building Hedonic Pricing Models Using GWmodel\nIn this section, we will model hedonic pricing using both the fixed and adaptive bandwidth schemes.\n\n9.1. Building Fixed Bandwidth GWR Model\n\n9.1.1. Computing Fixed Bandwidth\nIn the code chunk below, bw.gwr() of GWModel is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive = FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be used to determine the stopping rule, they are: cross-validation (CV) approach and AIC corrected (AICc) approach. We define the stopping rule using approach argument.\n\nIn this case, we are using the CV option using approach = \"CV\".\n\n\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + \n                     AGE + \n                     PROX_CBD + \n                     PROX_CHILDCARE + \n                     PROX_ELDERLYCARE  + \n                     PROX_URA_GROWTH_AREA + \n                     PROX_MRT + \n                     PROX_PARK + \n                     PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + \n                     PROX_BUS_STOP + \n                     NO_Of_UNITS + \n                     FAMILY_FRIENDLY + \n                     FREEHOLD, \n                   data = condo_resale.sp, \n                   approach = \"CV\", \n                   kernel = \"gaussian\", \n                   adaptive = FALSE, \n                   longlat = FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405m.\n\nRecommended bandwidth is the last line in the output. The bandwidth is a distance measure. In this case, as the data is in metres, the bandwidth is also in metres unit of measure.\n\n\n\n9.1.2. GWModel Method - Fixed Bandwidth\nNow we can use the code chunk below to calibrate the GWR model using fixed bandwidth and gaussian kernel.\n\nlonglat = TRUE means that the data is in decimal degree format. The algorithm will do the projection backend mathematically.\nlonglat = FALSE means that the data is in projected CRS format.\n\n\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                         AGE +\n                         PROX_CBD +\n                         PROX_CHILDCARE +\n                         PROX_ELDERLYCARE  +\n                         PROX_URA_GROWTH_AREA +\n                         PROX_MRT +\n                         PROX_PARK +\n                         PROX_PRIMARY_SCH +\n                         PROX_SHOPPING_MALL +\n                         PROX_BUS_STOP +\n                         NO_Of_UNITS +\n                         FAMILY_FRIENDLY +\n                         FREEHOLD,\n                       data = condo_resale.sp, \n                       bw = bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-10 15:20:48 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2022-12-10 15:20:49 \n\n\nThe output is saved in a list of class gwrm. The report shows that the adjusted r-square of the GWR is 0.8430, which is significantly better than the globel multiple linear regression model of 0.6472.\n\nThe first part of the report is the global report, i.e. non-geographically weighted. The second part is GW.\nWe compare the AICc values: 42,967.14 vs 42,263.61 -> select the model which gives the smallest AICc value.\nFor how much is the improvement, we look at the adjusted R-square: 0.6472 vs 0.8430417.\n\n\n\n\n9.2. Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the GWR-absed hedonic pricing model by using adaptive bandwidth approach.\n\n9.2.1. Computing Adaptive Bandwidth\nSimilar to the earlier section, we will first use bw.ger() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + \n                        AGE  + \n                        PROX_CBD + \n                        PROX_CHILDCARE + \n                        PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + \n                        PROX_MRT + \n                        PROX_PARK + \n                        PROX_PRIMARY_SCH + \n                        PROX_SHOPPING_MALL + \n                        PROX_BUS_STOP + \n                        NO_Of_UNITS + \n                        FAMILY_FRIENDLY + \n                        FREEHOLD, \n                      data = condo_resale.sp, \n                      approach = \"CV\", \n                      kernel = \"gaussian\", \n                      adaptive = TRUE, \n                      longlat = FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended number of data points to be used.\n\nWe note that it started with 895 CVs, and converged to 30.\nWe use the value 30 in our formula to calculate the matrix in the subsequent section.\n\n\n\n9.2.2. Constructive Adaptive Bandwidth GWR Model\nNow, we can go ahead to calibrate the GWR-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                            AGE + \n                            PROX_CBD + \n                            PROX_CHILDCARE + \n                            PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + \n                            PROX_MRT + \n                            PROX_PARK + \n                            PROX_PRIMARY_SCH + \n                            PROX_SHOPPING_MALL + \n                            PROX_BUS_STOP + \n                            NO_Of_UNITS + \n                            FAMILY_FRIENDLY + \n                            FREEHOLD, \n                          data = condo_resale.sp, \n                          bw = bw.adaptive, \n                          kernel = \"gaussian\", \n                          adaptive = TRUE, \n                          longlat = FALSE)\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-10 15:20:53 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2022-12-10 15:20:54 \n\n\nThe report shows that the adjusted r-square of the GWR is 0.8561 which is significantly better than the global multiple linear regression model of 0.6472. It is also slightly higher than the r-square of the GWR for fixed bandwidth model, which is 0.8430.\n\n\n\n9.3. Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardised residuals have a mean of 0 and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n9.3.1. Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\n\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, \n                                  as.matrix(gwr.adaptive.output))\n\nWe use glimpse() to display the content of condo_resale.sf.adaptive sf data frame.\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                <dbl> 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                <dbl> 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     <dbl> 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                <dbl> 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          <dbl> 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        <dbl> 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    <dbl> 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      <dbl> 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       <dbl> 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                <dbl> 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               <dbl> 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        <dbl> 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    <dbl> 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      <dbl> 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        <dbl> 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           <dbl> 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             <dbl> 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       <dbl> 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 <dbl> -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               <dbl> 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              <dbl> 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   <dbl> -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              <dbl> -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        <dbl> 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      <dbl> -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  <dbl> -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              <dbl> -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             <dbl> -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      <dbl> 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    <dbl> 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         <dbl> 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           <dbl> 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       <dbl> -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              <dbl> 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       <dbl> 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    <dbl> 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                <dbl> 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           <dbl> 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            <dbl> 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             <dbl> 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  <dbl> 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             <dbl> 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       <dbl> 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     <dbl> 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE <dbl> 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             <dbl> 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            <dbl> 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     <dbl> 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   <dbl> 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        <dbl> 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          <dbl> 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      <dbl> 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             <dbl> 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            <dbl> 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             <dbl> 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  <dbl> -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             <dbl> -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       <dbl> 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     <dbl> -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV <dbl> -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             <dbl> -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            <dbl> -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     <dbl> 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   <dbl> 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        <dbl> 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          <dbl> 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      <dbl> -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             <dbl> 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                <dbl> 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               <dbl> 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               <dbl> 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                <POINT [m]> POINT (22085.12 29951.54), POINT (25656.…\n\n\nWe also use summary() to review the summary statistics of yhat of gwr.adaptive.\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n9.3.2. Visualising Local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n9.3.3. Visualising Coefficient Estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsync = TRUE allows us to zoom in/out and scroll both plots in a synchronised manner.\n\n\ntmap_mode(\"plot\")\n\n\n\n9.3.4. Visualising by URA Planning Subzone\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#references",
    "href": "In-class_Ex/In-class_Ex4/In-class_Ex4.html#references",
    "title": "In-class Exercise 4",
    "section": "10. References",
    "text": "10. References\nLu, B., Harris, P., Charlton, M., Brunsdon, C. (2014.) The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models. Geo-spatial Information Science. 17, 85-101. https://doi.org/10.1080/10095020.2014.917453\nHargrave, M. (2021.) Hedonic Pricing: Definition, How the Model Is Used, and Example. Investopedia. https://www.investopedia.com/terms/h/hedonicpricing.asp"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn with Yiting - Geospatial Analytics",
    "section": "",
    "text": "In this webpage, I am going to share with you my learning journey of geospatial analytics."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Water is a scarce resource. With increasing pollution from various commercial activities, it becomes ever so difficult to obtain potable water for drinking, agriculture, sanitation and manufacturing. Particularly in developing countries, the lack of water sanitation has created major issues such as high rate of illness and mortality due to infection and malnutrition.\nThe United Nations (UN) estimate that 1.6 billion or close to 20% of people will lack safely managed drinking water in 2030 (UN, 2022). In Nigeria, an estimated 70% of water at the point of consumption is contaminated, which is a direct cause of Nigeria having the world’s largest number of deaths from waterborne diseases among children under five years old (VOA, 2022).\n\nHence, ensuring the availability and sustainable management of water and sanitation for all remains one of the 17 Sustainable Development Goals (SDGs) of the UN."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#problem-statement-and-objective",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#problem-statement-and-objective",
    "title": "Take-home Exercise 1",
    "section": "2. Problem Statement and Objective",
    "text": "2. Problem Statement and Objective\nIn this study, we aim to perform geospatial analytics to reveal the spatial patterns, including the distribution and autocorrelation, of water points in Nigeria which are non-functional. This will be done using geospatial data on Level-2 Administrative Boundaries and water-points of Nigeria. The results of the analysis is a starting point to help inform policy decisions on the potential placement and maintenance of water points for more effective delivery of potable water to the people of Nigeria"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-pre-preparation",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-pre-preparation",
    "title": "Take-home Exercise 1",
    "section": "3. Data Pre-Preparation",
    "text": "3. Data Pre-Preparation\n\n3.1. Water Point Data\nWe obtained the global water point and small water scheme level data from Water Point Data Exchange (WPdx) Global Data Repositories (WPdx, 2020). We accessed the WPdx-Plus (WPdx+) option and downloaded the full Shapefile under the Export option, as shown in the screenshot below. As the data consists of water points around the world, we will later filter for water points within Nigeria in R in a subsequent step.\n\nAfter downloading the Shapefile which can take a few minutes due to the large file size, unzip the folder and copy the Shapefiles (.dbf, .prj, .shp and .shx) into a data subfolder that shares the same directory as this Quarto file for ease of calling the files. We also want to rename all four files to geo_export so that we can reference these filenames more easily when we import the data.\n\n\n3.2. Geographical Boundaries of Nigeria\nWe also need the geographical boundaries of Nigeria to make meaningful sense of its water point locations and to aid spatial visualisation. Here, we downloaded the Level-2 Administrative Boundaries (also known as Local Government Area (LGA)) data (ADM2) for Nigeria in year 2020 from geoBoundaries, the largest open and free database of political administrative boundaries globally (geoBoundaries, 2022). The screenshot below shows the page for Shapefile data download. One can filter for Nigeria’s data by typing it in under the Name filter, followed by clicking on the download button under the column geoBoundaries, sub-column Full Release and for the row Nigeria, NGA, ADM2, 2020.\n\nSimilar to the water point data, we unzip the folder and copy the Shapefiles (.dbf, .prj, .shp and .shx) into the same folder as the water points Shapefiles. Here, we rename the files to geoBoundaries-NGA-ADM2 to indicate the data source (geoBoundaries), country (NGA) and administrative boundary level (ADM2)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#installing-and-loading-packages-in-r",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#installing-and-loading-packages-in-r",
    "title": "Take-home Exercise 1",
    "section": "4. Installing and Loading Packages in R",
    "text": "4. Installing and Loading Packages in R\nThe code chunk below uses p_load() from pacman package to brings in the R packages for:\n\nSpatial vector data encoding (sf);\nData-wrangling (tidyverse);\nMap plotting (tmap);\nGeospatial analysis (spdep); and\nRapid Exploratory Data Analysis (EDA) (funModeling).\n\n\npacman::p_load(sf, tidyverse, tmap, spdep, funModeling)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#importing-geospatial-data-in-r",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#importing-geospatial-data-in-r",
    "title": "Take-home Exercise 1",
    "section": "5. Importing Geospatial Data in R",
    "text": "5. Importing Geospatial Data in R\n\n5.1. Water Point Geospatial Data\nWe are now ready to import the geospatial data into the Quarto document. The code chunk below does so for the water point data by using st_read() function of the sf package. We specified the data source name (dsn) or directory of the file (\"data/geospatial\"), layer for the name of the Shapefiles (\"geo_export\"), and crs = 4326 to import the data in wgs84 geographic coordinate reference system (CRS), since the Shapefile is in wgs84. We also pipe a filter to obtain data that are in Nigeria only, by using the filter() function of dplyr package from tidyverse. The clean_country_name column is used for the filter, and note that the column name is truncated in the Shapefile due to character limit and should be keyed in correctly to perform the filter successfully.\n\nwp <- st_read(dsn = \"data/geospatial\",\n              layer = \"geo_export\",\n              crs = 4326) %>% \n  filter(clean_coun == \"Nigeria\")\n\nOne point to note is that while we can theoretically transform the data to projected CRS directly by using the st_transform() function of sf to facilitate the accurate computation of distances in a planar configuration, we want to keep it on hold for now as it will result in missing data points when we use st_intersects() subsequently to identify water points within each administrative boundary. This is because st_intersects() only works correctly if the geospatial data are in geographic CRS.\nThe simple feature data frame comprises 95,008 observations of 73 variables. In particular, we are interested in the variable status_clean (truncated to status_cle in the Shapefile), which tells us which water points are functional versus not. In addition, we will use the last variable, geometry, to perform data join for the recoded variables to the LGA boundaries data.\nOn a practical note, to avoid taking up too much memory space in GitHub, which has a memory limit of 100MB, we will extract the necessary data and save them in an rds file, and delete the geo_export Shapefiles from the data/geospatial folder, before committing and pushing the changes to GitHub. This is to prevent error in the process of pushing the commit to GitHub. We do so by running the relevant code chunks below and saving the rds file in the data/spatial folder, and then setting #| eval: false so that the codes that use the original Shapefiles and intermediate large files will not run when knitted. This way, those codes will be suppressed when rendering the Quarto file and analysis can be done using the eventual rds file.\n\nShould we wish to run certain lines of codes that are suppressed, we can set to #| eval: true to allow normal evaluation during rendering, or run it manually in the RStudio environment.\n\nIn the code chunk below, write_rds() of the readr package is used to save the extracted sf data table into an output file in rds data format. We then do not need to go back to the original Shapefile to reload the full set of global water points data each time we use it, as the data size is very large, the time to load is long and it cannot be pushed to GitHub.\n\nwp_nga <- write_rds(wp, \"data/geospatial/wp_nga.rds\")\n\nHowever, do note that after running the above code chunk, the wp_nga.rds file is still too large (140.2MB) to push to GitHub (100MB limit). Hence, we will further extract only the data that we wish to use for our analysis and save it as another .rds file, and remove this one, indicate #| eval: false and delete the wp_nga.rds file from our directory, before we commit and push the changes to GitHub.\n\n\n5.2. Nigeria Level-2 Administrative Boundary Geospatial Data\nWe also import the Nigeria Level-2 Administrative Boundary (LGA) data into our Quarto file, similarly using st_read() of sf in the code chunk below. The data are saved in the form of a simple feature data table nga.\n\nnga <- st_read(dsn = \"data/geospatial\",\n              layer = \"geoBoundaries-NGA-ADM2\",\n              crs = 4326)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\zhuyiting1\\ISSS624\\Take-home_Ex\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nglimpse(nga)\n\nRows: 774\nColumns: 6\n$ shapeName  <chr> \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", \"Abaka…\n$ Level      <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ shapeID    <chr> \"NGA-ADM2-72505758B79815894\", \"NGA-ADM2-72505758B67905963\",…\n$ shapeGroup <chr> \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NG…\n$ shapeType  <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((7.401109 5...., MULTIPOLYGON (…\n\n\nThere are 774 observations of 6 variables in the nga file, including shapeName for the LGA that each region belongs to and geometry for the polygons, as seen using the glimpse() function of dplyr above. The geometry type is multipolygon. It is also in the wgs84 geographic CRS, just like the water point data. Hence for now, there is no need to perform st_transform() to align their CRS.\nWe also run a check for invalid geometries in the LGA data, using st_is_valid() of sf.\n\nlength(which(st_is_valid(nga) == FALSE))\n\n[1] 0\n\n\nThe output is 0 - there is no invalid geometry for the LGA polygons.\nWe also check for missing values in the LGA data, using is.na() of ursa to return TRUE/FALSE values and rowSums() of raster to tally the number of TRUE.\n\nnga[rowSums(is.na(nga))!=0,]\n\nSimple feature collection with 0 features and 5 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  WGS 84\n[1] shapeName  Level      shapeID    shapeGroup shapeType  geometry  \n<0 rows> (or 0-length row.names)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-wrangling-cleaning-and-extracting-the-necessary-data",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#data-wrangling-cleaning-and-extracting-the-necessary-data",
    "title": "Take-home Exercise 1",
    "section": "6. Data Wrangling: Cleaning and Extracting the Necessary Data",
    "text": "6. Data Wrangling: Cleaning and Extracting the Necessary Data\n\n6.1. Recoding of Missing Water Point Status Data\nIn the code chunk below, we use replace_na() of tidyr to replace the “NA” data in the status_cle variable with “Unknown”, as this is the variable that will be used subsequently. This is so that the observations with “NA” will not be excluded in subsequent analyses.\n\nwp_nga <- read_rds(\"data/geospatial/wp_nga.rds\") %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))\n\n\n\n6.2. Exploratory Data Analysis (EDA)\nIn the code chunk below, we use freq() of funModeling to display the distribution of status_cle field in wp_nga for a quick view of the available classes and their distributions. As we need to suppress this code chunk due to file size limit when we commit the changes and push to GitHub, the bar chart is saved as image and reproduced below.\n\nfreq(data = wp_nga,\n     input = \"status_cle\")\n\n\nWe see that there are 3 status_cle values that describe functional water points, namely\n\nFunctional (45,883, 48%),\nFunctional but needs repair (4,579, 5%), and\nFunctional but not in use (1,686, 2%).\n\nOn the other hand, there are 5 values which indicate that the water points are not functional, including 7 mis-coded values due to a missing hyphen and lower “f”, and they are\n\nNon-Functional (29,385, 31%)\nNon-Functional due to dry season (2,403, 3%)\nAbandoned/Decommissioned (234, <1%)\nAbandoned (175, <1%)\nNon functional due to dry season (7, <1%)\n\nWe see that over 1/3 of the water points are non-functional.\nThere are also 10,656 or 11% missing values which we recoded to Unknown using replace_na().\n\n\n6.3. Extracting Water Point Data\nIn this section, we will extract the water point records by using the classes that we saw above in status_cle field. This will help us obtain the absolute numbers as well as allow us to calculate the % total later.\n\n\n6.4. Extracting Functional Water Points\nIn the code chunk below, we extract the data for the functional water points into wpt_functional using filter() of dplyr for the 3 classes that we identified using freq() of funModeling.\n\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nRunning freq() on wpt_functional to check, we can see that the same number of records for the 3 functional classes are captured as per in wp_nga. We similarly saved a copy of the frequency image below as we will suppress the evaluation of the code chunk below due to file size constraint.\n\nfreq(data = wpt_functional,\n     input = \"status_cle\")\n\n\n\n\n6.5. Extracting Non-Functional Water Points\nWe repeat the above process for non-functional water points, using the code chunks below.\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\n\nfreq(data=wpt_nonfunctional, \n     input = 'status_cle')\n\n\n\n\n6.6. Extracting Water Point with Unknown Class\nFinally, for completeness, we also need to extract the water points with unknown status (missing status_cle field), using the code chunk below. Using str() of R’s utils, we confirm that the number of observations (10,656) tallies with that in the earlier frequency bar chart plotted using freq() of funModeling. A screenshot of the top part of the internal structure of wpt_unknown is shown below.\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\nstr(wpt_unknown)\n\n\n\n\n6.7. Performing Point-in-Polygon Count\nWe want to find the number and proportion of functional, non-functional and unknown water points within each LGA. To do this, we use st_intersects() of sf to determine the cross-over between the LGA polygons in nga and water points in wp_nga. Thereafter, lengths() of Base R is used to return the number of water points in each class by LGA. Finally, we use mutate() of dplyr to add the new variables for total wpt, wpt functional, wpt non-functional and wpt unknown to nga sf data table, and assign it to a new variable nga_wp.\n\nnga_wp <- nga %>% \n  mutate(`total wpt` = lengths(\n    st_intersects(nga, wp_nga))) %>%\n  mutate(`wpt functional` = lengths(\n    st_intersects(nga, wpt_functional))) %>%\n  mutate(`wpt non-functional` = lengths(\n    st_intersects(nga, wpt_nonfunctional))) %>%\n  mutate(`wpt unknown` = lengths(\n    st_intersects(nga, wpt_unknown)))\n\nNote that the symbol used is ” ` ” (backtick) and not ” ’ ” (apostrophe). This is used when there is space and hyphen (-) in the variable name (e.g. total wpt).\nThereafter, we compute the percentage functional and percentage non-functional water points as pct_functional and pct_non-functional, using mutate() of dplyr in the code chunk below.\n\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt functional`/`total wpt`) %>%\n  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`)\n\n\n\n6.8 Saving the Analytical Data Table\nWith the tidy sf data table, we save it in rds file format as nga_wp.rds for subsequent analysis, using write_rds() of readr.\n\nwrite_rds(nga_wp, \"data/geospatial/nga_wp.rds\")\n\nBefore we move on to the next section on spatial analysis, we will set #| eval: false for all code chunks that rely on either the geo_export Shapefiles or wp_nga as the files are too large and need to be deleted before committing and pushing the changes to GitHub. We will work with the geoBoundaries-NGA-ADM2 Shapefiles and nga_wp.rds file, which is only around 2.1MB in size respectively."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#visualising-the-spatial-distribution-of-water-points---thematic-mapping",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#visualising-the-spatial-distribution-of-water-points---thematic-mapping",
    "title": "Take-home Exercise 1",
    "section": "7. Visualising the Spatial Distribution of Water Points - Thematic Mapping",
    "text": "7. Visualising the Spatial Distribution of Water Points - Thematic Mapping\nTo avoid error with the removal of the large data files and suppression of the relevant code chunks above, we use read_rds() to load the nga_wp.rds file at the start of the next section of our analysis.\n\nnga_wp <- read_rds(\"data/geospatial/nga_wp.rds\")\n\nAs we have performed st_intersects(), we can use st_transform() of sf to convert the data from an ellipsoid wgs84 CRS to a planar projected CRS via mathematical reprojection of the coordinates, prior to distance calculations. This is done using EPSG: 26392 for Minna / Nigeria Mid Belt (Spatial Reference, 2022), in the code chunk below. We also check that the transformation has been done correctly using st_geometry() of sf, where the projected CRS field indicates Minna / Nigeria Mid Belt.\n\nnga_wp26392 <- st_transform(nga_wp, \n                            crs = 26392)\nst_geometry(nga_wp26392)\n\nGeometry set for 774 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 26662.71 ymin: 30523.38 xmax: 1344157 ymax: 1096029\nProjected CRS: Minna / Nigeria Mid Belt\nFirst 5 geometries:\n\n\nWe see that the bounding box values have changed from the decimal degree format where the minimum and maximum values of x and y were between 2.7o and 14.7o, to between 26,663m and 1,344,157m (MapTools, 2022; epsg.io, 2022).\nWe set tmap_mode() of tmap to “view” to activate interactive viewing mode instead of static maps, to better zoom into any of the 774 LGAs for further analysis if needed. Nevertheless, for the purpose of limiting the file size in view of the limited memory on GitHub, we will use the “plot” option to display static maps in this report.\n\ntmap_mode(\"view\")\n\n\ntmap_mode(\"plot\")\n\nIn the code chunk below, we plot the wgs84 and crs = 26392 versions of the Nigerian LGA using tm_shape() of tmap, and note that the latter appears to flatten the mapping area out a little more than the former. We can also see that the plots are now in interactive mode.\n\nnga_wp_wgs <- tm_shape(nga_wp) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Geographic CRS\",\n            main.title.position = \"center\")\n\nnga_wp_proj <- tm_shape(nga_wp26392) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Projected CRS\",\n            main.title.position = \"center\")\n\ntmap_arrange(nga_wp_wgs, nga_wp_proj, asp = 1, ncol = 2)\n\n\n\n\n\n7.1. Quick Plots by Equal Classification\nThe code chunk below uses qtm() of tmap to do a quick thematic map plot of the Nigeria LGA, coloured by the number of water points in equal classification method. The four quadrants represent (from top left in a “Z” shape) total water points, functional water points, non-functional water points and water points with unknown functionality status.\n\ntotal <- qtm(nga_wp26392, \"total wpt\")\nwp_functional <- qtm(nga_wp26392, \"wpt functional\")\nwp_nonfunctional <- qtm(nga_wp26392, \"wpt non-functional\")\nunknown <- qtm(nga_wp26392, \"wpt unknown\")\n\ntmap_arrange(total, wp_functional, wp_nonfunctional, unknown, asp=1, ncol=2)\n\n\n\n\nBy zooming into the choropleth maps and clicking on the areas of interest, we see that Babura has the most number of total water points at 894. There are 11 LGAs in the Northeast region that are without any water points at all, namely Geidam, Mobbar, Abadam, Kukawa, Guzamala, Nganzai, Gubio, Marte, Kala/Balge, Kaga and Gujba.\nThe general patterns of the functional water points appear similar. We see that in Chikun, there are 4 water points in total but 0 functional ones.\nFor non-functional water points, they are mainly found in Ifelodun (278) and Igabi (216), forming 46% and 74% of all water points found in those regions, suggesting that heavy replacement or maintenance work may be needed there.\nThe water points with unknown status information are mainly found in the Central (e.g. Pankshin, Shendam) and South regions (e.g. Izzi, Ikwo).\nWe also visualise the functional and non-functional water points by their proportions of the total number of water points in each LGA, using the code chunk below.\n\npct_wp_functional <- qtm(nga_wp26392, \"pct_functional\")\npct_wp_nonfunctional <- qtm(nga_wp26392, \"pct_non-functional\")\n\ntmap_arrange(pct_wp_functional, pct_wp_nonfunctional, asp=1, ncol=2)\n\n\n\n\nOn the left plot, we see that the the highest proportion of functional water points are mostly found in the North half of Nigeria. On the right plot, the proportion of non-functional water points tend to be more dispersed in the remaining LGAs. In general, there are less LGAs with high proportion of non-functional water points compared to that of functional ones.\n\n\n7.2. Other Data Classification Methods\nWe also try visualising the spatial distribution of functional and non-functional water point proportions at LGA level using quantile, Jenks (natural breaks), pretty and Kmeans styles in tm_fill() of tmap, by using tm_shape() to create customisable choropleth plots.\n\nFunctional water point proportion\n\nfunc_quantile <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_functional\",\n          n = 5,\n          style = \"quantile\", \n          title = \"Functional: Quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nfunc_jenks <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_functional\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Functional: Jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nfunc_pretty <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_functional\",\n          n = 5,\n          style = \"pretty\", \n          title = \"Functional: Pretty\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nfunc_kmeans <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_functional\",\n          n = 5,\n          style = \"kmeans\", \n          title = \"Functional: Kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\n\ntmap_arrange(func_quantile, func_jenks, func_pretty, func_kmeans, asp = 1, ncol = 2)\n\n\n\n\nThe quantile plot in the top left quadrant shows that the water point proportions are somewhat normally distributed, as the bands are wider at the ends and narrower in the middle.\n\n\nNon-functional water point proportion\n\nnon_func_quantile <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_non-functional\",\n          n = 5,\n          style = \"quantile\", \n          title = \"Non-functional: Quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nnon_func_jenks <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_non-functional\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Non-functional: Jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nnon_func_pretty <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_non-functional\",\n          n = 5,\n          style = \"pretty\", \n          title = \"Non-functional: Pretty\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\nnon_func_kmeans <- tm_shape(nga_wp26392) +\n  tm_fill(\"pct_non-functional\",\n          n = 5,\n          style = \"kmeans\", \n          title = \"Non-functionals: Kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.height = 0.25, \n            legend.width = 0.35,\n            frame = TRUE)\n\ntmap_arrange(non_func_quantile, non_func_jenks, non_func_pretty, non_func_kmeans, asp = 1, ncol = 2)\n\n\n\n\nWe see that the spatial distribution of proportion of non-functional water points is quite different, in that 80% of the LGAs have <55% non-functional water points, from the quantile plot. This is consistent with what we observed in the equal interval plot using qtm() earlier.\n\n\n\n7.3. Calculating and Plotting Water Point Density\nWe can calculate the density of water points in each LGA by land area, by first creating a new Area variable in the data frame, using st_area() of sf in the code chunk below. We then use %>% and mutate() to create the density variables for total, functional, non-functional and unknown water points, using 1,000,000 to convert the scale from per m2 to per km2. Finally, we use summary() of Base R to look at the summary statistics of the densities.\n\nnga_wp26392$Area <- nga_wp26392 %>%\n  st_area()\n\nnga_wp26392 <- nga_wp26392 %>%\n  mutate(`total_density` = `total wpt` / Area * 1000000) %>%\n  mutate(`functional_density` = `wpt functional` / Area * 1000000) %>%\n  mutate(`non-functional_density` = `wpt non-functional` / Area * 1000000) %>%\n  mutate(`unknown_density` = `wpt unknown` / Area * 1000000)\n\nsummary(nga_wp26392[c(\"total_density\", \"functional_density\", \"non-functional_density\", \"unknown_density\")])\n\n total_density      functional_density non-functional_density unknown_density  \n Min.   : 0.00000   Min.   : 0.00000   Min.   :0.00000        Min.   :0.00000  \n 1st Qu.: 0.05131   1st Qu.: 0.02221   1st Qu.:0.01598        1st Qu.:0.00000  \n Median : 0.13951   Median : 0.05989   Median :0.04495        Median :0.00000  \n Mean   : 0.37827   Mean   : 0.21639   Mean   :0.12248        Mean   :0.03940  \n 3rd Qu.: 0.38443   3rd Qu.: 0.17388   3rd Qu.:0.11509        3rd Qu.:0.04186  \n Max.   :13.51065   Max.   :10.42417   Max.   :3.95434        Max.   :0.67632  \n          geometry  \n MULTIPOLYGON :774  \n epsg:26392   :  0  \n +proj=tmer...:  0  \n                    \n                    \n                    \n\n\nWe use customised plotting functions in tmap again to plot the density of the different types water points in each LGA, using the code chunk below.\n\ntotal_den <- tm_shape(nga_wp26392) +\n  tm_fill(\"total_density\",\n          n = 5,\n          style = \"equal\", \n          title = \"Total Density\") +\n  tm_borders(alpha = 0.5)\n\nwp_functional_den <- tm_shape(nga_wp26392) +\n  tm_fill(\"functional_density\",\n          n = 5,\n          style = \"equal\", \n          title = \"Functional Density\") +\n  tm_borders(alpha = 0.5)\n\nwp_nonfunctional_den <- tm_shape(nga_wp26392) +\n  tm_fill(\"non-functional_density\",\n          n = 5,\n          style = \"equal\", \n          title = \"Non-functional Density\") +\n  tm_borders(alpha = 0.5)\n\nunknown_den <- tm_shape(nga_wp26392) +\n  tm_fill(\"unknown_density\",\n          n = 5,\n          style = \"equal\", \n          title = \"Unknown Density\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(total_den, wp_functional_den, wp_nonfunctional_den, unknown_den, asp=1, ncol=2)\n\n\n\n\nWe see that most LGAs have similar in density in the lowest quantile, except for a handful of smaller LGAs such as Kano Municipal (13.5 total water points/km2), Dala (10.3 total water points/km2) and Ibadan North East (9.5 total water points/km2). In terms of the density of non-functional water points, with reference to the plot in the bottom left quadrant, it is most prominent in a similar pool of LGAs such as Dala (4.0 water points/km2), Ibadan North East (3.4 water points/km2) and Kano Municipal (3.1 water points/km2), due to the small area.\nAs the water point density is roughly equally distributed, we can use proportion instead of absolute number of non-functional water points in our subsequent analysis on global and local spatial association. This approach will help us better determine LGAs that have higher proportions of non-functional water points which may suggest a more detailed investigation to be done to address any possible underlying reasons."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#analytical-mapping---overview",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#analytical-mapping---overview",
    "title": "Take-home Exercise 1",
    "section": "8. Analytical Mapping - Overview",
    "text": "8. Analytical Mapping - Overview\nIn this section, we want to conduct spatial autocorrelation analysis to statistically answer questions such as on spatial distribution, clustering and outliers of the water point data in Nigeria, as a form of Confirmatory Data Analysis (CDA). The concepts of spatial autocorrelation are built upon Tobler’s First Law of Geography, which states that “Everything is related to everything else, but near things are more related than distant things.” (Tobler, 1970).\nWe will perform global spatial correlation to answer the high level question on whether the spatial distribution of non-functional water points is random (vis-à-vis having a certain pattern, be it positive or negative correlation between neighbouring regions or having outliers). This requires us to first construct spatial weights of the study area to define the neighbouring relationships between the LGAs, followed by conducting Global Moran’s I and Geary’s C statistical testing. Should there be any pattern discovered, we will perform cluster and outlier analysis using Local Moran’s I testing and Local Indicator of Spatial Association (LISA). Finally, we will conduct hot spot and cold spot area analysis and compute the Gi statistics."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#computing-contiguity-spatial-weights",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#computing-contiguity-spatial-weights",
    "title": "Take-home Exercise 1",
    "section": "9. Computing Contiguity Spatial Weights",
    "text": "9. Computing Contiguity Spatial Weights\nThere are 2 main methods of defining spatial weights:\n\nContiguity method (also known as adjacency method, based on shared boundaries);\n\nQueen’s criteria;\nRook’s criteria; and\n\nDistance method (based on distance between centroids);\n\nFixed weighting scheme (neighbours are those within a fixed distance or bandwidth to the centroid);\nAdaptive weighting scheme (neighbours defined by each region having k neighbours that are nearest to it, distance or bandwidth from each region is different);\nInverse distance (1/dist formula where closer neighbours have larger inverse distance value which is between 0 and 1).\n\n\nWe will explore these methods and determine an appropriate one for further analysis of spatial association.\n\n9.1. Computing Queen’s Contiguity-based Neighbours\nIn the code chunk below, poly2nb() of spdep is used to compute the contiguity weight matrix for the study area. This function builds a neighbours list based on regions with contiguous boundaries. The default option uses the Queen’s criteria to define each LGA’s neighbours (queen = TRUE), which is what we will use here. We will explore Rook’s criteria in the subsequent sub-section. A visual representation of the contiguity criteria is shown in the figure below.\n\n\nwm_q <- poly2nb(nga_wp26392)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\nLink number distribution:\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n  1   2  14  57 125 182 140 122  72  41  12   4   1   1 \n2 least connected regions:\n138 560 with 1 link\n1 most connected region:\n508 with 14 links\n\n\nThere are a total of 4,440 pairs of neighbours between the Nigeria LGAs based on Queen’s definition of neighbours where there must at least 1 shared boundary point between neighbours. Most LGAs have between 3 and 9 neighbours. There is 1 LGA that has 0 neighbour which we should look into. There are also up to 14 neighbours for another LGA (polygon ID 508), possibly a large area surrounded by multiple smaller regions.\nWe can look at the complete contiguity weight matrix using str() of utils in the code chunk below.\n\nstr(wm_q)\n\nOpening wm_q in the R environment, we see that polygon ID 86 has value 0 for its neighbours. We look at the names of the LGAs of polygon ID 86 (least neighbours) and 508 (most neighbours) using the code chunk below.\n\nnga_wp26392$shapeName[c(86, 508)]\n\n[1] \"Bakassi\" \"Mokwa\"  \n\n\nThe LGA with no neighbour is Bakassi, and that with the most number of neighbours (14) is Mokwa.\nPlotting the Nigeria LGAs using tm_shape(), tm_polygons() and tm_text() of tmap, we could visualise the polygons representing the LGAs (tm_polygons() fills the polygons and draws the polygon borders) with text displaying the name of each LGA within. A close-up screenshot of the Bakassi LGA is shown below. It has 0 neighbours by contiguity method as it does not share its borders with any other Nigeria LGAs, and is separated from its closest LGA, Akpabuyo, by the Atlantic Ocean. In fact, since 2008, Bakassi Penisula was transferred from Nigerian control to that of Cameroon, under the Greentree Agreement (Library of Congress, 2013).\n\ntm_shape(nga_wp26392) +\n  tm_polygons() +\n  tm_text(\"shapeName\", size = 1.2)\n\n\nWe can also further look at the neighbours of Mokwa using the code chunk below.\n\nnb508 <- wm_q[[508]]\nnb508 <- nga_wp26392$shapeName[nb508]\nnb508\n\n [1] \"Agaie\"    \"Bida\"     \"Borgu\"    \"Edati\"    \"Edu\"      \"Gbako\"   \n [7] \"Kaiama\"   \"Katcha\"   \"Lavun\"    \"Lokoja\"   \"Mashegu\"  \"Moro\"    \n[13] \"Pategi\"   \"Wushishi\"\n\n\nWe can also display the structure of the distance-bsaed weights matrix by using table() of Base R and card() of spdep. The latter tallies the numbers of neighbours of regions in the neighbours list, and feeds into the former to build a contingency table where each row is an LGA (alphabetically ordered) and “1” is indicated for the number of neighbours that it has (columns). We use head() of utils to limit the output to the first 6 rows.\n\nhead(table(nga_wp26392$shapeName, card(wm_q)))\n\n           \n            0 1 2 3 4 5 6 7 8 9 10 11 12 14\n  Aba North 0 0 0 0 1 0 0 0 0 0  0  0  0  0\n  Aba South 0 0 0 1 0 0 0 0 0 0  0  0  0  0\n  Abadam    0 0 0 1 0 0 0 0 0 0  0  0  0  0\n  Abaji     0 0 0 0 0 0 0 1 0 0  0  0  0  0\n  Abak      0 0 0 0 0 1 0 0 0 0  0  0  0  0\n  Abakaliki 0 0 0 0 0 0 0 1 0 0  0  0  0  0\n\n\nWe use n.comp.nb() of spdep to perform depth first search on neighbours list and confirm that all regions are neighbours to each other (i.e. within a single neighbours list). If any of the regions are disjoint, they will be indicated by a 2nd region etc.\n\nn_comp_q <- n.comp.nb(wm_q)\ntable(n_comp_q$comp.id)\n\n\n  1   2 \n773   1 \n\n\nIn this case, we see that 1 LGA is separate from the rest of the LGAs. This is the LGA which has no neighbour.\n\n\n9.2. Computing Rook’s Contiguity-based Neighbours\nWe repeat the process of deriving contiguity weight matrix, using Rook’s criteria, using the queen = FALSE argument in the code chunk below.\n\nwm_r <- poly2nb(nga_wp26392, queen = FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4420 \nPercentage nonzero weights: 0.7378029 \nAverage number of links: 5.710594 \n1 region with no links:\n86\nLink number distribution:\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n  1   2  14  59 127 181 141 124  66  42  11   4   1   1 \n2 least connected regions:\n138 560 with 1 link\n1 most connected region:\n508 with 14 links\n\n\nThere are a total of 4,420 pairs of neighbours between the Nigeria LGAs based on Rook’s definition of neighbours where there must more than 1 shared boundary point between neighbours. The minimum and maximum numbers of neighbours remain unchanged compared to Queen’s criteria.\nWe can similarly look at the complete contiguity weight matrix using str() of utils in the code chunk below.\n\nstr(wm_r)\n\nConducting the depth first search analysis on the neighbours list by Rook’s criteria, we find the same result that only 1 LGA is disjoint from the remaining LGAs.\n\nn_comp_r <- n.comp.nb(wm_r)\ntable(n_comp_r$comp.id)\n\n\n  1   2 \n773   1 \n\n\n\n\n9.3. Visualising Contiguity-based Neighbours\nA connectivity graph takes a point and displays a line between each pairs of neighbouring points. For this exercise, we need to obtain the points from the polygon geometry data. The most common method used is to obtain the polygon centroids, which we will do using the sf package.\nAs we have earlier projected the data from wgs84 geographical CRS to Minna / Nigeria Mid Belt projected CRS, we can use st_centroid() of sf to find the centroids of each polygon in point format. We then extract and store the longitude and latitude of the points in coords matrix using st_coordinates() of sf.\n\ncoords <- st_coordinates(st_centroid(st_geometry(nga_wp26392)))\nhead(coords)\n\n          X         Y\n1  545623.9  123092.0\n2  543365.1  119791.0\n3 1193361.8 1047639.4\n4  488844.6  533852.2\n5  589858.3  112961.1\n6  639608.2  249771.0\n\n\nWe can plot the Queen’s and Rook’s contiguity-based neighbours map. par() is used to set the parameters for the plots, having the plots in 1 row, 2 columns using the mfrow argument. We set the mode to \"plot\" for static plot to reduce the memory required to render the html file, as we do not require the rest of the plots to be interactive.\n\ntmap_mode(\"plot\")\n\n\npar(mfrow = c(1, 2))\nplot(nga_wp26392$geometry, border = \"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main = \"Queen's Contiguity\")\nplot(nga_wp26392$geometry, border = \"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main = \"Rook's Contiguity\")\n\n\n\n\n\n\n9.4. Row-standardised Queen’s Contiguity Weight Matrix\nAs the neighbours lists from Queen’s and Rook’s contiguity methods are similar, and we do not have a good reason to use >1 shared boundary points as the cutoff criteria (e.g. LGAs with just 2 shared boundary points would be considered neighbours by Rook’s criteria but not Queen’s), we will use Queen’s contiguity-based neighbours to compute the contiguity weight matrix.\nIn the contiguity weight matrix, we assign a spatial weight to each pairs of LGAs in. Each row and each column represent 1 of the 774 LGAs, forming a 774 x 774 matrix. The numbers 1 and 0 are used to indicate between each row and column pair (e.g. neighbour pair 1-2 represented by the value in the first row and second column), whether they are neighbours (1) or not (0). The values along the diagonal (from top left to bottom right) is always 0 as they represent the same region (e.g. 1-1, 2-2, etc.) Such a matrix is symmetrical along the same diagonal.\nAs each region has different number of neighbours, in practice, row-standardised weights are used instead of spatial weights. Row-standardisation is done by dividing the values in each row by the sum of that row, so that the weight is a relative fraction based on the total number of neighbours that the region has (proportion by row sum). Row-standardisation weights matrix ensures that the spatial parameter in many spatial stochastic processes are comparable between models. It is applicable where polygon features are defined by administrative boundaries (ArcGIS Desktop, 2021). While this is the most intuitive way of summarising the neighbours’ values, this approach has a limitation in that polygons along the edges of the study area will base their lagged values on fewer polygons, thus potentially over- or under-estimating the true nature of the spatial correlation in the data. More robust options such as the binary coding could be explored, such as in inverse distance weight (IDW) method.\nThe code chunk below performs row standardisation for spatial weights using the nb2listw() of spdep, with input being wm_q which is an object of class nb. The default style is “W” which is row standardisation. Other styles include “B” for basic binary coding, “C” for globally standaridsation, “U” for C divided by the number of neighbours, and “S” for variance-stablising coding scheme. Here, we use the default style = \"W\" to derive the weight matrix with row standardisation, to align the basis for analysis whether the LGA has many or few neighbours. For the zero.policy, we will set it to TRUE to permit the weights list to be formed with zero-length weights vector, which means that weights vector of zero length are inserted for LGAs without neighbour in the neighbours list, which we saw for Bakassi in the previous sub-section.\n\nset.ZeroPolicyOption(TRUE)\n\n[1] FALSE\n\nrswm_q <- nb2listw(wm_q,\n                   style = \"W\",\n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\n\nWeights style: W \nWeights constants summary:\n    n     nn  S0       S1       S2\nW 773 597529 773 285.0658 3198.414\n\n\n\n\n9.5. Computing Fixed Distance-based Neighbours\nAnother way to define neighbour relationship is to used distance-based matrix. Using dnearneigh() of spdep, neighbours of an LGA are determined based on the Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. As we have projected the data to projected CRS, we will set longlat = FALSE. Circle distances in m will be calculated.\n\n\nDetermining the Cut-off Distance\nFirst, we need to determine the upper limit for distance band by using the following steps:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number IDs by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. This function returns in the units of the coordinates if the coordinates are projected, and in km otherwise. Here, the unit used by projected CRS for Minna / Nigeria Mid Belt is m.\nRemove the list structure of the returned object using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = FALSE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2663   12812   20237   22050   27702   71724 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 71,724 m. We can use 71,730 m as the upper threshold to ensure that all regions will at least have 1 neighbour.\n\n\nComputing Fixed-Distance Weights Matrix\nUsing 71,730 m as the upper bound and 0 km as a lower bound (i.e. all regions with centroids within 71,730 m distance of a particular LGA will be considered its neighbours), we compute the distance weights matrix using dnearneigh() of spdep.\n\nwm_d71 <- dnearneigh(coords, 0, 71730, longlat = FALSE)\nwm_d71\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 17986 \nPercentage nonzero weights: 3.00229 \nAverage number of links: 23.23773 \n\nsummary(wm_d71)\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 17986 \nPercentage nonzero weights: 3.00229 \nAverage number of links: 23.23773 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 5  9 11 22 33 34 33 37 27 34 28 23 17 23 14 14 12 16 11 18 16 13 10 10  6 14 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n 9  6 17 12 13  7  7 10 10  8 13 14 14 11 10  3  2  7  6  7  9  8  4  6  2  3 \n53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 69 \n 2  4  5  6  7  1  3  6  6  8  7  3  3  1  3  1 \n5 least connected regions:\n90 112 123 237 670 with 1 link\n1 most connected region:\n585 with 69 links\n\n\nFor the 774 LGAs in Nigeria, there are a total of 17,986 links (neighbour-neighbour pairs) which are 71,730 m or less between each other, averaging to 23.2 neighbours per LGA.\nBased on the summary report, there are 5 LGAs with only 1 neighbour each. The most connected region has 69 neighbours within the 71,730 m radius, and it is polygon ID 585 (Okigwe), as seen using the code chunk below.\n\nnga_wp26392$shapeName[[585]]\n\n[1] \"Okigwe\"\n\n\nTo look at the number of neighbours for each LGA and their polygon IDs, we can use the str() function of R’s utils package to see its internal structure.\n\nstr(wm_d71)\n\nSimilar to earlier, we can use n.comp.nb() of spdep to perform depth first search on neighbours list and confirm that all LGAs are neighbours to each other (i.e. within a single neighbours list). If any of the LGAs are disjoint, they will be indicated by a second, third, etc. column.\n\nn_comp <- n.comp.nb(wm_d71)\ntable(n_comp$comp.id)\n\n\n  1   2 \n772   2 \n\n\nWe see that there are 2 LGAs which are not connected as neighbours to the remaining 772 LGAs.\n\n\n9.6. Plotting Fixed Distance Weight Matrix\nPlotting wm_d71 using plot() of R graphics shows us the neighbour relationship in map form, and points() of R graphics can show us disjoint neighbours lists in different colours.\n\nplot(nga_wp26392$geometry, border = \"lightgrey\")\nplot(wm_d71, coords, add = TRUE)\npoints(coords, col = n_comp$comp.id, pch = 19)\n\n\n\n\nBased on the plot, we see the 2 separate neighbour list plotted as pink points to the East of Nigeria. They are Bali and Gassol. They are relatively large LGAs which are surrounded by medium size LGAs, and are only neighbours to each other within the 71,730 m radius.\nWe also see that there are 3 broad areas in the extreme North, extreme South and Southwest part of Nigeria which are very dense in the neighbours link. These areas are relatively dense in smaller LGAs, hence each LGA is likely to have more neighbours, compared to the rest parts of the country.\nIf we want to visualise the links of 1st nearest neighbours only and compare that to all neighbour links, we can use the code chunk below. It plots k1 for 1st nearest neighbour links on the left and all neighbours based on 71,730 m distance threshold on the right. Both maps are underlain using the polygon geometries of nga_wp26392.\n\npar(mfrow = c(1, 2))\nplot(nga_wp26392$geometry, border = \"lightgrey\")\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08, main = \"1st nearest neighbours\")\nplot(nga_wp26392$geometry, border = \"lightgrey\")\nplot(wm_d71, coords, add = TRUE, pch = 19, cex = 0.6, main = \"Distance link\")\n\n\n\n\nWe notice that the density of 1st links is similar to that of all neighbours within 71,730 m radius, where the top, bottom and bottom left regions are the most dense with neighbour links.\nUsing nb2list() of spdep, we can assign the fixed distance-based weights matrix based on binary logic (style = \"B\"), in the code chunk below.\n\nwm71_lw <- nb2listw(wm_d71,\n                   style = \"B\", \n                   zero.policy = TRUE)\nwm71_lw\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 17986 \nPercentage nonzero weights: 3.00229 \nAverage number of links: 23.23773 \n\nWeights style: B \nWeights constants summary:\n    n     nn    S0    S1      S2\nB 774 599076 17986 35972 2581680\n\n\n\n\n9.7. Computing Adaptive Distance Weight Matrix\nOne of the characteristics of fixed distance weights matrix is that more densely settled areas (usually urban areas) tend to have more neighbours while less densely settled areas (usually rural areas) tend to have less neighbours. Having many neighbours smooths the neighbour relationship across more neighbours, resulting in “unfair” spatial association analysis between LGAs with more versus less neighbours within a fixed distance radius.\nWe can control the number of neighbours that each region has, by using k-nearest neighbours instead of stipulating a fixed distance threshold for neighbour-neighbour relationship, either accepting asymmetric neighbours or imposing symmetry, using knn2nb() and knearneigh() combination in the code chunk below. This is similar to how we determined the largest distance for 1st neighbour pairs in order to come up with the fixed distance weights matrix, except without needing to compute the distances between each pair.\n\nknn8 <- knn2nb(knearneigh(coords, k = 8))\nknn8\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 6192 \nPercentage nonzero weights: 1.033592 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nWe see that by setting k = 8 so that each region has exactly 8 neighbours, we get a total of 774 * 8 = 6,192 links.\nWe can similarly display the content of the matrix using str() of R’s utils to see the region IDs of all 8 neighbours for each region.\n\nstr(knn8)\n\nThe code chunk below uses n.comp.nb() of spdep to perform depth first search on neighbours list and confirm that all LGAs are neighbours to each other (i.e. within a single neighbours list). If any of the LGAs are disjoint, they will be indicated by a second, third, etc. column.\n\nn_comp <- n.comp.nb(knn8)\ntable(n_comp$comp.id)\n\n\n  1 \n774 \n\n\nWe see that there are all 774 LGAs are connected within a single neighbours list. This contrasts the fixed distance method using 71,730 m as the upper threshold. This is because by increasing the number of neighbour that each LGA has to exactly 8, it increases the chances of them being all inter-connected in one way or another. For example, Bali and Gassol, which are only neighbours to each other under the fixed distance method, are now neighbours to other LGAs which are in turn connected to the rest of the LGAs in a single neighbours list.\nUsing nb2list() of spdep, we can assign the adaptive distance-based weights matrix based on binary logic (style = \"B\"), which works the same as the row-standardised option (style = \"W\"), as the number of neighbours that each LGA has is exactly the same (8 neighbours each). However, for ease of interpretation, we will use “W” style as the weighted sum of the proportion of non-functional water points can be interpreted against a scale of 0-1, since each neighbour contributes 12.5% weightage to the proportion.\n\nrswm_knn8 <- nb2listw(knn8,\n                      style = \"W\", \n                      zero.policy = TRUE)\nrswm_knn8\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 6192 \nPercentage nonzero weights: 1.033592 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\nWeights style: W \nWeights constants summary:\n    n     nn  S0     S1       S2\nW 774 599076 774 174.25 3155.344\n\n\nIn the code chunk below, we use the variable weights of matrix rswm_knn8 to check that the weights have been standardised to 1/8 (0.125) for each of the 8 neighbours for each LGA. We check using the head() function of utils to reveal the weights for the first 6 observations.\n\nhead(rswm_knn8$weights)\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n[[2]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n[[3]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n[[4]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n[[5]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n[[6]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\n\n\n9.8. Plotting Adaptive Distance-based Neighbours\nWe plot the adaptive distance weights matrix using the code chunk below. plot() of R graphics is used to plot the LGA polygons by their borders using nga_wp26392$geometry, with a second plot of knn8 layered on to show the neighbour-neighbour relationship. pch = 19 is the argument for the point size of the centroid of each LGA, given by the coords. col = \"red\" defines the colour of the neighbour-neighbour relationship displayed.\n\nplot(nga_wp26392$geometry, border = \"lightgrey\")\nplot(knn8, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nWe observe that while the densest regions are still in the North, South and Northwest, it is much less compared to that in the fixed distance method. This is because the number of neighbours of each LGA is fixed at 8, resulting in a total of 6,192 neighbour links, which is just about 1/3 of the 17,986 links from the fixed distance method at 71,730 m cutoff.\n\n\n9.9. Computing Inverse Distance Weight (IDW) Matrix\nWe can also derive spatial weight matrix based on inverse distance method.\nThe code chunk below computes the distance between areas by using nbdists() of spdep, similar to how we computed the distances between 1st neighbour pairs in the fixed distance weights matrix approach. We then use lapply() of base R to apply the function to inverse the distance computed (1/dist). Here, we use wm_q, which defines neighbouring relationship using Queen’s contiguity method, as a basis for neighbour links. We read the first 6 observations of the ids object using head() of utils as the full output list of IDW for 774 LGAs is too long.\n\ndist <- nbdists(wm_q, coords, longlat = FALSE)\nids <- lapply(dist, function(x) 1/(x))\nhead(ids)\n\n[[1]]\n[1] 2.500091e-04 9.047885e-05 1.074832e-04 9.376298e-05\n\n[[2]]\n[1] 2.500091e-04 8.863439e-05 1.410608e-04\n\n[[3]]\n[1] 1.584784e-05 1.656692e-05 1.257362e-05\n\n[[4]]\n[1] 1.716918e-05 3.003258e-05 1.330532e-05 2.169091e-05 5.979524e-05\n[6] 4.456967e-05 1.353021e-05\n\n[[5]]\n[1] 5.994962e-05 5.404533e-05 4.763619e-05 3.801725e-05 6.151752e-05\n\n[[6]]\n[1] 3.032934e-05 3.666317e-05 3.520442e-05 5.109169e-05 3.849929e-05\n[6] 2.871852e-05 1.682539e-05\n\n\nWe see that the inverse distance values are small (on the scale of 0.0001), as the distance is in m and are very large for each LGA.\nUsing nb2listw() of spdep and arguments glist = ids and style = \"B\" so that each neighbour link is assigned the weight that is equal to the inverse distance calculated, we derive the IDW matrix via the code chunk below.\n\nrswm_ids <- nb2listw(wm_q, glist = ids, style = \"B\", zero.policy = TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\n\nWeights style: B \nWeights constants summary:\n    n     nn        S0           S1           S2\nB 773 597529 0.1822073 2.605823e-05 0.0002517888\n\n\nThe weights assigned to each neighbour is no longer uniform across neighbours for the same region, but are standardised based on the inverse of the distance between the centroids of the region and each neighbour. We use summary() and unlist() of base R for the summary statistics of the weights by IDW method using the code chunk below.\n\nsummary(unlist(rswm_ids$weights))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n5.360e-06 2.051e-05 3.156e-05 4.104e-05 4.825e-05 3.756e-04 \n\n\nThe minimum, maximum, mean, median and interquartile range of the IDW weights are returned. We can potentially use some of these as thresholds in determining other suitable cut-offs inverse distance thresholds for neighbour definition if necessary.\n\n\n9.10. Evaluation of Spatial Weighting Methods in the case of Nigeria\nBefore diving into the spatial association analysis, we weigh the pros and cons of the spatial weighting methods discussed, in the context of the problem statement of this study which is on the evaluation of spatial association between the proportion of non-functional water points by LGAs. The assessment is listed in the table below.\n\n\n\n\n\n\n\n\nSpatial Weighting Methods\nPros\nCons\n\n\n\n\nContiguity method\nEffective when polygons are similar in size and distribution.\nAlso effective when spatial relationships are a function of polygon proximity.\nConcept of shared boundary is more intuitive.\nThere are 4,440 neighbour links in total using Queen’s criteria. The spread of neighbour count is appropriately at 14 or less (not too many neighbours for a single LGA).\nThe applicability of the concept of shared boundary depends on how water points are governed.\n\nE.g., if the local government of each LGA is in charge of maintaining the water points within the LGA, contiguity method would be very applicable in answering questions on spatial autocorrelation in the proportion of non-functional between LGAs.\n\nThere is 1 LGA, Bakassi, which does not have any neighbour as it does not share boundary with any other LGA in Nigeria.\n\n\nFixed distance method\nWorks well for point data.\nOften effective for polygon data where there is large variation in polygon size as a consistent scale of analysis is used.\nThe number of neighbours vary widely between 1 and 69, using 71,730 m as the threshold for fixed distance weights. Too many neighbours may be captured in some instances which would affect the spatial autocorrelation analysis.\nTotal number of neighbour links is large at 17,986, hence analysis using this approach would be more computationally demanding than Queen’s contiguity method.\n\n\nAdaptive distance method\nEffective for ensuring a minimum number of neighbours for analysis, such as assigning 8 neighbours to all LGAs as a rule of thumb.\nWorks well when values associated with features are skewed (not normally distributed), which we saw is true in our case for the problem statement relating to the proportion of non-functional water points (80% of LGAs have <54% non-functional water points).\nIntermediate in terms of the total number of neighbour links (6,192), which is more than Queen’s contiguity method (4,440) but less than fixed distance method (17,986).\nDo not need to be row-standardised as each LGA is assigned the same number of neighbours.\nTends to smooth out for smaller areas but appear more choppy for larger areas.\n\n\nInverse distance method\nWorks well for modeling processes where the closer 2 features are in space, the more likely they are to interact or influence each other. E.g., water points in close proximity may be subject to similar weather conditions such as extreme dry weather.\nThis method is computationally intensive as theoretically, every LGA is neighbour of another LGA, hence there are 774 x 774 = 599,076 neighbour links.\n\nIn this study, we chose to use Queen’s contiguity-based weights to compute the IDW matrix to limit the number of neighbours of each LGA.\n\n\n\n\nOn balance, adaptive distance method is selected for the subsequent spatial association analysis. This is because it has an intermediate number of neighbour links (which can also be adjusted by changing the “k” in k nearest neighbours), unlike contiguity-based methods which have fixed number of neighbours based on shared boundaries and fixed distance method which will not go below 17,986 if we require there to be at least 1 neighbour for all LGAs. Furthermore, we have already observed that the proportion of non-functional water points does not follow a normal distribution in Nigeria by LGAs. On the other hand, the underlying assumptions for contiguity-based (governance of water points by LGAs or otherwise) and inverse distance methods (whether points in close proximity are affected by similar factors in relation to proportion of non-functional water points) are not addressed based on the information available so far and would need to be studied further."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#global-spatial-autocorrelation",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#global-spatial-autocorrelation",
    "title": "Take-home Exercise 1",
    "section": "10. Global Spatial Autocorrelation",
    "text": "10. Global Spatial Autocorrelation\n\n10.1. Global Spatial Autocorrelation: Moran’s I\nNext, we will perform Moran’s I statistical testing using moran.test() of spdep. (Statistics How To, 2022). Moran’s I describes how features differ from the values in the study area as a whole. The formula to compute Global Moran’s I is given as follows:\n\n, where\n\\(wij\\) refers to the spatial weight between region i and region j,\n\\(xi\\) refers to the observed value (in our case number of non-functional water points) of region i,\n\\(xj\\) refers to the observed value of region j,\n\\(x bar\\) refers to the mean of all observed values (excluding the diagonal in the weight matrix), and\n\\(n\\) refers to the number of observed values or regions.\nThe interpretation of Moran’s I (Z-value), which is between 1 and -1, is as follows:\n\nPositive I (I > 0): Clustered. Neighbouring observations tend to be similar.\nNegative I (I < 0): Dispersed. Neighbouring observations tend to be dissimilar.\nApproximately 0: Observations are arranged randomly over space (no spatial autocorrelation).\n\n\nMoran’s I Statistical Testing\nThe code chunk below performs Moran’s I statistical testing. Using an upper-tailed test, the null and alternative hypotheses are as follows:\n\nH0: The observed spatial patterns of proportion of non-functional water points in Nigeria are not clustered (i.e. either random or dispersed).\nH1: The observed spatial patterns of proportion of non-functional water points in Nigeria are clustered.\n\n\nmoran.test(nga_wp26392$`pct_non-functional`,\n           listw = rswm_knn8,\n           zero.policy = TRUE,\n           alternative = \"greater\",\n           na.action = na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nga_wp26392$`pct_non-functional`  \nweights: rswm_knn8 \nomitted: 3, 86, 241, 250, 252, 261, 400, 406, 447, 473, 492, 507, 526   \n\nMoran I statistic standard deviate = 26.176, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.4489103292     -0.0013157895      0.0002958437 \n\n\nAt 5% significance level, we note that p-value is 2.2 x 10-16 < 0.05, and we reject H0. Hence, there is sufficient evidence to support that the observed spatial patterns of the proportion of non-functional water points in Nigeria are clustered. This is supported by a positive Moran’s I statistic of 0.449 which indicates positive clustering (i.e. LGAs with higher % non-functional water points tend to be geographically clustered/neighbours).\n\n\nMonte Carlo Moran’s I\nWhen we doubt that the assumptions of Moran’s I (i.e. normality and randomisation) are true, we can use a Monte Carlo simulation under the assumption of no spatial pattern and assigning all regions the mean value. We then compare the actual Moran’s I to that of the randomly simulated distribution to obtain the p-value (pseudo significance).\nThe code chunk below performs permutation test for Moran’s I statistics by using moran.mc() of spdep. A total of 1000 simulations will be performed with the seed number 1234.\n\nset.seed(1234)\nbperm = moran.mc(nga_wp26392$`pct_non-functional`,\n                 listw = rswm_knn8,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 alternative = \"greater\",\n                 na.action = na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nga_wp26392$`pct_non-functional` \nweights: rswm_knn8 \nomitted: 3, 86, 241, 250, 252, 261, 400, 406, 447, 473, 492, 507, 526 \nnumber of simulations + 1: 1000 \n\nstatistic = 0.44891, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nUsing an upper-tailed test, we see that p-value = 0.001 is still < 0.05. We similarly reject H0 and conclude that at 5% significance level, there is sufficient evidence to support that the spatial distribution of % non-functional water points is positively clustered in Nigeria.\n\n\nVisualising Monte Carlo Moran’s I\nIn the code chunk below, we study the summary statistics of the simulated Moran’s I test statistics using mean() of Base R, var() of mvp and summary() of Base R, and visualise it by plotting the distribution of the statistical values as a histogram using hist() and abline() of R Graphics to plot the histogram of the simulated Moran’s I and draw a cutoff line at value 0 (complete randomness) respectively. We will also use the relevant functions in ggplot2 for a more customisable version of the plot (e.g. editing the title of the plot and possibility of adjusting the visual styles). The line-by-line explanations of the codes and arguments are included as comments in the code chunks of the plots.\n\nmean(bperm$res[1:1000])\n\n[1] -0.001010919\n\n\n\nvar(bperm$res[1:1000])\n\n[1] 0.0004899894\n\n\n\nsummary(bperm$res[1:1000])\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.065357 -0.012534 -0.002440 -0.001011  0.010275  0.448910 \n\n\nWe see that by Monte Carlo simulation, the mean and median Moran’s I value are slightly negative at -0.001 and -0.002 respectively. The variance is small at 0.0005.\n\nhist(bperm$res, # plotting simulated Moran's I values\n     freq = TRUE, # histogram is a representation of frequencies, i.e. counts\n     breaks = 100, # breaks it into 100 cells or bins \n     xlab = \"Simulated Moran's I\") # x-axis label\nabline(v = 0, # plotting vertical line at value 0\n       col = \"red\") # plot the vertical line in red colour\n\n\n\n\n\ndf <- data.frame(bperm$res) # converting the simulated Moran's I values into a data frame\nggplot(df,\n       aes(x = bperm$res)) + # plotting the simulated Moran's I values\n  geom_histogram(binwidth = 0.005, # 100 bins\n                 boundary = 0, # shift the boundary of the bin align with value 0 (rather than let value 0 cut the bin in half)\n                 color = \"black\", # colour of borders of bins\n                 fill = \"grey\") + # colour of fill of bins\n  geom_vline(xintercept = 0, # plotting a vertical line at x-axis = 0\n             color = \"red\") + # colour the vertical line red\n  labs(title = \"Histogram of simulated Moran's I (adaptive distance weight matrix)\", # title of plot\n       x = \"Simulated Moran's I\", # x-axis label\n       y = \"Frequency\") + # y-axis label\n  theme_bw() + # black-and-white theme \n  theme(panel.grid.major = element_blank(), # remove the default grids (major) that are included in ggplot\n        panel.grid.minor = element_blank(), # remove the default grids (minor) that are included in ggplot\n        plot.title = element_text(hjust = 0.4)) # specifying text size of plot title \n\n\n\n\nWe see that the maximum value of 0.449 from the summary statistics, which is the Moran’s I computed from the row-standardised weight matrix of adaptive distance method (rswm_knn8), appear on the extreme right of the histogram plot. The plot shows visually how the Moran’s I value is unlikely to be by chance against randomly simulated values, hence statistically significant at 0.05 significance level.\n\n\n\n10.2. Global Spatial Autocorrelation: Geary’s C\nIn this section, we will perform Geary’s C statistical testing by using the geary.test() function of spdep. Geary’s C describes how features differ from their immediate neighbours. The formula for calculating Global Geary’s C is given below.\n\nThe interpretation of Geary’s C (Z-value), which is between 0 and 2, is as follows:\n\nSmall C (< 1): Clustered. Neighbouring observations tend to be similar.\nLarge C (> 1): Dispersed. Neighbouring observations tend to be dissimilar.\nC = 1: Observations are arranged randomly over space (no spatial autocorrelation).\n\nNote that the direction which represents clustering is opposite for Moran’s I as for Geary’s C.\n\nGeary’s C Statistical Testing\nThe null and alternative hypotheses are similar to that in Moran’s I. However, as Geary’s C test cannot take NA values (geary.test() has no argument for na.action = na.omit, unlike moran.mc()) which are present in the pct_non-functional field where there are no water points in certain area, we will use the absolute number of non-functional water points for this analysis.\n\nH0: The observed spatial patterns of number of non-functional water points in Nigeria are not clustered (i.e. either random or dispersed).\nH1: The observed spatial patterns of number of non-functional water points in Nigeria are clustered.\n\n\ngeary.test(nga_wp26392$`wpt non-functional`,\n           listw = rswm_knn8,\n           zero.policy = TRUE,\n           alternative = \"greater\")\n\n\n    Geary C test under randomisation\n\ndata:  nga_wp26392$`wpt non-functional` \nweights: rswm_knn8 \n\nGeary C statistic standard deviate = 19.417, p-value < 2.2e-16\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n     0.6045492320      1.0000000000      0.0004147634 \n\n\nWith a p-value = 2.2 x 10-16 < 0.05, at 5% significant level and 95% confidence level, we reject H0. Hence, there is sufficient evidence to support that the observed spatial patterns of the number of non-functional water points in Nigeria are clustered. This is supported by a small Geary’s C statistic of 0.691 < 1. This implies that LGAs with higher number of non-functional water points tend to be geographically clustered/neighbours.\n\n\nMonte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic using geary.mc() of spdep. We similarly set seed number of 1234 for the simulation for reproducible results.\n\nset.seed(1234)\nbperm = geary.mc(nga_wp26392$`wpt non-functional`,\n                 listw = rswm_knn8,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 alternative = \"greater\")\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  nga_wp26392$`wpt non-functional` \nweights: rswm_knn8 \nnumber of simulations + 1: 1000 \n\nstatistic = 0.60455, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe Monte Carlo simulation returned similar result of a significant p-value (0.001) at 5% significance level. Hence, we reject H0 and conclude that there is sufficient evidence to support that the spatial distribution of the number of non-functional water points is positively clustered in Nigeria.\n\n\nVisualising Monte Carlo Geary’s C\nLike we did for Moran’s I, we will plot a histogram to reveal the distribution of the simulated values of Geary’s C by using the following code chunks.\n\nsummary(bperm$res[1:1000])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6045  0.9857  0.9996  0.9988  1.0127  1.0690 \n\n\nSimilar to Moran’s I which is the maximum among the simulated values, the Geary’s C is the minimum compared to the simulated values.\nDoing a quick plot using hist() and abline() of R Graphics in the code chunk below, we see that the Geary’s C statistics is at the extreme left of the plot, some distance away from the rest of the simulated values which is distributed rather narrowly around value 1 which represents randomness.\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 100,\n     xlab = \"Simulated Geary's C\")\nabline(v = 1,\n       col = \"red\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatial-correlogram",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#spatial-correlogram",
    "title": "Take-home Exercise 1",
    "section": "11. Spatial Correlogram",
    "text": "11. Spatial Correlogram\nSpatial correlograms are used to examine patterns of spatial autocorrelation. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance. Row-standardisation is not needed.\n\n11.1. Moran’s I Correlogram\nIn the code chunk below, sp.correlogram() of spdep is used to compute a 1- to 10-lag spatial correlogram of the number of non-functional water points. The global spatial autocorrelation used is Moran’s I. The plot() of R’s base Graph is used to plot the output.\n\nMI_corr <- sp.correlogram(knn8,\n                          nga_wp26392$`wpt non-functional`,\n                          order = 10,\n                          method = \"I\",\n                          style = \"W\")\nplot(MI_corr)\n\n\n\n\nFrom the plot, Moran’s I is positive for up to 7 lags, beyond which Moran’s I drops below 0. It is also noted that the rate of decrease of Moran’s I slows between 2 and 7 lags. This means that the number of non-functional water points has a wider spread across order 2-7 LGAs.\nIn addition to this, it is necessary to examine the full statistical report as not all autocorrelation values are statistically significant. This is done using the print() function of Base R in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for nga_wp26392$`wpt non-functional` \nmethod: Moran's I\n            estimate expectation    variance standard deviate Pr(I) two sided\n1 (774)   3.8301e-01 -1.2937e-03  2.8620e-04          22.7165       < 2.2e-16\n2 (774)   2.4971e-01 -1.2937e-03  1.4425e-04          20.8986       < 2.2e-16\n3 (774)   1.7521e-01 -1.2937e-03  9.8556e-05          17.7796       < 2.2e-16\n4 (774)   1.1511e-01 -1.2937e-03  7.3007e-05          13.6237       < 2.2e-16\n5 (774)   7.3431e-02 -1.2937e-03  5.7167e-05           9.8830       < 2.2e-16\n6 (774)   3.3089e-02 -1.2937e-03  4.7503e-05           4.9886       6.083e-07\n7 (774)   2.5333e-03 -1.2937e-03  4.1561e-05           0.5936       0.5527628\n8 (774)  -2.3521e-02 -1.2937e-03  3.7193e-05          -3.6447       0.0002677\n9 (774)  -5.7915e-02 -1.2937e-03  3.4219e-05          -9.6795       < 2.2e-16\n10 (774) -9.0564e-02 -1.2937e-03  3.2638e-05         -15.6260       < 2.2e-16\n            \n1 (774)  ***\n2 (774)  ***\n3 (774)  ***\n4 (774)  ***\n5 (774)  ***\n6 (774)  ***\n7 (774)     \n8 (774)  ***\n9 (774)  ***\n10 (774) ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAt 5% significance level, the autocorrelation values are statistically significant for all lags between 1 and 10, except 7-lag where Moran’s I is close to 0, suggesting randomness.\n\n\n11.2. Geary’s C Correlogram\nIn the code chunk below, we perform a similar analysis using the sp.correlogram() function from the spdep package, except using Geary’s C global spatial autocorrelation. We also plot the output using plot() from R’s base Graph, and print() of Base R the full report for the p-values.\n\nGC_corr <- sp.correlogram(knn8,\n                          nga_wp26392$`wpt non-functional`,\n                          order = 10,\n                          method = \"C\",\n                          style = \"W\")\nplot(GC_corr)\n\n\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for nga_wp26392$`wpt non-functional` \nmethod: Geary's C\n           estimate expectation   variance standard deviate Pr(I) two sided    \n1 (774)  0.60454923  1.00000000 0.00041476         -19.4175       < 2.2e-16 ***\n2 (774)  0.73870958  1.00000000 0.00030714         -14.9092       < 2.2e-16 ***\n3 (774)  0.83010250  1.00000000 0.00031562          -9.5632       < 2.2e-16 ***\n4 (774)  0.90114899  1.00000000 0.00026699          -6.0497       1.451e-09 ***\n5 (774)  0.94956303  1.00000000 0.00024652          -3.2123        0.001317 ** \n6 (774)  0.97528831  1.00000000 0.00024335          -1.5841        0.113168    \n7 (774)  1.01619690  1.00000000 0.00026266           0.9994        0.317608    \n8 (774)  1.05667901  1.00000000 0.00031434           3.1969        0.001389 ** \n9 (774)  1.07820775  1.00000000 0.00037455           4.0410       5.321e-05 ***\n10 (774) 1.10044556  1.00000000 0.00041110           4.9540       7.270e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results for Geary’s C correlogram are similar to that for Moran’s I in that positive clustering pattern for the number of non-functional water points is observed for up to 6 lags, where Geary’s C is <1 and Moran’s I is > 0. At 7-lag, the spatial distribution is approximately random. Beyond which, the spatial pattern reverses to negative clustering pattern (dispersion).\nHowever, unlike Moran’s I correlogram, Geary’s C correlogram has p-values which are significant for 1- to 10-lag except both 6- and 7-lag, at 5% significance level. Nevertheless, the overall trend is similar and we can interpret the slowing of the increase in Geary’s C to have a wider spread as the number of lags increases."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#cluster-and-outlier-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#cluster-and-outlier-analysis",
    "title": "Take-home Exercise 1",
    "section": "12. Cluster and Outlier Analysis",
    "text": "12. Cluster and Outlier Analysis\nLocal Indicator of Spatial Association (LISA) is a subset of localised geospatial statistics methods for analysing the location-related tendency (clusters or outliers) in the attributes of geographically referenced data (points or area). The LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation. The sum of LISAs for all observations is proportional to a global indicator of spatial association.\nWe will apply local Moran’s I to detect clusters and/or outliers from the water point data in Nigeria.\n\n12.1. Local Moran’s I\nThe code chunk below computes the local Moran’s I using the localmoran() function of the spdep package. We will use the row-standardised weights matrix here. We will similarly use the number of non-functional water points instead of proportion due to NA value handling.\n\nfips <- order(nga_wp26392$`wpt non-functional`)\nlocalMI <- localmoran(nga_wp26392$`wpt non-functional`, \n                      rswm_knn8, \n                      zero.policy = TRUE)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.080696263 -9.995243e-04 9.573134e-02 -0.2575808    0.796730455\n2 -0.022923567 -4.092463e-05 3.923396e-03 -0.3653214    0.714871500\n3  1.258199847 -1.627684e-03 1.557965e-01  3.1917746    0.001414017\n4 -0.031922684 -5.427505e-05 5.203215e-03 -0.4417988    0.658634818\n5  0.091666434 -2.590965e-04 2.483385e-02  0.5833297    0.559671349\n6  0.007875149 -1.538445e-07 1.474949e-05  2.0505897    0.040306916\n\n\nWe obtain the following statistics for the first 6 output:\n\nIi: Local Moran statistic\nE.Ii: Expectation of local Moran statistic\nVar.Ii: Variance of local Moran statistic\nZ.Ii: Standard deviate of local Moran statistic\nPr(): p-value of local Moran statistic\n\nThe code chunk below can help to list the content of the local Moran matrix derived using printCoefmat() function from R’s Stats package. For the purpose of this report, we will set #| eval: false to avoid displaying the full list of statistics for all 774 LGAs, but to show a screenshot of the first 6 observations below.\n\nprintCoefmat(data.frame(localMI[fips,]))\n\n\nWe will map the Local Moran’s I in the next sub-section to visualise the LGAs by their p-values.\n\n\n12.2. Mapping Local Moran’s I\nBefore mapping the local Moran’s I, we want to append the local Moran’s I dataframe (i.e. localMI) to the nga_wp26392 SpatialPolygonDataFrame. The code chunk below does this using the cbind() function from R base which combine the vectors as columns in the final matrix. We also rename the p-value (Pr.z….E.Ii) variable title to Pr.Ii for neatness.\n\nnga_wp26392.localMI <- cbind(nga_wp26392, localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\nnga_wp26392.localMI\n\nSimple feature collection with 774 features and 21 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 26662.71 ymin: 30523.38 xmax: 1344157 ymax: 1096029\nProjected CRS: Minna / Nigeria Mid Belt\nFirst 10 features:\n        shapeName Level                    shapeID shapeGroup shapeType\n1       Aba North  ADM2 NGA-ADM2-72505758B79815894        NGA      ADM2\n2       Aba South  ADM2 NGA-ADM2-72505758B67905963        NGA      ADM2\n3          Abadam  ADM2 NGA-ADM2-72505758B57073987        NGA      ADM2\n4           Abaji  ADM2 NGA-ADM2-72505758B61968000        NGA      ADM2\n5            Abak  ADM2 NGA-ADM2-72505758B39432389        NGA      ADM2\n6       Abakaliki  ADM2 NGA-ADM2-72505758B36739173        NGA      ADM2\n7  Abeokuta North  ADM2 NGA-ADM2-72505758B86358915        NGA      ADM2\n8  Abeokuta South  ADM2 NGA-ADM2-72505758B56925175        NGA      ADM2\n9             Abi  ADM2  NGA-ADM2-72505758B1616690        NGA      ADM2\n10    Aboh-Mbaise  ADM2 NGA-ADM2-72505758B78555816        NGA      ADM2\n   total.wpt wpt.functional wpt.non.functional wpt.unknown pct_functional\n1         17              7                  9           1      0.4117647\n2         71             29                 35           7      0.4084507\n3          0              0                  0           0            NaN\n4         57             23                 34           0      0.4035088\n5         48             23                 25           0      0.4791667\n6        233             82                 42         109      0.3519313\n7         34             16                 15           3      0.4705882\n8        119             72                 33          14      0.6050420\n9        152             79                 62          11      0.5197368\n10        66             18                 26          22      0.2727273\n   pct_non.functional             Area      total_density functional_density\n1           0.5294118   18683264 [m^2] 0.90990526 [1/m^2] 0.37466687 [1/m^2]\n2           0.4929577   43293877 [m^2] 1.63995477 [1/m^2] 0.66984068 [1/m^2]\n3                 NaN 3940900450 [m^2] 0.00000000 [1/m^2] 0.00000000 [1/m^2]\n4           0.5964912  832227367 [m^2] 0.06849090 [1/m^2] 0.02763668 [1/m^2]\n5           0.5208333  178106187 [m^2] 0.26950215 [1/m^2] 0.12913645 [1/m^2]\n6           0.1802575  463152896 [m^2] 0.50307361 [1/m^2] 0.17704737 [1/m^2]\n7           0.4411765  774688813 [m^2] 0.04388859 [1/m^2] 0.02065345 [1/m^2]\n8           0.2773109   56726506 [m^2] 2.09778476 [1/m^2] 1.26924792 [1/m^2]\n9           0.4078947  267817253 [m^2] 0.56755119 [1/m^2] 0.29497726 [1/m^2]\n10          0.3939394  176198100 [m^2] 0.37457839 [1/m^2] 0.10215774 [1/m^2]\n   non.functional_density     unknown_density           Ii          E.Ii\n1      0.48171455 [1/m^2] 0.053523839 [1/m^2] -0.080696263 -9.995243e-04\n2      0.80842841 [1/m^2] 0.161685681 [1/m^2] -0.022923567 -4.092463e-05\n3      0.00000000 [1/m^2] 0.000000000 [1/m^2]  1.258199847 -1.627684e-03\n4      0.04085422 [1/m^2] 0.000000000 [1/m^2] -0.031922684 -5.427505e-05\n5      0.14036570 [1/m^2] 0.000000000 [1/m^2]  0.091666434 -2.590965e-04\n6      0.09068280 [1/m^2] 0.235343449 [1/m^2]  0.007875149 -1.538445e-07\n7      0.01936261 [1/m^2] 0.003872523 [1/m^2]  0.158989521 -6.654187e-04\n8      0.58173863 [1/m^2] 0.246798207 [1/m^2]  0.113892231 -6.950696e-05\n9      0.23150114 [1/m^2] 0.041072783 [1/m^2]  0.363969643 -3.916720e-04\n10     0.14756118 [1/m^2] 0.124859462 [1/m^2]  0.191048508 -2.288125e-04\n         Var.Ii       Z.Ii       Pr.Ii                       geometry\n1  9.573134e-02 -0.2575808 0.796730455 MULTIPOLYGON (((548795.5 11...\n2  3.923396e-03 -0.3653214 0.714871500 MULTIPOLYGON (((541412.3 12...\n3  1.557965e-01  3.1917746 0.001414017 MULTIPOLYGON (((1248985 104...\n4  5.203215e-03 -0.4417988 0.658634818 MULTIPOLYGON (((510864.9 57...\n5  2.483385e-02  0.5833297 0.559671349 MULTIPOLYGON (((594269 1209...\n6  1.474949e-05  2.0505897 0.040306916 MULTIPOLYGON (((660767 2522...\n7  6.375306e-02  0.6323126 0.527182630 MULTIPOLYGON (((78621.56 37...\n8  6.663359e-03  1.3960869 0.162688338 MULTIPOLYGON (((95752.33 34...\n9  3.753596e-02  1.8806523 0.060019238 MULTIPOLYGON (((632244.2 21...\n10 2.193186e-02  1.2915931 0.196498081 MULTIPOLYGON (((540081.3 15...\n\n\nThe code chunk below plots the local Moran’s I values and their statistical significance (based on p-values) using the choropleth mapping functions from the tmap package.\n\nlocalMI.map <- tm_shape(nga_wp26392.localMI) +\n  tm_fill(col = \"Ii\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(nga_wp26392.localMI) +\n  tm_fill(col = \"Pr.Ii\",\n          breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette = \"-Blues\",\n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp = 1, ncol = 2)\n\n\n\n\nOn the left, we note regions of positive (blue) and negative (orange) Moran’s I statistics, indicative of positive and negative clustering relationships. Zooming in, we see that Ifelodun, Igabi and Kudan have the highest Local Moran’s I statistics.\nOn the right, we see that the p-values are significant at 5% significance level for the regions in top 3 darkest shades of blue. Besides LGAs with high Local Moran’s I above 6 that we listed, where the p-values are statistically significant, we see that the East, South, Southwest and Central regions generally have statistically significant p-values for the cluster/outlier spatial relationships that they see."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#creating-a-lisa-cluster-map",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#creating-a-lisa-cluster-map",
    "title": "Take-home Exercise 1",
    "section": "13. Creating a LISA Cluster Map",
    "text": "13. Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations colour-coded by type of spatial autocorrelation. The first step is to plot the Moran scatterplot.\n\n13.1. Plotting Moran Scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attributes at each location and the average value of the same attribute at neighbouring locations.\nThe code chunk below plots the Moran scatterplot of the number of non-functional water points by using moran.plot() of spdep.\n\nnci <- moran.plot(nga_wp26392$`wpt non-functional`,\n                  rswm_knn8,\n                  labels = as.character(nga_wp26392$shapeName),\n                  xlab = \"No. of non-functional water points\",\n                  ylab = \"Spatially lagged non-functional water points\")\n\n\n\n\nThe Moran scatterplot can be interpreted by the 4 quadrants:\n\nTop-right (high-high): Positive autocorrelation, i.e. clusters (the region and its neighbours all have high values)\nBottom-left (low-low): Positive autocorrelation, i.e. clusters (the region and its neighbours all have low values)\nTop-left (low-high): Negative autocorrelation, i.e. outlier (low outlier among high neighbours)\nBottom-right (high-low): Negative autocorrelation, i.e. outlier (high outlier among low neighbours)\n\nThe direction and magnitude of global autocorrelation can be observed in the Moran scatterplot, as the slope of the linear regression of the lagged values of number of non-functional water points versus the original number of non-functional water points is equivalent to the Moran’s I score (Kam & Zhi, 2018).\nWe see that more regions follow a cluster autocorrelation pattern (high-high and low-low quadrants) rather than outlier pattern (low-high and high-low quadrants).\nWe also plot the Moran scatterplot for the number of functional water points, and notice a similar trend.\n\nnci_f <- moran.plot(nga_wp26392$`wpt functional`,\n                  rswm_knn8,\n                  labels = as.character(nga_wp26392$shapeName),\n                  xlab = \"No. of functional water points\",\n                  ylab = \"Spatially lagged functional water points\")\n\n\n\n\n\n\n13.2. Plotting Moran Scatterplot with Standardised Variables\nFirst, we use scale() from Base R to centre and scale the variables. Here, centering is done by subtracting the mean (omitting NAs) from the corresponding columns, and scaling is done by dividing the (centred) variables by their standard deviations. The as.vector() from the pbdDMAT package added at the end of the code chunk below is to ensure that the data type for nga_wp26392$Z.nf and nga_wp26392$Z.f is a non-distributed vector instead of a distributed matrix. This is so that we can then append it to our dataframe later.\n\nnga_wp26392$Z.nf <- scale(nga_wp26392$`wpt non-functional`) %>% as.vector\nnga_wp26392$Z.f <- scale(nga_wp26392$`wpt functional`) %>% as.vector\n\nPlotting the Moran scatterplot again using the code chunk below, this time as nci2 for non-functional water points and nci2_f for functional water points.\n\nnci2 <- moran.plot(nga_wp26392$Z.nf,\n                   rswm_knn8,\n                   labels = as.character(nga_wp26392$shapeName),\n                   xlab = \"z-No. of functional water points\",\n                   ylab = \"Spatially lagged z-No. of functional water points\")\n\n\n\n\n\nnci2_f <- moran.plot(nga_wp26392$Z.f,\n                   rswm_knn8,\n                   labels = as.character(nga_wp26392$shapeName),\n                   xlab = \"z-No. of functional water points\",\n                   ylab = \"Spatially lagged z-No. of functional water points\")\n\n\n\n\nWe see that the x- and y-axes are scaled to 0 (for the division of the 4 quadrants).\n\n\n13.3. Preparing LISA Map Classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant <- vector(mode = \"numeric\",\n                   length = nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. no. of non-functional water points) and centre the variable of interest around its mean.\n\nnga_wp26392$lag_wpt_nf <- lag.listw(rswm_knn8, nga_wp26392$`wpt non-functional`)\nDV <- nga_wp26392$`wpt non-functional` - mean(nga_wp26392$`wpt non-functional`)\n\nThis is followed by centering the local Moran’s I around its mean. This is for consistency with the DV method, and it is sufficient to simply use the local Moran’s I value without centering it around the mean (i.e. the code chunk below works the same as C_mI <- localMI[, 1]).\n\nC_mI <- localMI[, 1] - mean(localMI[, 1])\n\nThen, we set a statistical significance level for local Moran’s at 5%.\n\nsignif <- 0.05\n\nThe next 4 command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) quadrants.\n\nquadrant[DV < 0 & C_mI > 0] <- 1 # C_mi > 0 -> cluster // DV_nf refers to no. of non-functional water points wrt mean -> -ve means low-low\nquadrant[DV < 0 & C_mI < 0] <- 2 # C_mi < 0 -> outlier\nquadrant[DV > 0 & C_mI < 0] <- 3 # C_mi < 0 -> outlier\nquadrant[DV > 0 & C_mI > 0] <- 4 # C_mi > 0 -> cluster\n\nFinally, we place the non-significant Moran’s value in category 0.\n\nquadrant[localMI[,5] > signif] <- 0\n\nIn fact, we can combine all the steps into a single code chunk below.\n\nquadrant <- vector(mode = \"numeric\",\n                   length = nrow(localMI))\nnga_wp26392$lag_wpt_nf <- lag.listw(rswm_knn8, nga_wp26392$`wpt non-functional`)\nDV <- nga_wp26392$`wpt non-functional` - mean(nga_wp26392$`wpt non-functional`)\nC_mI <- localMI[, 1] - mean(localMI[, 1])\nsignif <- 0.05\nquadrant[DV < 0 & C_mI > 0] <- 1\nquadrant[DV < 0 & C_mI < 0] <- 2\nquadrant[DV > 0 & C_mI < 0] <- 3\nquadrant[DV > 0 & C_mI > 0] <- 4\nquadrant[localMI[,5] > signif] <- 0\n\n\n\n13.4. Plotting LISA Map for Non-Functional Water Points\nFinally, we can build the LISA map using the code chunk below.\n\nwp_nf <- qtm(nga_wp26392, \"wpt non-functional\", fill.palette = \"Blues\")\n\nnga_wp26392.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", # white for non-significant Moran's values\n            \"#f1fa73\", # yellow for low-low\n            \"#91fa5c\", # green for low-high\n            \"#5cfacb\", # cyan for high-low\n            \"#1239ff\") # blue for high-high\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap_nf <- tm_shape(nga_wp26392.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(wp_nf, LISAmap_nf, asp=1, ncol=2)\n\n\n\n\nThe plot on the right shows that the statistically significant Moran’s I values are in blue for high-high autocorrelation and yellow for low-low autocorrelation (clusters). These regions are all found on the (i) Central, Southeast and Southwest parts (high-high) and (ii) Northeast and South parts (low-low) of Nigeria respectively. It also shows several outlier regions in green and cyan, generally in the proximity of high-high regions as well as low-low regions in the South.\n\n\n13.5. Plotting LISA Map for Functional Water Points\nWe repeat the process to study the LISA map for functional water points. The code chunk below puts together the relevant preparation and plotting.\n\nlocalMI_f <- localmoran(nga_wp26392$`wpt functional`, \n                      rswm_knn8, \n                      zero.policy = TRUE)\n\nnga_wp26392.localMI_f <- cbind(nga_wp26392, localMI_f) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nquadrant <- vector(mode = \"numeric\",\n                   length = nrow(localMI_f))\nnga_wp26392$lag_wpt_f <- lag.listw(rswm_knn8, nga_wp26392$`wpt functional`)\nDV <- nga_wp26392$`wpt functional` - mean(nga_wp26392$`wpt functional`)\nC_mI <- localMI_f[, 1] - mean(localMI_f[, 1])\nsignif <- 0.05\nquadrant[DV < 0 & C_mI > 0] <- 1\nquadrant[DV < 0 & C_mI < 0] <- 2\nquadrant[DV > 0 & C_mI < 0] <- 3\nquadrant[DV > 0 & C_mI > 0] <- 4\nquadrant[localMI[,5] > signif] <- 0\n\nwp_f <- qtm(nga_wp26392, \"wpt functional\", fill.palette = \"Blues\")\n\nnga_wp26392.localMI_f$quadrant <- quadrant\ncolors <- c(\"#ffffff\", # white for non-significant Moran's values\n            \"#f1fa73\", # yellow for low-low\n            \"#91fa5c\", # green for low-high\n            \"#5cfacb\", # cyan for high-low\n            \"#1239ff\") # blue for high-high\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap_f <- tm_shape(nga_wp26392.localMI_f) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(wp_f, LISAmap_f, asp=1, ncol=2)\n\n\n\n\nThere are a lot less high-high and low-low regions (clusters) and more low-high and high-low regions (outliers) for number of functional water points than non-functional water points. Nevertheless, the broad areas for the clusters are outliers are consistent across both plots."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Take-home Exercise 1",
    "section": "14. Hot Spot and Cold Spot Area Analysis",
    "text": "14. Hot Spot and Cold Spot Area Analysis\nBesides detecting clusters and outliers, localised spatial statistics can also be used to detect hot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings.\n\n14.1. Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics. It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix (completed in section 9.7.)\nComputing Gi statistics\nMapping Gi statistics"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#computing-gi-statistics",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#computing-gi-statistics",
    "title": "Take-home Exercise 1",
    "section": "15. Computing Gi Statistics",
    "text": "15. Computing Gi Statistics\n\n15.1. Gi Statistics for Non-Functional Water Points Using Fixed Distance\nThe code chunk below calculates the Gi statistics of the 774 LGAs using the fixed distance weights matrix.\n\nfips <- order(nga_wp26392$shapeName)\ngi.fixed <- localG(nga_wp26392$`wpt non-functional`, wm71_lw)\nhead(gi.fixed)\n\n[1] -3.4117747 -3.4325327 -1.5896655  0.1024036 -1.4372021  3.1612220\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding nga_wp26392 sf dataframe by using the code chunk below.\n\nnga_wp26392.gi <- cbind(nga_wp26392, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nThe code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join nga_wp26392 and gi.fixed matrix to produce a new SpatialPolygonDataFrame called nga_wp26392.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n15.2. Gi Statistics for Non-Functional Water Points Using AdaptiveDistance\nThe code chunk below are used to compute the Gi values for number of non-functional water points by using an adaptive distance weight matrix (i.e rswm_knn8).\n\nfips <- order(nga_wp26392$shapeName)\ngi.adaptive <- localG(nga_wp26392$`wpt non-functional`, rswm_knn8)\nnga_wp26392.gi <- cbind(nga_wp26392.gi, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n15.3. Mapping Gi Values for Non-Functional Water Points with Fixed and Adaptive Distance Weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed and adaptive distance weights matrix, with respect to the number of non-functional water points.\n\nGimap.fixed <- tm_shape(nga_wp26392.gi) + \n  tm_fill(col = \"gstat_fixed\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Fixed distance weights matrix\",\n            main.title.size = 0.9,\n            main.title.position = \"center\",\n            legend.height = 0.25, \n            legend.width = 0.35)\n\nGimap.adaptive <- tm_shape(nga_wp26392.gi) + \n  tm_fill(col = \"gstat_adaptive\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Adaptive distance weights matrix\",\n            main.title.size = 0.9,\n            main.title.position = \"center\",\n            legend.height = 0.25, \n            legend.width = 0.35)\n\ntmap_arrange(Gimap.fixed,\n             Gimap.adaptive,\n             asp = 1,\n             ncol = 2)\n\n\n\n\nWe observe that the hot spots (surrounded by high numbers of non-functional water points with positive Local Gi, in blue) and cold spots (surrounded by low numbers of non-functional water points with negative Gi, in orange), are largely consistent between the fixed and adaptive distance methods. The hot spots tend to occur in the Central, Southwest and Southeast regions of Nigeria, while cold spots are found more in the Northeast, South, West and Northwest regions. The fixed distance method shows a wider range of Local Gi values, from ~-8 to ~8, versus ~-4 to ~6 for adaptive distance method. Notably, the fixed distance method showed interesting radiating patterns of hot spots and cold spots in the South and Southwest regions respectively, where the centre of those regions are hotter (Awka South) and colder (Ila) respective and get closer to 0 as the area spreads (see screenshot below). Such a pattern is not observed in the plot using the adaptive distance method. This may suggest that the LGAs with hot spots should be looked into for reasons of higher numbers of non-functioning water points, and potentially learn from the LGAs with cold spots.\n\n\n\n\n15.4. Gi Statistics for Functional Water Points Using Fixed Distance\nWe repeat the process for functional water points to determine whether there is any notable trends.\nThe code chunk below calculates the Gi statistics of the 774 LGAs using the fixed distance weights matrix.\n\nfips <- order(nga_wp26392$shapeName)\ngi.fixed <- localG(nga_wp26392$`wpt functional`, wm71_lw)\nhead(gi.fixed)\n\n[1] -4.7966255 -4.7601280 -1.1787856 -0.3266329 -4.7190630  1.6971466\n\n\nNext, we will join the Gi values to their corresponding nga_wp26392 sf dataframe by using the code chunk below.\n\nnga_wp26392.gi <- cbind(nga_wp26392, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nThe code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join nga_wp26392 and gi.fixed matrix to produce a new SpatialPolygonDataFrame called nga_wp26392.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n15.5. Gi Statistics for Functional Water Points Using AdaptiveDistance\nThe code chunk below are used to compute the Gi values for number of functional water points by using an adaptive distance weight matrix (i.e rswm_knn8).\n\nfips <- order(nga_wp26392$shapeName)\ngi.adaptive <- localG(nga_wp26392$`wpt functional`, rswm_knn8)\nnga_wp26392.gi <- cbind(nga_wp26392.gi, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n15.6. Mapping Gi Values for Functional Water Points with Fixed and Adaptive Distance Weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed and adaptive distance weights matrix, with respect to the number of functional water points.\n\nGimap.fixed <- tm_shape(nga_wp26392.gi) + \n  tm_fill(col = \"gstat_fixed\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Fixed distance weights matrix\",\n            main.title.size = 0.9,\n            main.title.position = \"center\",\n            legend.height = 0.25, \n            legend.width = 0.35)\n\nGimap.adaptive <- tm_shape(nga_wp26392.gi) + \n  tm_fill(col = \"gstat_adaptive\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Adaptive distance weights matrix\",\n            main.title.size = 0.9,\n            main.title.position = \"center\",\n            legend.height = 0.25, \n            legend.width = 0.35)\n\ntmap_arrange(Gimap.fixed,\n             Gimap.adaptive,\n             asp = 1,\n             ncol = 2)\n\n\n\n\nSimilar to the Gi plots for non-functional water points, the Local Gi values for functional water points have a wider range using fixed distance weight matrix compared to adaptive distance weight matrix. Hot spots are generally seen in the North to Central/Southwest and Southeast regions, while cold spots cover the Northeast, Northwest and South regions, quite similar to what we saw for the non-functional water points.\nGiven that hot spots for non-functional water points tend to coexist with hot spots for functional water points, we may need to use the proportion rather than the absolute number of water points as future works."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "title": "Take-home Exercise 1",
    "section": "16. References",
    "text": "16. References\nArcGIS desktop (2021). Modeling spatial relationships. https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/modeling-spatial-relationships.htm#GUID-DB9C20A7-51DB-4704-A0D7-1D4EA22C23A7\nepsg.io (2022). EPSG: 26392 Minna / Nigeria Mid Belt. https://epsg.io/26392\nKam, T. S., & Zhi, V. (2018) “Is There Space for Violence? A Data-driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict”. Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities, Seattle, WA, USA, 2018 November 6. 1-10. Research Collection School of Information Systems. https://doi.org/10.1145/1234567890\nLibrary of Congress (2013 August 23). Cameroon; Nigeria: Bakassi Peninsula Transition Completed. https://www.loc.gov/item/global-legal-monitor/2013-08-23/cameroon-nigeria-bakassi-peninsula-transition-completed/\nMapTools (2022). Symbols for degrees, minutes and second. https://www.maptools.com/tutorials/lat_lon/formats\nSpatial Reference (2022). EPSG: 26392. https://spatialreference.org/ref/epsg/26392/\nStatistics How To (2022). Moran’s I: Definition, Examples. https://www.statisticshowto.com/morans-i/\nTobler W. (1970) “A computer movie simulating urban growth in the Detroit region”. Economic Geography, 46(Supplement): 234–240.\nUnited Nations (2022). Goals 6 Ensure availability and sustainable management of water and sanitation for all. https://sdgs.un.org/goals/goal6\nVOA news (2022 March 21). UNICEF Nigeria Warns Millions at Risk of Water Contamination Ailments. https://www.voanews.com/a/unicef-nigeria-warns-millions-at-risk-of-water-contamination-ailments/6494928.html\nWater Point Data Exchange (2020). Water Point Data Exchange. https://www.waterpointdata.org/"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#learning-points-from-peers",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#learning-points-from-peers",
    "title": "Take-home Exercise 1",
    "section": "17. Learning Points from Peers",
    "text": "17. Learning Points from Peers"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#objective",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#objective",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "2. Objective",
    "text": "2. Objective\nIn this study, we aim to perform geospatial analytics to regionalise Nigeria by using data related to its water points. We will perform both spatially and non-spatially constrained clustering methods, including the following measures:\n\nTotal number of functional water points;\nTotal number of non-functional water points;\nPercentage of functional water points;\nPercentage of non-functional water points;\nPercentage of main water point technology (i.e. hand-pump);\nPercentage of usage capacity (i.e. < 1,000 and >= 1,000);\nPercentage of rural water points."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-pre-preparation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-pre-preparation",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "3. Data Pre-Preparation",
    "text": "3. Data Pre-Preparation\n\n3.1. Water Point Data\nWe obtained the global water point and small water scheme level data from Water Point Data Exchange (WPdx) Global Data Repositories (WPdx, 2020). We accessed the WPdx-Plus (WPdx+) option and downloaded the full shapefile under the Export option, as shown in the screenshot below. As the data consists of water points around the world, we will later filter for water points within Nigeria in R in a subsequent step.\n\n\n\n\n\nAfter downloading the shapefile which can take a few minutes due to the large file size, unzip the folder and copy the shapefile (.dbf, .prj, .shp and .shx) into a data subfolder that shares the same directory as this Quarto file for ease of calling the files. We also want to rename all four files to geo_export so that we can reference these filenames more easily when we import the data.\n\n\n3.2. Geographical Boundaries of Nigeria\nWe also need the geographical boundaries of Nigeria to make meaningful sense of its water point locations and to aid spatial visualisation. Here, we downloaded the Level-2 Administrative Boundaries (also known as Local Government Area (LGA)) data (ADM2) for Nigeria in year 2020 from geoBoundaries, the largest open and free database of political administrative boundaries globally (geoBoundaries, 2022). The screenshot below shows the page for shapefile format of the data for download. One can filter for Nigeria’s data by typing it in under the Name filter, followed by clicking on the download button under the column geoBoundaries, sub-column Full Release and for the row Nigeria, NGA, ADM2, 2020.\n\nSimilar to the water point data, we unzip the folder and copy the shapefile (.dbf, .prj, .shp and .shx) into the same folder as the water points shapefile. Here, we rename the files to geoBoundaries-NGA-ADM2 to indicate the data source (geoBoundaries), country (NGA) and administrative boundary level (ADM2)."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#installing-and-loading-packages-in-r",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#installing-and-loading-packages-in-r",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "4. Installing and Loading Packages in R",
    "text": "4. Installing and Loading Packages in R\nThe code chunk below uses p_load() from pacmanpackage to brings in the R packages for:\n\nSpatial vector data encoding (sf);\nData-wrangling (tidyverse), including:\n\nWriting rds file (readr);\nVariable creation (dplyr);\nCreating statistical plots (ggplot2);\n\nRapid Exploratory Data Analysis (EDA) (funModeling);\nCombining multiple statistical plots (ggpubr);\n(Hmisc);\nMap plotting (tmap);\nCreating spanning trees (spdep);\nBuilding correlation plot (corrplot);\n(ape);\nCluster analysis (cluster, NbClust, ClustGeo, psych);\nGrouping and visualising optimal cluster (factoextra); and\nVisualising cluster attributes (heatmaply).\n\n\npacman::p_load(sf, tidyverse, funModeling, ggpubr, Hmisc,\n               tmap, spdep, corrplot, ape, cluster, \n               NbClust, ClustGeo, psych, factoextra, heatmaply)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#importing-geospatial-data-in-r",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#importing-geospatial-data-in-r",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "5. Importing Geospatial Data in R",
    "text": "5. Importing Geospatial Data in R\n\n5.1. Water Point Geospatial Data\nThe code chunk below imports the water point geospatial data that we downloaded into the R environment, using st_read() function of the sf package. We specified the data source name (dsn) or directory of the file (\"data/geospatial\"), layer for the name of the Shapefiles (\"geo_export\"), and crs = 4326 to import the data in wgs84 geographic coordinate reference system (CRS), since the shapefile is in wgs84, as seen from the prj file when opening it with a text reader program (e.g. Notepad). We also pipe a filter to obtain data that are in Nigeria only, by using the filter() function of dplyr package from tidyverse. The clean_country_name column is used for the filter, and note that the column name is truncated in the shapefile due to character limit (clean_coun) and should be keyed in correctly to perform the filter successfully.\n\nwp <- st_read(dsn = \"data/geospatial\",\n              layer = \"geo_export\",\n              crs = 4326) %>% \n  filter(clean_coun == \"Nigeria\")\n\nAs we are not performing any planar distance computation in this study, we do not need to project the coordinates using st_transform() of sf.\nThe simple feature data frame comprises 95,008 observations of 73 variables. We will study the variables in greater details in the next section for meaningful variable creation for each geographical region, prior to clustering analysis.\nIn the code chunk below, write_rds() of the readr package is used to save the extracted sf data table into an output file in rds data format. We then do not need to go back to the original shapefile to reload the full set of global water points data each time we use it, as the data size is very large, the time to load is long and it cannot be pushed to GitHub.\n\nwp_nga <- write_rds(wp, \"data/geospatial/wp_nga.rds\")\n\nHowever, do note that after running the above code chunk, the wp_nga.rds file is still too large (140.2MB) to push to GitHub (100MB limit). Hence, we will further extract only the data that we wish to use for our analysis and save it as another rds file, and remove this one, indicate #| eval: false and delete the wp_nga.rds file from our directory, before we commit and push the changes to GitHub.\n\n\n5.2. Nigeria Level-2 Administrative Boundary Geospatial Data\nWe also import the Nigeria Level-2 Administrative Boundary (LGA) data into our R environment, similarly using st_read() of sf, in the code chunk below. The data are saved in the form of a simple feature data table nga.\n\nnga <- st_read(dsn = \"data/geospatial\",\n              layer = \"geoBoundaries-NGA-ADM2\",\n              crs = 4326)\n\n\nglimpse(nga)\n\nThere are 774 observations of 6 variables in the nga file, including shapeName for the LGA that each region belongs to and geometry for the polygons, as seen using the glimpse() function of dplyr above. The geometry type is multipolygon. It is also in the wgs84 geographic CRS, just like the water point data. Hence, there is no need to perform st_transform() to align their CRS.\nWe also run a check for invalid geometries in the LGA data, using st_is_valid() of sf.\n\nlength(which(st_is_valid(nga) == FALSE))\n\nThe output is 0 - there is no invalid geometry for the LGA polygons.\nWe also check for missing values in the LGA data, using is.na() of ursa to return TRUE/FALSE values and rowSums() of raster to tally the number of TRUE.\n\nnga[rowSums(is.na(nga))!=0,]"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-wrangling-data-cleaning-and-variable-creation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-wrangling-data-cleaning-and-variable-creation",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "6. Data Wrangling: Data Cleaning and Variable Creation",
    "text": "6. Data Wrangling: Data Cleaning and Variable Creation\n\n6.1. Recoding LGA Names\nWe note that while the number of LGAs is correct (774), some LGAs in different States have the same name. Hence, we checked through the list of LGAs under the shapeName column of our simple feature data table nga against the LGAs listed on Wikipedia (2022) for accuracy. We found that there are 12 LGAs with duplicate names, and 6 with typos/outdates names. We will study them further and update them in the subsections below.\n\n6.1.1. Duplicate LGA Names\nWe first extract the LGAs with duplicate names into a separate variable nga_duplicates using subset() of base R in the code chunk below.\n\nnga_duplicates <- subset(nga, shapeName %in% c(\"Bassa\", \"Ifelodun\", \"Irepodun\", \"Nasarawa\", \"Obi\", \"Surulere\"))\nnga_duplicates\n\nWe also add a column id to keep track of which row of nga_duplicates we will be plotting later and to visualise and match them to the correct state more easily, using mutate() of dplyr in the code chunk below.\n\nnga_duplicates <- nga_duplicates %>% \n  mutate(id = row_number())\nnga_duplicates\n\nWe switch the mode for tmap to interactive viewing so that we can zoom in on the LGAs plotted and take reference using the borders and states of Nigeria to determine which state each of them is in, by setting tmap_mode() of tmap to \"view\" in the code chunk below.\n\ntmap_mode(\"view\")\n\nNext, we plot the 12 LGAs with duplicate names using tmap functions in the code chunk below. We compare them with the map of the States of Nigeria from Wikipedia (2022) by overlaying the 2 images, for easy comparison.\n\nname <- tm_shape(nga_duplicates) +\n  tm_polygons() +\n  tm_text(\"shapeName\", size = 0.6)\n\nID <- tm_shape(nga_duplicates) +\n  tm_polygons() +\n  tm_text(\"id\", size = 0.6)\n\ntmap_arrange(name, ID, asp = 1, nrow = 2)\n\n\n\n\n\n\n\n \nBased on the map comparison, we copy the nga simple features data table into a new nga_recoded simple features data table, and create a new field called LGA and copy over the original shapeName using mutate() of dplyr. We then rename the duplicate LGA names to include the State name, using their row numbers and based on the result of the map match. This is so that subsequent analysis involving the LGAs will not be confused where the duplicate names are involved.\n\nnga_recoded <- nga %>% \n  mutate(LGA = shapeName)\n\nnga_recoded$LGA[c(94, 95, 304, 305, 355, 356, 519, 520, 546, 547, 693, 694)] <- c(\"Bassa, Kogi State\", \"Bassa, Plateau State\", \n                                                                          \"Ifelodun, Kwara State\", \"Ifelodun, Osun State\", \n                                                                          \"Irepodun, Kwara State\", \"Irepodun, Osun State\", \n                                                                          \"Nasarawa, Kano State\", \"Nasarawa, Nasarawa State\",  \n                                                                          \"Obi, Benue State\", \"Obi, Nasarawa State\", \n                                                                          \"Surulere, Lagos State\", \"Surulere, Oyo State\")\n\nWe visually check that LGA names have been recoded correctly using the code chunk below. This completes the inspection and recoding of LGAs with duplicate names.\n\nnga_duplicates <- subset(nga_recoded, shapeName %in% c(\"Bassa\", \"Ifelodun\", \"Irepodun\", \"Nasarawa\", \"Obi\", \"Surulere\"))\n\ntm_shape(nga_duplicates) +\n  tm_polygons() +\n  tm_text(\"LGA\", size = 0.6)\n\n\n\n6.1.2. Misspelled and Outdated LGA Names\nWe also note that there are some differences in the spellings of the shapeName versus the LGA names on Wikipedia. For those which are typos or are old names, we replace them using the mutate() and recode() functions of dplyr in the code chunk below.\n\nnga_recoded <- nga_recoded %>%\n  mutate(LGA = recode(LGA, \"Birni Kudu\" = \"Birnin Kudu\", \"Isiukwuato\" = \"Isuikwuato\", \"Markafi\" = \"Makarfi\", \"Muya\" = \"Moya\", \"Egbado North\" = \"Yewa North\", \"Egbado South\" = \"Yewa South\"))\n\n\n\n6.1.3. Dropping Irrelevant Fields\nWe also note that some fields in nga_recoded are not meaningful and can be dropped before joining with the water point data, as follows:\n\nshapeName: The LGA names have been recoded in the LGA field.\nLevel: All values are ADM2.\nshapeID: The IDs are long and not easily readable.\nshapeGroup: All values are NGA.\nshapeType: All values are ADM2.\n\nHence, we wish to drop the 5 columns, and are left with only LGA and geometry for the geometrical information of each LGA, using the code chunk below. Note that only LGA needs to be selected, as the geometry column is retained by default for simple features data table. We name this nga_r.\n\nnga_r <- nga_recoded[\"LGA\"]\nnga_r\n\n\n\n\n6.2. Recoding Missing Water Point Data\nIn the code chunk below, we use is.na() of ursa to replace the NA data in all variable with Unknown. This is so that the observations with “NA” will not be excluded in subsequent analyses.\n\nwp_nga <- read_rds(\"data/geospatial/wp_nga.rds\") \nwp_nga[is.na(wp_nga)] <- \"Unknown\"\n\n\n\n6.3. Exploratory Data Analysis (EDA) for Water Point Data\nWe then plot the distribution of all fields in wp_nga to determine the variables that are worth looking into for cluster analysis, using freq() of funModeling in the code chunk below. We note that some of the variables show sufficiently meaningful spread across multiple categories (see screenshots below), namely:\n\nX_water_tec: Water Point Technology - Describes the system being used to transport the water from the source to the point of collection (e.g. Handpump, Kiosk, Tapstand);\nmanageme_2: Management Structure - Selects the classification of the entity that directly manages the water point (e.g. Community Management, Direct Government Operation);\nstatus_cle: Condition - Provides a status of the physical/mechanical condition of the water point (e.g. Functional, Non-functional);\nsubjective: Subjective Quality - Information regarding the perceived quality of the water including taste, appearance, and/or odour;\nusage_cap: Usage Capacity - Recommended maximum users per water point, extended from Sphere Guidelines; and\nis_urban: Is Urban - Is in an urban area as defined by EU Global Human Settlement Database.\n\n\nfreq(data = wp_nga, input = names(wp_nga))\n\n\n\n\nWater technology distribution\n\n\n\n\n\nManagement structure distribution\n\n\n\n\n\nCondition distribution\n\n\n\n\n\nWater quality distribution\n\n\n\n\n\nUsage capacity distribution\n\n\n\n\n\nIs urban distribution\n\n\n\n\n6.3. Variable Creation for Water Point Data\n\nwp_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nwp_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\nwp_handpump <- wp_nga %>%\n  filter(X_water_tec == \"Hand Pump\")\n\nwp_usage_less_than_1000 <- wp_nga %>%\n  filter(usage_cap < 1000)\n\nwp_usage_1000 <- wp_nga %>%\n  filter(usage_cap >= 1000)\n\nwp_rural <- wp_nga %>%\n  filter(is_urban == \"False\")\n\nwp_government <- wp_nga %>%\n  filter(manageme_2 == \"Direct Government Operation\")\n\nwp_quality_pass <- wp_nga %>%\n  filter(subjective %in%\n           c(\"Acceptable quality\",\n             \"Within National standards (potable)\",\n             \"Within National limits (potable)\"))\n\n\n\n6.4. Performing Point-in-Polygon Count\n\nnga_wp <- nga_r %>% \n  mutate(total_wp = lengths(\n    st_intersects(nga_r, wp_nga))) %>%\n  mutate(wp_functional = lengths(\n    st_intersects(nga_r, wp_functional))) %>%\n  mutate(wp_nonfunctional = lengths(\n    st_intersects(nga_r, wp_nonfunctional))) %>%\n  mutate(wp_handpump = lengths(\n    st_intersects(nga_r, wp_handpump))) %>%\n  mutate(wp_usage_less_than_1000 = lengths(\n    st_intersects(nga_r, wp_usage_less_than_1000))) %>%\n  mutate(wp_usage_1000 = lengths(\n    st_intersects(nga_r, wp_usage_1000))) %>%\n  mutate(wp_rural = lengths(\n    st_intersects(nga_r, wp_rural))) %>%\n  mutate(wp_government = lengths(\n    st_intersects(nga_r, wp_government))) %>%\n  mutate(wp_quality_pass = lengths(\n    st_intersects(nga_r, wp_quality_pass)))\n\n\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp) %>%\n  mutate(pct_handpump = wp_handpump/total_wp) %>%\n  mutate(pct_usage_less_than_1000 = wp_usage_less_than_1000/total_wp) %>%\n  mutate(pct_usage_1000 = wp_usage_1000/total_wp) %>%\n  mutate(pct_rural = wp_rural/total_wp) %>%\n  mutate(pct_government = wp_government/total_wp) %>%\n  mutate(pct_quality_pass = wp_quality_pass/total_wp)\n\n\nnga_wp[is.na(nga_wp)] <- 0\n\n\nwrite_rds(nga_wp, \"data/geospatial/nga_wp.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualising-spatial-distribution-of-water-point-variables---thematic-mapping",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualising-spatial-distribution-of-water-point-variables---thematic-mapping",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "7. Visualising Spatial Distribution of Water Point Variables - Thematic Mapping",
    "text": "7. Visualising Spatial Distribution of Water Point Variables - Thematic Mapping\n\nnga_wp <- read_rds(\"data/geospatial/nga_wp.rds\")\n\n\ntmap_mode(\"plot\")\n\n\ntotal <- qtm(nga_wp, \"total_wp\")\nwp_functional <- qtm(nga_wp, \"wp_functional\")\nwp_nonfunctional <- qtm(nga_wp, \"wp_nonfunctional\")\npct_functional <- qtm(nga_wp, \"pct_functional\")\npct_nonfunctional <- qtm(nga_wp, \"pct_nonfunctional\")\npct_handpump <- qtm(nga_wp, \"pct_handpump\")\npct_usage_less_than_1000 <- qtm(nga_wp, \"pct_usage_less_than_1000\")\npct_usage_1000 <- qtm(nga_wp, \"pct_usage_1000\")\npct_rural <- qtm(nga_wp, \"pct_rural\")\npct_government <- qtm(nga_wp, \"pct_government\")\npct_quality_pass <- qtm(nga_wp, \"pct_quality_pass\")\n\ntmap_arrange(total, wp_functional, wp_nonfunctional, \n             pct_functional, pct_nonfunctional, \n             pct_handpump, pct_usage_less_than_1000, pct_usage_1000,\n             pct_rural, pct_government, pct_quality_pass,\n             asp = 1, ncol = 2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#correlation-analysis",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#correlation-analysis",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "8. Correlation Analysis",
    "text": "8. Correlation Analysis\n\nnga_wp_var <- nga_wp[,!(names(nga_wp) == \"LGA\")]\n\nnga_wp_var <- nga_wp_var %>%\n  st_set_geometry(NULL)\n  \nnga_wp_var.cor = cor(nga_wp_var)\ncorrplot.mixed(nga_wp_var.cor,\n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\nWe tabulate variable pairs with correlation (r) > 0.85, where at least 1 of each pair should be removed to avoid multicollinearity which affects clustering results due to over-representation of similar variables.\n\nVariables with correlation > 0.85\n\n\nVariable 1\nVariable 2\nCorrelation (r)\n\n\n\n\ntotal_wp\nwp_functional\n0.90\n\n\ntotal_wp\nwp_handpump\n0.92\n\n\ntotal_wp\nwp_usage_less_than_1000\n0.96\n\n\ntotal_wp\nwp_rural\n0.93\n\n\ntotal_wp\nwp_quality_pass\n0.93\n\n\nwp_functional\nwp_handpump\n0.94\n\n\nwp_functional\nwp_usage_less_than_1000\n0.89\n\n\nwp_functional\nwp_rural\n0.85\n\n\nwp_functional\nwp_quality_pass\n0.95\n\n\nwp_handpump\nwp_usage_less_than_1000\n0.96\n\n\nwp_handpump\nwp_rural\n0.90\n\n\nwp_handpump\nwp_quality_pass\n0.93\n\n\nwp_usage_less_than_1000\nwp_rural\n0.94\n\n\nwp_usage_less_than_1000\nwp_quality_pass\n0.89\n\n\nwp_rural\nwp_quality_pass\n0.87\n\n\npct_usage_less_than_1000\npct_usage_1000\n-0.91\n\n\n\nAs total_wp, wp_functional, wp_handpump, wp_quality_pass, wp_rural and wp_usage_less_than_1000 are all correlated to each other, we will drop all of them except wp_functional. Between pct_usage_1000 and pct_usage_less_than_1000, we will also remove pct_usage_less_than_1000, since the two variables add up to 100%. The final list of 11 clustering variables are as follows:\n\nwp_functional\nwp_nonfunctional\nwp_usage_1000\nwp_government\npct_functional\npct_nonfunctional\npct_handpump\npct_usage_1000\npct_rural\npct_government\npct_quality_pass\n\nThe clustering variables are extracted into data.frame using select() of dplyr in the following code chunk.\n\ncluster_vars <- nga_wp %>%\n  st_set_geometry(NULL) %>%\n  dplyr::select(\"LGA\", \"wp_functional\", \"wp_nonfunctional\", \n                \"wp_usage_1000\", \"wp_government\", \"pct_functional\", \n                \"pct_nonfunctional\", \"pct_handpump\", \"pct_usage_1000\",\n                \"pct_rural\", \"pct_government\", \"pct_quality_pass\")\nhead(cluster_vars, 5)\n\n        LGA wp_functional wp_nonfunctional wp_usage_1000 wp_government\n1 Aba North             7                9            14             6\n2 Aba South            29               35            62            10\n3    Abadam             0                0             0             0\n4     Abaji            23               34            34            20\n5      Abak            23               25            44             6\n  pct_functional pct_nonfunctional pct_handpump pct_usage_1000  pct_rural\n1      0.4117647         0.5294118   0.11764706      0.8235294 0.00000000\n2      0.4084507         0.4929577   0.09859155      0.8732394 0.05633803\n3      0.0000000         0.0000000   0.00000000      0.0000000 0.00000000\n4      0.4035088         0.5964912   0.40350877      0.5964912 0.84210526\n5      0.4791667         0.5208333   0.08333333      0.9166667 0.83333333\n  pct_government pct_quality_pass\n1      0.3529412        0.7647059\n2      0.1408451        0.8028169\n3      0.0000000        0.0000000\n4      0.3508772        0.9824561\n5      0.1250000        0.7291667\n\n\n\nrow.names(cluster_vars) <- cluster_vars$\"LGA\"\nhead(cluster_vars, 5)\n\n                LGA wp_functional wp_nonfunctional wp_usage_1000 wp_government\nAba North Aba North             7                9            14             6\nAba South Aba South            29               35            62            10\nAbadam       Abadam             0                0             0             0\nAbaji         Abaji            23               34            34            20\nAbak           Abak            23               25            44             6\n          pct_functional pct_nonfunctional pct_handpump pct_usage_1000\nAba North      0.4117647         0.5294118   0.11764706      0.8235294\nAba South      0.4084507         0.4929577   0.09859155      0.8732394\nAbadam         0.0000000         0.0000000   0.00000000      0.0000000\nAbaji          0.4035088         0.5964912   0.40350877      0.5964912\nAbak           0.4791667         0.5208333   0.08333333      0.9166667\n           pct_rural pct_government pct_quality_pass\nAba North 0.00000000      0.3529412        0.7647059\nAba South 0.05633803      0.1408451        0.8028169\nAbadam    0.00000000      0.0000000        0.0000000\nAbaji     0.84210526      0.3508772        0.9824561\nAbak      0.83333333      0.1250000        0.7291667\n\n\n\ncluster_vars <- dplyr::select(cluster_vars, c(2:12))\nhead(cluster_vars, 5)\n\n          wp_functional wp_nonfunctional wp_usage_1000 wp_government\nAba North             7                9            14             6\nAba South            29               35            62            10\nAbadam                0                0             0             0\nAbaji                23               34            34            20\nAbak                 23               25            44             6\n          pct_functional pct_nonfunctional pct_handpump pct_usage_1000\nAba North      0.4117647         0.5294118   0.11764706      0.8235294\nAba South      0.4084507         0.4929577   0.09859155      0.8732394\nAbadam         0.0000000         0.0000000   0.00000000      0.0000000\nAbaji          0.4035088         0.5964912   0.40350877      0.5964912\nAbak           0.4791667         0.5208333   0.08333333      0.9166667\n           pct_rural pct_government pct_quality_pass\nAba North 0.00000000      0.3529412        0.7647059\nAba South 0.05633803      0.1408451        0.8028169\nAbadam    0.00000000      0.0000000        0.0000000\nAbaji     0.84210526      0.3508772        0.9824561\nAbak      0.83333333      0.1250000        0.7291667"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-standardisation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-standardisation",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "9. Data Standardisation",
    "text": "9. Data Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis. Some common variable standardisation techniques are:\n\nZ-score - transforms normal variants to standard score form;\nMin-Max - transforms data to a value between 0 and 1; and\nDecimal Scaling - normalises by moving the decimal points of the maximum value of the variable to <1.\n\nIn this case, since the proportion variables are already computed on the basis of between 0 and 1, we will use Min-Max standardisation across all variables.\n\n9.1. Min-Max Standardisation\nIn the code chunk below, normalize() of heatmaply is used to stadardise the clustering variables by using Min-Max method. The summary() of base R is then used to display the summary statistics of the standardised clustering variables.\n\nnga_wp.std <- normalize(cluster_vars)\nsummary(nga_wp.std)\n\n wp_functional     wp_nonfunctional  wp_usage_1000    wp_government     \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.000000  \n 1st Qu.:0.02261   1st Qu.:0.04406   1st Qu.:0.0449   1st Qu.:0.003367  \n Median :0.06051   Median :0.12230   Median :0.1041   Median :0.011784  \n Mean   :0.08957   Mean   :0.14962   Mean   :0.1352   Mean   :0.036693  \n 3rd Qu.:0.11669   3rd Qu.:0.21853   3rd Qu.:0.1878   3rd Qu.:0.035354  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.0000   Max.   :1.000000  \n pct_functional   pct_nonfunctional  pct_handpump    pct_usage_1000  \n Min.   :0.0000   Min.   :0.0000    Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.3261   1st Qu.:0.2105    1st Qu.:0.1670   1st Qu.:0.1220  \n Median :0.4741   Median :0.3505    Median :0.5099   Median :0.3127  \n Mean   :0.4984   Mean   :0.3592    Mean   :0.4873   Mean   :0.3754  \n 3rd Qu.:0.6699   3rd Qu.:0.5076    3rd Qu.:0.7778   3rd Qu.:0.5771  \n Max.   :1.0000   Max.   :1.0000    Max.   :1.0000   Max.   :1.0000  \n   pct_rural      pct_government    pct_quality_pass\n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.5727   1st Qu.:0.02481   1st Qu.:0.5425  \n Median :0.8645   Median :0.07764   Median :0.7706  \n Mean   :0.7271   Mean   :0.15546   Mean   :0.7052  \n 3rd Qu.:1.0000   3rd Qu.:0.18182   3rd Qu.:0.9209  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n\n\nWe see that the value range of the Min-Max standarised clustering variables is now within 0 and 1 (inclusive).\n\n\n9.2. Visualising the Standardised Clustering Variables\nWe also visualise the distribution of the standardised variables graphical as good practice using hist.data.frame() of Hmisc, in the code chunk below.\n\npar(mar=c(1,1,1,1))\nhist.data.frame(nga_wp.std)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-transformation",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#data-transformation",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "10. Data Transformation",
    "text": "10. Data Transformation\n\n10.1. Log Transformation\n\nnga_wp.tsf <- log(nga_wp.std + 1)\nsummary(nga_wp.tsf)\n\n wp_functional     wp_nonfunctional  wp_usage_1000     wp_government     \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000000  \n 1st Qu.:0.02235   1st Qu.:0.04312   1st Qu.:0.04392   1st Qu.:0.003361  \n Median :0.05875   Median :0.11538   Median :0.09901   Median :0.011716  \n Mean   :0.08165   Mean   :0.13323   Mean   :0.12109   Mean   :0.033310  \n 3rd Qu.:0.11037   3rd Qu.:0.19764   3rd Qu.:0.17207   3rd Qu.:0.034743  \n Max.   :0.69315   Max.   :0.69315   Max.   :0.69315   Max.   :0.693147  \n pct_functional   pct_nonfunctional  pct_handpump    pct_usage_1000  \n Min.   :0.0000   Min.   :0.0000    Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2822   1st Qu.:0.1911    1st Qu.:0.1545   1st Qu.:0.1151  \n Median :0.3880   Median :0.3005    Median :0.4120   Median :0.2721  \n Mean   :0.3912   Mean   :0.2949    Mean   :0.3715   Mean   :0.2972  \n 3rd Qu.:0.5128   3rd Qu.:0.4105    3rd Qu.:0.5754   3rd Qu.:0.4556  \n Max.   :0.6931   Max.   :0.6931    Max.   :0.6931   Max.   :0.6931  \n   pct_rural      pct_government    pct_quality_pass\n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.4528   1st Qu.:0.02451   1st Qu.:0.4334  \n Median :0.6230   Median :0.07478   Median :0.5713  \n Mean   :0.5248   Mean   :0.13160   Mean   :0.5205  \n 3rd Qu.:0.6931   3rd Qu.:0.16705   3rd Qu.:0.6528  \n Max.   :0.6931   Max.   :0.69315   Max.   :0.6931  \n\n\n\npar(mar=c(1,1,1,1))\nhist.data.frame(nga_wp.tsf)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#computing-proximity-matrix",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#computing-proximity-matrix",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "11. Computing Proximity Matrix",
    "text": "11. Computing Proximity Matrix\n\nproxmat.std <- dist(nga_wp.std, method = 'euclidean')\n\n\nproxmat.tsf <- dist(nga_wp.tsf, method = 'euclidean')\n\n\nproxmat.std.man <- dist(nga_wp.std, method = 'manhattan')\n\n\nproxmat.tsf.man <- dist(nga_wp.tsf, method = 'manhattan')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#hierarchical-clustering",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#hierarchical-clustering",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "12. Hierarchical Clustering",
    "text": "12. Hierarchical Clustering\n\n12.1. Computing Conventional Hierarchical Clustering\n\nhclust_ward.std <- hclust(proxmat.std, method = 'ward.D')\nhclust_ward.tsf <- hclust(proxmat.tsf, method = 'ward.D')\nhclust_ward.std.man <- hclust(proxmat.std.man, method = 'ward.D')\nhclust_ward.tsf.man <- hclust(proxmat.tsf.man, method = 'ward.D')\n\n\npar(mfrow = c(2, 2))\nplot(as.phylo(hclust_ward.std), cex = 0.1)\nplot(as.phylo(hclust_ward.tsf), cex = 0.1)\nplot(as.phylo(hclust_ward.std.man), cex = 0.1)\nplot(as.phylo(hclust_ward.tsf.man), cex = 0.1)\n\n\n\n\n\n\n12.2. Selecting Optimal Clustering Algorithm\n\nm <- c(\"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c(\"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(nga_wp.tsf, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8587496 0.7561142 0.9163428 0.9837044 \n\n\n\n\n12.3. Determining Optimal Number of Clusters\n\nset.seed(1234)\ngap_stat <- clusGap(nga_wp.tsf, \n                    FUN = hcut, \n                    nstart = 25,\n                    K.max = 20, \n                    B = 50)\n#print(gap_stat, method = \"firstmax\")\n\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nPost-standardisation and transformation\n\n\n\nset.seed(1234)\ngap_stat <- clusGap(nga_wp.std, \n                    FUN = hcut, \n                    nstart = 25,\n                    K.max = 20, \n                    B = 50)\n#print(gap_stat, method = \"firstmax\")\n\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nPost-standardisation only\n\n\n\nset.seed(1234)\ngap_stat <- clusGap(cluster_vars, \n                    FUN = hcut, \n                    nstart = 25,\n                    K.max = 20, \n                    B = 50)\n#print(gap_stat, method = \"firstmax\")\n\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nPre-standardisation and transformation\n\n\n\n\n12.4. Interpreting Cluster Dendrogram\n\nplot(hclust_ward.tsf.man, cex = 0.6)\nrect.hclust(hclust_ward.tsf.man, \n            k = 7, \n            border = 2:5)\n\n\n\n\n\n\n12.5. Visualising Cluster Heatmap\n\nnga_wp_mat <- data.matrix(nga_wp.tsf)\n\n\nheatmaply(normalize(nga_wp_mat),\n          Colv=NA,\n          dist_method = \"manhattan\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 7,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Nigeria LGAs by Water Point Attributes\",\n          xlab = \"Water point attributes\",\n          ylab = \"Nigeria LGAs\"\n          )\n\n\n\n\n\n\n\n12.6. Mapping 7 Clusters\n\ngroups <- as.factor(cutree(hclust_ward.tsf.man, k=7))\nnga_wp_cluster <- cbind(nga_wp, as.matrix(groups)) %>%\n  rename(`CLUSTER`=`as.matrix.groups.`)\nqtm(nga_wp_cluster, \"CLUSTER\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#spatially-constrained-clustering---skater-approach",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#spatially-constrained-clustering---skater-approach",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "13. Spatially Constrained Clustering - SKATER Approach",
    "text": "13. Spatially Constrained Clustering - SKATER Approach\nIn this section, we will derive spatially constrained cluster by using skater() method of spdep package.\n\n13.1. Converting Simple Features Data.frame into SpatialPolygonsDataFrame\nFirst, we need to convert nga_wp into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert nga_wp into a SpatialPolygonDataFrame called nga_wp_sp.\n\nnga_wp_sp <- as_Spatial(nga_wp)\n\n\n\n13.2. Computing Neighbour List\n\n13.2.1. Queen’s Contiguity-based Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list. In the code chunk below, the default Queen’s contiguity method is used.\n\nnga_wp.nb <- poly2nb(nga_wp_sp)\nsummary(nga_wp.nb)\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\nLink number distribution:\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n  1   2  14  57 125 182 140 122  72  41  12   4   1   1 \n2 least connected regions:\n138 560 with 1 link\n1 most connected region:\n508 with 14 links\n\n\nThere are 4,440 pairs of neighbours defined based on the definition of at least 1 shared boundary point, among the 774 LGAs in Nigeria.\nWhat is important to note is that there is 1 LGA (polygon ID 86, corresponding to Bakassi) with no link as it does not share any boundary with any other LGA in Nigeria. This creates an issue for the subsequent step in computing the minimum spanning tree.\n\n\n\nBakassi\n\n\n\n\n13.2.2. Delaunay Triangulation-based Neighbour List\nInstead, we will use Delaunay Triangulation to create a nonoverlapping mesh of triangles from feature centroids (esri, 2022; sfdep, 2022). Each feature is a triangle node, and nodes that share edges are considered neighbours. This will address the issue where the LGA borders themselves are not contiguous.\nThe creation of neighbours list using Delaunay Triangulation method is done using tri2nb() of spdep in the code chunk below, where we first change nga_wp into a two-column point coordinates object coords. Using summary(), we can then see the overview of the neighbours list nga_wp.nb.\n\ncoords <- st_coordinates(st_centroid(st_geometry(nga_wp)))\nnga_wp.nb <- tri2nb(coords)\nsummary(nga_wp.nb)\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4602 \nPercentage nonzero weights: 0.768183 \nAverage number of links: 5.945736 \nLink number distribution:\n\n  3   4   5   6   7   8   9  10  11 \n  4  71 224 251 139  64  17   2   2 \n4 least connected regions:\n461 500 663 685 with 3 links\n2 most connected regions:\n207 315 with 11 links\n\n\nBased on Delaunay Triangulation, 4,602 pairs of neighbours are defined, 162 more than Queen’s contiguity. We also see that all LGAs are connected, with the minimum number of neighbours increasing from 1 to 3 and the maximum number of neighbours decreasing from 14 to 11.\nWe can plot the neighbours list on nga_wp_sp (first part of the code) with the neighbours list object overlaid (second part of the code) by using the code chunk below. The plot of the neighbour list object has coordinates applied to the original SpatialPolygonDataFrame (Nigeria LGA boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(nga_wp_sp, \n     border = grey(.5))\nplot(nga_wp.nb, \n     coordinates(nga_wp_sp), \n     col = \"blue\", \n     add = TRUE)\n\n\n\n\n\n\n\n13.3. Computing Edge Costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts <- nbcosts(nga_wp.nb, nga_wp.tsf)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style=\"B\" to make sure the cost values are binary and not row-standardised.\n\nnga_wp.w <- nb2listw(nga_wp.nb, \n                     lcosts, \n                     style = \"B\")\nsummary(nga_wp.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4602 \nPercentage nonzero weights: 0.768183 \nAverage number of links: 5.945736 \nLink number distribution:\n\n  3   4   5   6   7   8   9  10  11 \n  4  71 224 251 139  64  17   2   2 \n4 least connected regions:\n461 500 663 685 with 3 links\n2 most connected regions:\n207 315 with 11 links\n\nWeights style: B \nWeights constants summary:\n    n     nn     S0       S1      S2\nB 774 599076 2107.7 2492.609 27394.4\n\n\n\n\n13.4. Computing Minimum Spanning Tree (MST)\nThe minimum spanning tree (MST) is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nnga_wp.mst <- mstree(nga_wp.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nclass(nga_wp.mst)\n\n[1] \"mst\"    \"matrix\"\n\n\n\ndim(nga_wp.mst)\n\n[1] 773   3\n\n\nR reveals that nga_wp.mst is a 773 by 3 matrix. The dimension is 1 less than the total number of LGAs of 774 in Nigeria, because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of nga_wp.mstby using head() as shown in the code chunk below.\n\nhead(nga_wp.mst)\n\n     [,1] [,2]      [,3]\n[1,]  679  385 0.1317961\n[2,]  679  243 0.1587286\n[3,]  385  373 0.1594585\n[4,]  373  418 0.1892220\n[5,]  679  744 0.2051613\n[6,]  744  127 0.2256656\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(nga_wp_sp, \n     border=gray(.5))\nplot.mst(nga_wp.mst, \n         coordinates(nga_wp_sp), \n         col = \"blue\", \n         cex.lab = 0.6, \n         cex.circles = 0.005, \n         add = TRUE)\n\n\n\n\n\n\n13.5. Computing Spatially Constrained Clusters Using SKATER Method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust7 <- skater(edges = nga_wp.mst[,1:2], \n                 data = nga_wp_mat, \n                 method = \"manhattan\", \n                 ncuts = 6)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using str() of utils to display the internal structure of clust7, using the code chunk below.\n\nstr(clust7)\n\nList of 8\n $ groups      : num [1:774] 6 6 3 2 6 2 7 7 2 6 ...\n $ edges.groups:List of 7\n  ..$ :List of 3\n  .. ..$ node: num [1:225] 440 479 388 477 705 225 383 132 767 472 ...\n  .. ..$ edge: num [1:224, 1:3] 472 480 767 485 700 132 489 422 714 382 ...\n  .. ..$ ssw : num 241\n  ..$ :List of 3\n  .. ..$ node: num [1:289] 369 287 751 163 76 427 547 694 29 67 ...\n  .. ..$ edge: num [1:288, 1:3] 287 751 163 76 427 547 521 438 46 67 ...\n  .. ..$ ssw : num 271\n  ..$ :List of 3\n  .. ..$ node: num [1:21] 271 91 160 406 447 146 261 473 252 3 ...\n  .. ..$ edge: num [1:20, 1:3] 473 146 512 160 271 271 406 91 271 447 ...\n  .. ..$ ssw : num 16\n  ..$ :List of 3\n  .. ..$ node: num [1:40] 728 579 618 23 537 189 291 294 215 292 ...\n  .. ..$ edge: num [1:39, 1:3] 292 579 291 193 309 294 579 23 537 309 ...\n  .. ..$ ssw : num 35.2\n  ..$ :List of 3\n  .. ..$ node: num [1:68] 570 37 629 200 197 199 312 323 727 523 ...\n  .. ..$ edge: num [1:67, 1:3] 371 370 523 720 727 669 206 335 371 562 ...\n  .. ..$ ssw : num 59.3\n  ..$ :List of 3\n  .. ..$ node: num [1:84] 66 103 338 551 203 334 331 730 5 288 ...\n  .. ..$ edge: num [1:83, 1:3] 549 364 338 726 190 551 338 204 203 338 ...\n  .. ..$ ssw : num 80.5\n  ..$ :List of 3\n  .. ..$ node: num [1:47] 598 316 651 327 671 297 22 623 693 544 ...\n  .. ..$ edge: num [1:46, 1:3] 671 297 327 22 623 316 317 352 693 558 ...\n  .. ..$ ssw : num 52\n $ not.prune   : NULL\n $ candidates  : int [1:7] 1 2 3 4 5 6 7\n $ ssto        : num 1017\n $ ssw         : num [1:7] 1017 929 868 843 817 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:774] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the groups argument of clust7 in the code chunk below.\n\nccs7 <- clust7$groups\nccs7\n\n  [1] 6 6 3 2 6 2 7 7 2 6 5 2 2 2 7 2 2 2 2 2 2 7 4 2 6 5 5 2 2 2 2 7 1 2 2 1 5\n [38] 2 2 2 2 2 5 2 2 2 1 2 7 1 7 5 5 4 6 2 5 5 1 2 2 7 1 1 2 6 2 5 1 2 2 2 2 2\n [75] 1 2 2 4 4 5 1 7 1 2 1 6 1 1 1 1 3 2 2 2 2 1 1 1 1 1 1 2 6 2 1 2 1 1 1 1 1\n[112] 2 1 1 1 1 1 2 2 2 5 6 2 2 2 5 1 2 1 1 2 1 2 5 2 6 6 2 1 1 2 1 1 1 1 3 1 2\n[149] 1 1 1 1 1 1 1 1 5 2 1 3 1 2 2 1 4 1 1 1 5 2 2 2 2 2 2 7 7 2 2 5 6 2 5 6 2\n[186] 2 2 2 4 6 6 2 4 4 4 7 5 5 5 5 5 6 6 6 5 5 7 6 6 5 5 5 2 7 4 6 2 2 1 2 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 2 3 1 1 1 2 6 1 1 1 3 1 3 1 2 1 2 2 1 1\n[260] 1 3 1 2 1 2 1 1 2 2 1 3 1 1 1 7 7 7 7 7 5 2 2 2 7 6 6 2 6 2 2 4 4 4 4 2 2\n[297] 7 2 2 2 2 2 2 2 2 7 2 2 4 4 4 5 4 6 7 7 7 7 2 2 6 5 5 1 6 7 7 2 2 2 6 7 6\n[334] 6 5 6 2 6 2 5 2 2 2 2 1 2 2 2 7 1 6 7 5 2 2 2 2 2 1 2 2 2 2 6 6 6 2 6 2 5\n[371] 5 6 1 2 6 2 2 2 2 2 1 1 1 1 1 2 2 1 1 2 2 2 2 1 2 2 2 1 1 3 2 2 1 2 1 3 2\n[408] 1 2 1 2 1 1 1 1 2 2 1 2 1 2 1 2 1 2 1 2 2 2 6 1 1 1 1 1 2 2 2 5 1 2 2 7 1\n[445] 1 2 3 1 1 1 1 2 1 2 1 2 1 1 2 2 7 7 1 2 2 2 1 2 1 2 2 1 3 1 3 2 1 1 1 1 1\n[482] 2 1 2 1 1 2 1 1 2 1 3 2 2 1 1 1 6 6 1 1 2 1 1 6 2 3 2 3 2 2 3 3 1 7 2 1 1\n[519] 1 2 2 5 5 5 3 3 2 6 1 1 4 4 2 2 4 4 4 6 6 6 4 1 4 7 2 2 2 6 6 2 6 6 2 2 7\n[556] 2 2 7 6 2 2 5 2 4 5 2 2 2 2 5 6 7 5 6 6 2 2 2 4 7 2 2 2 2 6 5 6 5 2 6 2 2\n[593] 2 2 7 2 6 7 2 2 2 4 4 6 6 5 2 5 2 2 4 2 6 4 6 6 6 4 4 2 5 4 7 6 2 2 5 5 5\n[630] 5 6 6 6 2 2 4 6 2 2 2 2 2 5 2 6 1 2 1 2 1 7 2 1 1 1 2 1 1 2 1 2 1 5 2 2 2\n[667] 1 2 5 2 7 1 2 1 1 1 2 1 1 2 1 7 1 2 1 1 1 5 1 2 1 2 7 2 2 1 6 1 2 1 2 1 1\n[704] 2 1 1 1 1 2 1 1 1 1 1 4 4 5 6 5 5 6 5 6 2 6 6 5 4 6 6 1 4 6 6 2 2 5 6 4 2\n[741] 1 2 1 1 5 5 5 2 2 1 2 1 2 2 2 2 2 2 1 1 2 5 1 1 1 1 1 1 1 1 2 1 1 2\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetically, we can also find this as the dimension of each vector in the lists contained in edges.groups.\n\ntable(ccs7)\n\nccs7\n  1   2   3   4   5   6   7 \n225 289  21  40  68  84  47 \n\n\nFinally, we can also plot the pruned tree that shows the 7 clusters, overlaid above the LGA boundaries.\n\nplot(nga_wp_sp, \n     border = gray(.5))\nplot(clust7, \n     coordinates(nga_wp_sp), \n     cex.lab = 0.6,\n     groups.colors = c(\"red\",\"green\",\"blue\", \"brown\", \"pink\", \"yellow\", \"black\"),\n     cex.circles = 0.005, \n     add = TRUE)\n\n\n\n\n\n\n13.6. Visualising Clusters in Choropleth Map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat <- as.matrix(clust7$groups)\nnga_wp_spatialcluster <- cbind(nga_wp_cluster, as.factor(groups_mat)) %>%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(nga_wp_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\nFor easy comparison, we place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map <- qtm(nga_wp_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map <- qtm(nga_wp_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\n\n\n\nThe difference between the SKATER approach versus the conventional hierarchical clustering is clear - the former uses the spatial configuration of the LGAs and cluster those closer in proximity (neighbours by Delaunay Triangulation) as seen on the plot on the right, while the former shows clusters that are all over the place as they are based on water point attributes only which are non-spatial.\nInterestingly, despite the corners of the Nigeria LGAs being assigned as neighbours to each other using the Delaunay Triangulation method (e.g. the Northeast and Northwest corners of the map were assigned as neighbours to each other), this did not feature in the SKATER map where the corners are in different clusters."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "14. Spatially Constrained Clustering ClustGeo Method",
    "text": "14. Spatially Constrained Clustering ClustGeo Method\nIn this section, we will use the ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n14.1. Ward-like Hierarchical Clustering: ClustGeo\nClustGeo package provides function called hclustgo() to perform a typical Ward-like hierarchical clustering, similar to hclust() of base R stats.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function of a dissimilarity matrix, as shown in the code chunk below.\n\nnongeo_cluster <- hclustgeo(proxmat.tsf.man)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster,\n            k = 7,\n            border = 2 : 5)\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist().\n\n\n14.2. Mapping the Clusters Formed\nSimilar to the hierarchical clustering visualisation, we can plot the clusters on a categorical area shaded map by using the steps below.\n\ngroups <- as.factor(cutree(nongeo_cluster, k = 7))\n\n\nnga_wp_ngeo_clust <- cbind(nga_wp, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed. We compare this (right plot) to the hierarchical clustering formed by hclust() of base R stats (left plot).\n\nhclust.map <- qtm(nga_wp_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nhclustgeo.map <- qtm(nga_wp_ngeo_clust, \"CLUSTER\")\n\ntmap_arrange(hclust.map, hclustgeo.map,\n             asp = NA, ncol = 2)\n\n\n\n\nWe see that the clustering outcomes are similar across both methods (e.g. clusters 1, 2, 4, 7 of the hierarchical clustering using hclust() is similar to clusters 7, 2, 4, 6 of hclustgeo()).\n\n\n14.3. Spatially Constrained Hierarchical Clustering\nBefore we can perform spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist <- st_distance(nga_wp, nga_wp)\ndistmat <- as.dist(dist)\n\nNote that as.dist() is used to convert the data frame into a matrix.\nNext, choicealpha() is used to determine a suitable value for the mixing parameter alpha, as shown in the code chunk below.\n\ncr <- choicealpha(proxmat.tsf.man, \n                  distmat, \n                  range.alpha = seq(0, 1, 0.1), \n                  K = 7, \n                  graph = TRUE)\n\n\n\n\n\n\n\n\nchoicealpha() is for us to balance 2 matrices.\nBalance out the homogeneity in attributes space (D0) and geographical space (spatial, e.g Queen’s contiguity weight matrix) (D1).\nRanges from 0 to 1\n0-stage = only considering attribute space without consideration of attribute homogeneity\n1: spatial homogeneity\nst_distance() takes the centroid of polygons\nClustGeo is more rigid in terms of algorithm - only accepts Ward\nBut more flexible in terms of being able to use either contiguity-based or distance-based weight matrix\nseq(0, 1, 0.1): 0.1 = interval (increment) between 0 and 1 in the plotting\nK = 6: Note that “K” in this argument is in upper case!! Different from hclust().\n2 graphs plotted\n\n1st graph based on raw\n2nd graph based on normalisation values -> if we find that our data is highly skewed. We will look at this in this exercise.\n\nHelps us determine the optimal alpha value -> aim is to have as high Qnorm as possible\nBased on 2nd graph: can either choose either alpha 0.2 or 0.3.\nSharp increase in spatial homogeneity with <20% drop in attribute homogeneity from 0.1 to 0.2 alpha value.\nIn practice, we should compare a few alpha values to see how the map changes.\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG <- hclustgeo(proxmat.tsf.man, distmat, alpha = 0.25)\n\nNext, cutree() is used to derive the cluster object.\n\ngroups <- as.factor(cutree(clustG, k = 7))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nnga_wp_Gcluster <- cbind(nga_wp, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters. We do so by comparing it (on the right plot) side-by-side with the Ward-like hierarchical clustering that we did earlier without considering the spatial component (on the left plot).\n\nhclustgeo.map <- qtm(nga_wp_ngeo_clust, \"CLUSTER\")\n\nhclustgeo0.25.map <- qtm(nga_wp_Gcluster, \"CLUSTER\")\n\ntmap_arrange(hclustgeo.map, hclustgeo0.25.map,\n             asp = NA, ncol = 2)\n\n\n\n\nWe see that now the clusters are no longer all over the place, but follows a more geospatial-related set of rules. At the same time, it is not completely geospatial either, with some clusters still spanning across different regions, e.g. cluster 4.\n\nTo interpret the clusters, we can use heatmaply() to study the features of each cluster, OR do a boxplot (summary statistics) to do so.\n\n\n\n14.4. Interpreting and Naming Clusters"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#conclusion",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "15. Conclusion",
    "text": "15. Conclusion"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#references",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#references",
    "title": "Regionalisation of Water Point Attributes in Nigeria",
    "section": "16. References",
    "text": "16. References\nWikipedia, 2022. Local government areas of Nigeria. https://en.wikipedia.org/wiki/Local_government_areas_of_Nigeria\nWikipedia, 2022. States of Nigeria. https://en.wikipedia.org/wiki/States_of_Nigeria\nesri, 2022. https://pro.arcgis.com/en/pro-app/2.9/tool-reference/spatial-statistics/how-spatially-constrained-multivariate-clustering-works.htm\nsfdep, 2022. https://sfdep.josiahparry.com/reference/st_nb_delaunay.html"
  }
]